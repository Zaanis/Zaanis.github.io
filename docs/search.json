[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Joshua Chen‚Äôs Resume",
    "section": "",
    "text": "This is my general resume:\nDownload PDF file.\nThis is my NLP/DL specific resume\nDownload PDF file.\nThis is my Machine Learning specific resume\nDownload PDF file."
  },
  {
    "objectID": "projects/project1/proto.html",
    "href": "projects/project1/proto.html",
    "title": "Prototype Selection",
    "section": "",
    "text": "The goal of this project is to find a way to down sample to size n from an entire training set so that the training time for the nearest neighbor could be reduced while remaining the accuracy of the original training set.\nThe algorithms are tested on the MNIST dataset."
  },
  {
    "objectID": "projects/project1/proto.html#algorithm-1",
    "href": "projects/project1/proto.html#algorithm-1",
    "title": "Prototype Selection",
    "section": "Algorithm 1",
    "text": "Algorithm 1\nK-Means Clustering 1: Split the dataset into 10 clusters (as there are 10 digits) and take an equal amount of datapoints from each cluster as the subsample"
  },
  {
    "objectID": "projects/project1/proto.html#algorithm-2",
    "href": "projects/project1/proto.html#algorithm-2",
    "title": "Prototype Selection",
    "section": "Algorithm 2",
    "text": "Algorithm 2\nK-Means Clustering 2: Split the dataset into n clusters and take one datapoint from each cluster as the subsample"
  },
  {
    "objectID": "projects/project1/proto.html#algorithm-3",
    "href": "projects/project1/proto.html#algorithm-3",
    "title": "Prototype Selection",
    "section": "Algorithm 3",
    "text": "Algorithm 3\nModified Active Learning: Randomly select datapoints and run a classification model and select the most uncertain datapoints."
  },
  {
    "objectID": "projects/project1/proto.html#algorithm-4",
    "href": "projects/project1/proto.html#algorithm-4",
    "title": "Prototype Selection",
    "section": "Algorithm 4",
    "text": "Algorithm 4\nModified K-Means: Subsample proportionately to training, identify digits that are likely to be misclassified and apply K-means to clustered digits"
  },
  {
    "objectID": "projects/nlp/restaurant.html",
    "href": "projects/nlp/restaurant.html",
    "title": "Restaurant Category Prediction",
    "section": "",
    "text": "The goal of this project is to predict the restaurant type using details about the restaurant and their reviews. There is also a Kaggle Competition Page associated with this project.\n\n\nThis is my position on the Kaggle Leader Board:"
  },
  {
    "objectID": "projects/nlp/restaurant.html#leaderboard",
    "href": "projects/nlp/restaurant.html#leaderboard",
    "title": "Restaurant Category Prediction",
    "section": "",
    "text": "This is my position on the Kaggle Leader Board:"
  },
  {
    "objectID": "projects/MA/hw4_questions.html",
    "href": "projects/MA/hw4_questions.html",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "This post implements a few measure of variable importance, interpreted as a key drivers analysis, for certain aspects of a payment card on customer satisfaction with that payment card.\nTo start with the computation of variable importance, I first loaded the data."
  },
  {
    "objectID": "projects/MA/hw4_questions.html#data",
    "href": "projects/MA/hw4_questions.html#data",
    "title": "Key Drivers Analysis",
    "section": "Data",
    "text": "Data\n\nKey Variables\nThe explanatory variables of the dataset are:\n\ntrust - Is this a brand I trust\nbuild - Does the card build credit quickly\ndiffers - Is it different from other cards\neasy - Is it easy to use\nappealing - Does it have appealing benefits/rewards\nrewarding - Does it reward me for responsible usage\npopular - Is it used by a lot of people\nservice - Does it provide outstanding customer service\nimpact - Does it make a difference in my life\n\nThe target variable is:\n\nsatisfaction - How satisfied the customer is with the card (1-5)\n\n\n\nData Description\n\n\n\n\n\n\nSurvey Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                 \n\n Loading ITables v2.6.1 from the internet... (need help?)\n\n\n\n\n\n\n\n\n\n\n\nAs seen above, the explanatory variables are all binary representing the survey responses.\n\n\nThere are 2553 rows in this data\nThere are 12 columns in this data\n\n\nThe extra column is brand, numbered from 1-10, it will be excluded from further analysis."
  },
  {
    "objectID": "projects/MA/hw4_questions.html#calculating-variable-importance",
    "href": "projects/MA/hw4_questions.html#calculating-variable-importance",
    "title": "Key Drivers Analysis",
    "section": "Calculating Variable Importance",
    "text": "Calculating Variable Importance\n\nPearson Correlation\nThe Pearson correlation coefficient measures the strength and direction of a linear relationship between two continuous variables. The coefficient ranges from -1 to 1, where:\n\n1 indicates a perfect positive linear relationship,\n-1 indicates a perfect negative linear relationship,\n0 indicates no linear relationship.\n\nValues closer to 1 or -1 suggest a stronger linear relationship, while values near 0 indicate a weaker relationship. Positive values mean that as one variable increases, so does the other, and negative values indicate that as one variable increases, the other decreases.\nTo calculate the Pearson Correlation, I used the corr method of the pandas DataFrame and specified the method as pearson.\nThe calculation is shown below:\n\ncorrelation_matrix = data.corr(method='pearson')['satisfaction'].drop(['satisfaction', 'brand', 'id'])\n\n\n\n\n\n\n\n\n\n\n\nPearson Correlation\n\n\n\n\ntrust\n0.255706\n\n\nbuild\n0.191896\n\n\ndiffers\n0.184801\n\n\neasy\n0.212985\n\n\nappealing\n0.207997\n\n\nrewarding\n0.194561\n\n\npopular\n0.171425\n\n\nservice\n0.251098\n\n\nimpact\n0.254539\n\n\n\n\n\n\n\n\nThe results show that features trust, service, and impact have the highest positive correlations with customer satisfaction\n\n\nPolychoric Correlations\nPolychoric correlations are used to estimate the relationship between two ordinal variables by assuming that each is a discretized representation of an underlying continuous variable. This statistical method is particularly useful for data derived from surveys using Likert scales or similar ordinal scales, where the actual data points represent categories that approximate a continuous scale. Polychoric correlations operate by estimating the thresholds that separate these continuous latent variables into observed ordinal categories and then calculating the correlation between these latent variables.\nTo calculate the Polychoric Correlations, I used the semopy module from Python\nThe calculation is shown below:\n\nfrom semopy import Model\nfrom semopy.examples import multivariate_regression\n\ndesc = '''satisfaction ~ trust + build + differs + easy + appealing + rewarding + popular + service + impact'''\nmod = Model(desc)\nmod.fit(data)\nprint(mod.inspect())\n\n           lval  op          rval  Estimate  Std. Err    z-value       p-value\n0  satisfaction   ~         trust  0.272677  0.056080   4.862253  1.160573e-06\n1  satisfaction   ~         build  0.045942  0.053785   0.854175  3.930081e-01\n2  satisfaction   ~       differs  0.069876  0.055060   1.269073  2.044149e-01\n3  satisfaction   ~          easy  0.052377  0.056755   0.922853  3.560838e-01\n4  satisfaction   ~     appealing  0.078803  0.055862   1.410653  1.583469e-01\n5  satisfaction   ~     rewarding  0.010950  0.056160   0.194979  8.454093e-01\n6  satisfaction   ~       popular  0.038642  0.051026   0.757302  4.488689e-01\n7  satisfaction   ~       service  0.209313  0.056714   3.690688  2.236485e-04\n8  satisfaction   ~        impact  0.319913  0.056430   5.669205  1.434619e-08\n9  satisfaction  ~~  satisfaction  1.222821  0.034226  35.728140  0.000000e+00\n\n\nThe results show that features trust, service, and impact have the highest positive Polychoric correlations with customer satisfaction\n\n\nStandardized Regression Coefficients\nStandardized regression coefficients, often referred to as beta coefficients, are used in multiple regression analyses to assess the relative importance and impact of each independent variable on the dependent variable. These coefficients are derived from a regression model in which all variables (both independent and dependent) have been standardized to have a mean of zero and a standard deviation of one. This standardization removes the units, allowing the coefficients to be compared directly.\nTo compute the Standardized Regression Coefficients, I used a linear regression model from sklearn and also scaled the data with StandardScaler from sklearn\nThe computation is shown below\n\nX = data.drop(['satisfaction', 'brand', 'id'], axis=1)\ny = data['satisfaction']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nmodel = LinearRegression()\nmodel.fit(X_scaled, y)\n\nstandardized_coefficients = model.coef_\n\n\n\n\n\n\n\n\n\n\n\nStandardized Coefficients\n\n\n\n\ntrust\n0.135635\n\n\nbuild\n0.023411\n\n\ndiffers\n0.032631\n\n\neasy\n0.025744\n\n\nappealing\n0.039647\n\n\nrewarding\n0.005937\n\n\npopular\n0.019470\n\n\nservice\n0.103573\n\n\nimpact\n0.150482\n\n\n\n\n\n\n\n\nThe results show that features trust, service, and impact have the highest significant positive effect on satisfaction when all other variables are held constant.\n\n\nUsefulness (Shapley Values)\nShapley values, a concept borrowed from cooperative game theory, offer a powerful method for interpreting machine learning models. These values measure the contribution of each feature to the prediction of a particular instance, by considering all possible combinations of features.\nTo compute the Shapley Values, I used the Shap module from Python and passed in the linear regression model above.\nThe computation is shown below\n\nexplainer = shap.Explainer(model, X_scaled)\n\n# Compute SHAP values\nshap_values = explainer(X_scaled)\n\n# Get the mean absolute SHAP values for each feature across all data points\nshap_values = np.abs(shap_values.values).mean(axis=0)\n\n\n\n\n\n\n\n\n\n\n\nShapley Values\n\n\n\n\ntrust\n0.136576\n\n\nbuild\n0.023157\n\n\ndiffers\n0.028857\n\n\neasy\n0.025924\n\n\nappealing\n0.039060\n\n\nrewarding\n0.005861\n\n\npopular\n0.019465\n\n\nservice\n0.102030\n\n\nimpact\n0.130708\n\n\n\n\n\n\n\n\nThe results show that features trust, service, and impact have the highest positive impact on the linear regression model‚Äôs prediction for satisfaction.\n\n\nUsefulness (Permutation Importance)\nPermutation importance is a technique used to measure the importance of individual features in a predictive model by evaluating the impact of shuffling each feature on the model‚Äôs performance. The process involves systematically randomizing the values of each feature across the dataset and observing the change in the model‚Äôs accuracy or other performance metrics. By disrupting the association between the feature and the target, the model‚Äôs performance typically decreases if the feature is important. The magnitude of the decrease, averaged over multiple shuffles, quantifies the feature‚Äôs importance.\nTo compute the Permutatio Importance, I used the permutation_importance function from sklearn.inspection and passed in the linear regression model above.\nThe computation is shown below:\n\nfrom sklearn.inspection import permutation_importance\n\n# Fit a linear model again to ensure it's correctly fit\nmodel.fit(X_scaled, y)\n\n# Compute permutation importance\nresults = permutation_importance(model, X_scaled, y, n_repeats=30, random_state=42)\n\n\n\n\n\n\n\n\n\n\n\nFeature\nPermutation Importance\n\n\n\n\n0\ntrust\n0.027039\n\n\n1\nbuild\n0.000866\n\n\n2\ndiffers\n0.001641\n\n\n3\neasy\n0.001097\n\n\n4\nappealing\n0.002386\n\n\n5\nrewarding\n0.000097\n\n\n6\npopular\n0.000523\n\n\n7\nservice\n0.016162\n\n\n8\nimpact\n0.033854\n\n\n\n\n\n\n\n\nThe results show that features trust, service, and impact have the highest permutation importance, suggesting that they are the most importance features in determining customer satisfaction in the linear regression model.\n\n\nJoshnson‚Äôs Relative Weights\nJohnson‚Äôs relative weights are a method used to assess the importance of predictor variables in a regression model, especially useful in contexts where predictors are correlated. This technique calculates the contribution of each predictor to the R-squared value, adjusted for the overlap with other predictors. Each predictor‚Äôs relative weight is computed by first transforming the predictor variables into orthogonal (uncorrelated) components. Then, the squared multiple correlation (R-squared) from a regression of the dependent variable on these orthogonal components is computed. Each original predictor‚Äôs relative contribution is assessed by reconstructing the R-squared from these orthogonal components, attributing portions of the variance explained back to the original correlated predictors.\nThe computation is shown below:\n\nbetas = model.coef_\nfeature_stds = X.std().values\n\n# Calculate the relative importance of each feature using Johnson's relative weights method\nraw_importance = np.square(betas * feature_stds)\nrelative_weights = raw_importance / raw_importance.sum()\n\n\n\n\n\n\n\n\n\n\n\nJohnson Relative Weight\n\n\n\n\ntrust\n0.343209\n\n\nbuild\n0.010266\n\n\ndiffers\n0.017863\n\n\neasy\n0.012421\n\n\nappealing\n0.029334\n\n\nrewarding\n0.000658\n\n\npopular\n0.007105\n\n\nservice\n0.201248\n\n\nimpact\n0.377896\n\n\n\n\n\n\n\n\nThe results show that features trust, service, and impact have the highest relative weights, suggesting that they are the most importance features in explaining the prediction variance of customer satisfaction in the linear regression model.\n\n\nMean Decrease in Gini Coefficient\nThe Mean Decrease in Gini Coefficient is a feature importance metric used in tree-based models such as Decision Trees and Random Forests. It quantifies each feature‚Äôs contribution to the model by calculating the average decrease in node impurity, measured by the Gini impurity, when the model splits on that feature. Gini impurity indicates the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the node. A feature with a higher Mean Decrease in Gini Coefficient is considered more important as it contributes more significantly to reducing uncertainty or ‚Äúpurifying‚Äù the outcomes at each split, thus enhancing the model‚Äôs predictive accuracy and efficiency in classifying or predicting the target variable.\nTo compute the Mean Decrease in Gini Coefficient, I trained a Random Forest and extracted the feature importance (which is the mean decrease in Gini Coefficient).\nThe computation is shown below:\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_model.fit(X_scaled, y)\n\n# Extract the feature importances (Mean Decrease in Gini Coefficient)\nfeature_importances = rf_model.feature_importances_\n\n\n\n\n\n\n\n\n\n\n\nFeature\nMean Decrease in Gini\n\n\n\n\n0\ntrust\n0.155865\n\n\n1\nbuild\n0.102301\n\n\n2\ndiffers\n0.089897\n\n\n3\neasy\n0.099904\n\n\n4\nappealing\n0.085534\n\n\n5\nrewarding\n0.101057\n\n\n6\npopular\n0.094944\n\n\n7\nservice\n0.129664\n\n\n8\nimpact\n0.140834\n\n\n\n\n\n\n\n\nThe results show that features trust, service, and impact have the highest impurity decrease, suggesting that they are the most importance features in reducing impurity in the RF, indicating that they are the most importance features in predicting customer satisfaction.\n\n\nXGBoost Feature Importance\nXGBoost feature importance quantifies the contribution of each feature to the model‚Äôs predictive power, typically calculated using three different metrics: gain, cover, and frequency. Gain measures the average contribution of a feature to the model‚Äôs performance, specifically how much each feature contributes to improving the accuracy of splits it is involved in, weighted by the number of observations affected. Cover evaluates the number of observations affected by a feature‚Äôs inclusion in splits, emphasizing the feature‚Äôs relevance across different data points. Frequency counts how often a feature is used in splits across all trees, providing insight into its general utility. To compute the Mean Decrease in Gini Coefficient, I trained a Random Forest and extracted the feature importance (which is the mean decrease in Gini Coefficient).\nTo compute the XGBoost Feature Importance, I trained a XGBoost Classifier and extracted the feature importance.\nThe computation is shown below:\n\nxgb_model = xgb.XGBRegressor(random_state=42)\nxgb_model.fit(X_scaled, y)\n\n# Get feature importances from the XGBoost model\nxgb_importances = pd.Series(xgb_model.feature_importances_)\n\n\n\n\n\n\n\n\n\n\n\nFeature\nXGBoost Feature Importance\n\n\n\n\n0\ntrust\n0.289867\n\n\n1\nbuild\n0.078933\n\n\n2\ndiffers\n0.056405\n\n\n3\neasy\n0.071498\n\n\n4\nappealing\n0.065786\n\n\n5\nrewarding\n0.065396\n\n\n6\npopular\n0.076136\n\n\n7\nservice\n0.111327\n\n\n8\nimpact\n0.184652\n\n\n\n\n\n\n\n\nThe results show that features trust, service, and impact have the highest feature importance, suggesting that they are the most importance in predicting customer satisfaction for the XGBoost model."
  },
  {
    "objectID": "projects/MA/hw4_questions.html#analysis-and-summary",
    "href": "projects/MA/hw4_questions.html#analysis-and-summary",
    "title": "Key Drivers Analysis",
    "section": "Analysis and Summary",
    "text": "Analysis and Summary\nBelow is the table that combines all the metrics created above:\n\n\n\n\n\n\n\n\n\n\nFeature\nPearson Correlation\nPolychoric Correlation\nStandardized Regression Coefficients\nShapley Values\nJohnson's Relative Weights\nPermutation Importance\nMean Decrease in Gini\nXGBoost Feature Importance\n\n\n\n\n0\ntrust\n0.255706\n0.272677\n0.135635\n0.136576\n0.343209\n0.027039\n0.155865\n0.289867\n\n\n1\nbuild\n0.191896\n0.045942\n0.023411\n0.023157\n0.010266\n0.000866\n0.102301\n0.078933\n\n\n2\ndiffers\n0.184801\n0.069876\n0.032631\n0.028857\n0.017863\n0.001641\n0.089897\n0.056405\n\n\n3\neasy\n0.212985\n0.052377\n0.025744\n0.025924\n0.012421\n0.001097\n0.099904\n0.071498\n\n\n4\nappealing\n0.207997\n0.078803\n0.039647\n0.039060\n0.029334\n0.002386\n0.085534\n0.065786\n\n\n5\nrewarding\n0.194561\n0.010950\n0.005937\n0.005861\n0.000658\n0.000097\n0.101057\n0.065396\n\n\n6\npopular\n0.171425\n0.038642\n0.019470\n0.019465\n0.007105\n0.000523\n0.094944\n0.076136\n\n\n7\nservice\n0.251098\n0.209313\n0.103573\n0.102030\n0.201248\n0.016162\n0.129664\n0.111327\n\n\n8\nimpact\n0.254539\n0.319913\n0.150482\n0.130708\n0.377896\n0.033854\n0.140834\n0.184652\n\n\n\n\n\n\n\n\nFrom the table above, we can see that Trust, Impact, and Service are consistantly ranked top 3 in all of the metrics created. Trust is either first or Impact is first and Service is always third.\nThis suggests that Trust, Impact and Service are the most important features for customer satisfaction when it comes to payment cards, indicating that customers value whether or not the brand is trustable, the impact the card will have in the customers life, and the customer service the company provides when they are considering their choice of cards.\nWith this information, payment card companies should:\n\nFocus on building and maintaining their company‚Äôs image as a trusted braand to the public\nOffer benefits that can positive impact customer‚Äôs life\nInvest in their customer service representatives and aim to offer more support to customers as well as increase the quality of support."
  },
  {
    "objectID": "projects/MA/hw2_questions.html",
    "href": "projects/MA/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\nThis is the first 5 rows of the data\n\n\nThe shape of the dataframe is: (1500, 4)\n\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n1\n0\nMidwest\n32.5\n0\n\n\n786\n3\nSouthwest\n37.5\n0\n\n\n348\n4\nNorthwest\n27.0\n1\n\n\n927\n3\nNortheast\n24.5\n0\n\n\n830\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\nThere are 1500 rows and 4 columns\n\n\n\n\n\n\n\n\n\n\n\n\npatents\nage\niscustomer\n\n\n\n\ncount\n1500.000000\n1500.000000\n1500.000000\n\n\nmean\n3.684667\n26.357667\n0.131333\n\n\nstd\n2.352500\n7.242528\n0.337877\n\n\nmin\n0.000000\n9.000000\n0.000000\n\n\n25%\n2.000000\n21.000000\n0.000000\n\n\n50%\n3.000000\n26.000000\n0.000000\n\n\n75%\n5.000000\n31.625000\n0.000000\n\n\nmax\n16.000000\n49.000000\n1.000000\n\n\n\n\n\n\n\n\n\n\n\n\npatents: Number of patents awarded over the last 5 years.\nregion: Regional location of the firm.\nage: Age of the firm since incorporation.\niscustomer: Indicates whether the firm uses Blueprinty‚Äôs software (1 = customer, 0 = not a customer)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean_customers = df[df['iscustomer'] == 1]['patents'].mean()\nmean_non_customers = df[df['iscustomer'] == 0]['patents'].mean()\n(mean_customers, mean_non_customers)\n\n(4.091370558375634, 3.6231772831926325)\n\n\nThe mean number of patents for customers appears to be slightly higher at 4.09, while the mean number of patents for non-customers appear to be lower at 3.62.\n\n\n\nHowever, Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nTo investigate any systematic differences, below are plots that compare regions and ages by customer status\n\n\n\n\n\n\n\n\n\n\nmean_age_customers = data_customers['age'].mean()\nmean_age_non_customers = data_non_customers['age'].mean()\n(mean_age_customers, mean_age_non_customers)\n\n(24.1497461928934, 26.691481197237145)\n\n\nIt appears that the average age of customers are lower than the average age of non customers.\n\n\n\n\n\n\n\n\n\nThe distribution of customers and non-customers across regions is not uniform. Both histograms reveal differences in the concentration of customers versus non-customers in various regions, suggesting regional preferences or market penetration differences.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe likelihood function for a set of independent and identically distributed observations ( Y_1, Y_2, , Y_n ) from a Poisson distribution, where each ( Y_i ) represents the number of patents awarded to an engineering firm in a given period and follows a Poisson distribution with parameter ( ), is given by:\n\\[\nL(\\lambda | Y_1, Y_2, \\ldots, Y_n) = \\prod_{i=1}^n f(Y_i | \\lambda)\n\\]\nGiven the probability mass function of the Poisson distribution ( f(Y|) = e{-}Y/Y! ), the likelihood function can be written as:\n\\[\nL(\\lambda | Y_1, Y_2, \\ldots, Y_n) = \\prod_{i=1}^n \\left( \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!} \\right)\n\\]\nThis can be simplified to:\n\\[\nL(\\lambda | Y_1, Y_2, \\ldots, Y_n) = e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i} \\prod_{i=1}^n \\frac{1}{Y_i!}\n\\]\nHere, ( n ) is the total number of observations (engineering firms), ( {i=1}^n Y_i ) is the total number of patents awarded across all firms, and ( {i=1}^n ) is the product of the factorials of the counts of patents for each firm.\nThe log-likelihood function for the Poisson model, coded in a function of lambda and Y would look like the following:\n\nimport numpy as np\nfrom scipy.special import factorial\n\ndef poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        return -np.inf  # log-likelihood is negative infinity if lambda is non-positive\n    return np.sum(-lambda_ + Y * np.log(lambda_) - np.log(factorial(Y)))\n\nBelow I use the function above to plot lambda on the horizontal axis and the likelihood on the vertical axis for a range of lambdas, which I used the number of patents as the input.\n\n\n\n\n\n\n\n\n\nNow, I used scipy.optimize to find the MLE after optimizing the likelihood function\n\nfrom scipy.optimize import minimize\n\n# Define the negative log-likelihood function since we minimize in the optimization\ndef negative_loglikelihood(lambda_, Y):\n    if lambda_[0] &lt;= 0:\n        return np.inf  # Return infinity if lambda is non-positive\n    lambda_val = lambda_[0]\n    return -np.sum(-lambda_val + Y * np.log(lambda_val) - np.log(factorial(Y)))\n\n# Initial guess for lambda\ninitial_lambda = [1.0]\n\n# Using scipy's minimize function to find the MLE of lambda\nresult = minimize(negative_loglikelihood, initial_lambda, args=(patents_data,), method='L-BFGS-B', bounds=[(0, None)])\n\nprint(f\"MLE for lambda: {result['x'][0]}\")\n\nMLE for lambda: 3.6846667021660804\n\n\nThe optimization has successfully found the maximum likelihood estimate (MLE) of ùúÜ for the Poisson distribution based on the patent data. The MLE of ùúÜ is approximately 3.685. This indicates that the best estimate for the average number of patents awarded per engineering firm over the observed period, under the assumption of a Poisson distribution, is about 3.685 patents. ‚Äã\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nFirst, I need to update my previous log-likelihood function to reflect that:\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    linear = X @ beta\n    lambda_i = np.exp(np.clip(linear, None, 20))\n    \n    log_likelihood = np.sum(Y * np.log(lambda_i) - lambda_i - np.log(factorial(Y)))\n    return -log_likelihood  \n\nI then used the updated function to find the MLE vector and the Hessian of the Poisson model with covariates. I also printed out the coefficient and the standard effor for each variable.\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom scipy.linalg import inv\n\n\ndf['age_squared'] = df['age'] ** 2\n\nencoder = OneHotEncoder(drop='first')\nregion_encoded = encoder.fit_transform(df[['region']]).toarray()\n\n\nY = df['patents'].values\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the continuous variables\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(df[['age', 'age_squared']])\n\n# Construct the design matrix with scaled features\nX = np.hstack([np.ones((df.shape[0], 1)), scaled_features, df[['iscustomer']].values, region_encoded])\n\ninitial_beta = np.zeros(X.shape[1])\n\n# Run the optimizer with detailed logging\nresult = minimize(\n    fun=poisson_regression_loglikelihood,\n    x0=initial_beta,\n    args=(Y, X),\n    method='L-BFGS-B',\n    bounds=[(None, None)] * X.shape[1],\n    options={'disp': True}\n)\n\nhess_inv = result.hess_inv.todense()  # if using L-BFGS, convert to dense matrix\n\ndef neg_log_likelihood(beta, Y, X):\n    lambda_ = np.exp(np.dot(X, beta))\n    return np.sum(-Y * np.log(lambda_) + lambda_ + gammaln(Y + 1))\n\ndef grad_neg_log_likelihood(beta, Y, X):\n    lambda_ = np.exp(np.dot(X, beta))\n    grad = np.dot(X.T, lambda_ - Y)\n    return grad\n\ndef hessian_neg_log_likelihood(beta, Y, X):\n    lambda_ = np.exp(np.dot(X, beta))\n    diag_lambda = np.diag(lambda_)\n    hessian = np.dot(X.T, np.dot(diag_lambda, X))\n    return hessian\n\nhessian_matrix = hessian_neg_log_likelihood(result.x, Y, X)\n\ncovariance_matrix_from_hessian = inv(hessian_matrix)\n\nstandard_errors_from_hessian = np.sqrt(np.diag(covariance_matrix_from_hessian))\nvariables = ['Age', 'Age Squared', 'Customer Status', 'Region Northeast', 'Region Northwest', 'Region South', 'Region Southwest']\n# Print the coefficients and their standard errors\nfor v, coef, std_err in zip(variables, result.x, standard_errors_from_hessian):\n    print(f\"{v}| Coefficient: {coef:.4f}, Standard Error: {std_err:.4f}\")\n\nAge| Coefficient: 1.2155, Standard Error: 0.0364\nAge Squared| Coefficient: 1.0464, Standard Error: 0.1005\nCustomer Status| Coefficient: -1.1408, Standard Error: 0.1025\nRegion Northeast| Coefficient: 0.1182, Standard Error: 0.0389\nRegion Northwest| Coefficient: 0.0986, Standard Error: 0.0420\nRegion South| Coefficient: -0.0201, Standard Error: 0.0538\nRegion Southwest| Coefficient: 0.0572, Standard Error: 0.0527\n\n\nNow we try to replicate the coefficients and standard errors with statsmodels.GLM()\n\nimport statsmodels.api as sm\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\n\n# Fit the model\nresult = poisson_model.fit()\n\n# Display the summary\n# Extract standard errors\ncoefficients = result.params\nstandard_errors = result.bse\np_values = result.pvalues\nconf_int = pd.DataFrame(result.conf_int(), columns=['95% CI Lower', '95% CI Upper'])\n\nstats_table = pd.DataFrame({\n    'Coefficient': coefficients,\n    'Standard Error': standard_errors,\n    'P-value': p_values,\n    '95% CI Lower': conf_int['95% CI Lower'],\n    '95% CI Upper': conf_int['95% CI Upper']\n})\n\nprint(stats_table)\n\n   Coefficient  Standard Error        P-value  95% CI Lower  95% CI Upper\n0     1.215438        0.036426  4.023701e-244      1.144045      1.286831\n1     1.046460        0.100487   2.144106e-25      0.849508      1.243412\n2    -1.140845        0.102495   8.884562e-29     -1.341731     -0.939960\n3     0.118114        0.038920   2.407229e-03      0.041832      0.194397\n4     0.098596        0.042007   1.891865e-02      0.016264      0.180928\n5    -0.020094        0.053783   7.086909e-01     -0.125508      0.085319\n6     0.057172        0.052676   2.777636e-01     -0.046071      0.160414\n7     0.051347        0.047212   2.767834e-01     -0.041188      0.143882\n\n\nAs seen in the table, coefficients and standard errors perfectly match the ones above.\n\n\n\nThe coefficient for the variable (index 3) representing whether or not it is a customer of Blueprinty is positive (0.1181) and statistically significant, suggests that firms using Blueprinty‚Äôs software likely have a higher expected patent count. The coefficient further implies that using Blueprinty‚Äôs software increases the patent count likelihood by approximately exp(0.1181) ‚âà 1.125, or 12.5%."
  },
  {
    "objectID": "projects/MA/hw2_questions.html#blueprinty-case-study",
    "href": "projects/MA/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\nThis is the first 5 rows of the data\n\n\nThe shape of the dataframe is: (1500, 4)\n\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n1\n0\nMidwest\n32.5\n0\n\n\n786\n3\nSouthwest\n37.5\n0\n\n\n348\n4\nNorthwest\n27.0\n1\n\n\n927\n3\nNortheast\n24.5\n0\n\n\n830\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\nThere are 1500 rows and 4 columns\n\n\n\n\n\n\n\n\n\n\n\n\npatents\nage\niscustomer\n\n\n\n\ncount\n1500.000000\n1500.000000\n1500.000000\n\n\nmean\n3.684667\n26.357667\n0.131333\n\n\nstd\n2.352500\n7.242528\n0.337877\n\n\nmin\n0.000000\n9.000000\n0.000000\n\n\n25%\n2.000000\n21.000000\n0.000000\n\n\n50%\n3.000000\n26.000000\n0.000000\n\n\n75%\n5.000000\n31.625000\n0.000000\n\n\nmax\n16.000000\n49.000000\n1.000000\n\n\n\n\n\n\n\n\n\n\n\n\npatents: Number of patents awarded over the last 5 years.\nregion: Regional location of the firm.\nage: Age of the firm since incorporation.\niscustomer: Indicates whether the firm uses Blueprinty‚Äôs software (1 = customer, 0 = not a customer)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean_customers = df[df['iscustomer'] == 1]['patents'].mean()\nmean_non_customers = df[df['iscustomer'] == 0]['patents'].mean()\n(mean_customers, mean_non_customers)\n\n(4.091370558375634, 3.6231772831926325)\n\n\nThe mean number of patents for customers appears to be slightly higher at 4.09, while the mean number of patents for non-customers appear to be lower at 3.62.\n\n\n\nHowever, Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nTo investigate any systematic differences, below are plots that compare regions and ages by customer status\n\n\n\n\n\n\n\n\n\n\nmean_age_customers = data_customers['age'].mean()\nmean_age_non_customers = data_non_customers['age'].mean()\n(mean_age_customers, mean_age_non_customers)\n\n(24.1497461928934, 26.691481197237145)\n\n\nIt appears that the average age of customers are lower than the average age of non customers.\n\n\n\n\n\n\n\n\n\nThe distribution of customers and non-customers across regions is not uniform. Both histograms reveal differences in the concentration of customers versus non-customers in various regions, suggesting regional preferences or market penetration differences.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe likelihood function for a set of independent and identically distributed observations ( Y_1, Y_2, , Y_n ) from a Poisson distribution, where each ( Y_i ) represents the number of patents awarded to an engineering firm in a given period and follows a Poisson distribution with parameter ( ), is given by:\n\\[\nL(\\lambda | Y_1, Y_2, \\ldots, Y_n) = \\prod_{i=1}^n f(Y_i | \\lambda)\n\\]\nGiven the probability mass function of the Poisson distribution ( f(Y|) = e{-}Y/Y! ), the likelihood function can be written as:\n\\[\nL(\\lambda | Y_1, Y_2, \\ldots, Y_n) = \\prod_{i=1}^n \\left( \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!} \\right)\n\\]\nThis can be simplified to:\n\\[\nL(\\lambda | Y_1, Y_2, \\ldots, Y_n) = e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i} \\prod_{i=1}^n \\frac{1}{Y_i!}\n\\]\nHere, ( n ) is the total number of observations (engineering firms), ( {i=1}^n Y_i ) is the total number of patents awarded across all firms, and ( {i=1}^n ) is the product of the factorials of the counts of patents for each firm.\nThe log-likelihood function for the Poisson model, coded in a function of lambda and Y would look like the following:\n\nimport numpy as np\nfrom scipy.special import factorial\n\ndef poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        return -np.inf  # log-likelihood is negative infinity if lambda is non-positive\n    return np.sum(-lambda_ + Y * np.log(lambda_) - np.log(factorial(Y)))\n\nBelow I use the function above to plot lambda on the horizontal axis and the likelihood on the vertical axis for a range of lambdas, which I used the number of patents as the input.\n\n\n\n\n\n\n\n\n\nNow, I used scipy.optimize to find the MLE after optimizing the likelihood function\n\nfrom scipy.optimize import minimize\n\n# Define the negative log-likelihood function since we minimize in the optimization\ndef negative_loglikelihood(lambda_, Y):\n    if lambda_[0] &lt;= 0:\n        return np.inf  # Return infinity if lambda is non-positive\n    lambda_val = lambda_[0]\n    return -np.sum(-lambda_val + Y * np.log(lambda_val) - np.log(factorial(Y)))\n\n# Initial guess for lambda\ninitial_lambda = [1.0]\n\n# Using scipy's minimize function to find the MLE of lambda\nresult = minimize(negative_loglikelihood, initial_lambda, args=(patents_data,), method='L-BFGS-B', bounds=[(0, None)])\n\nprint(f\"MLE for lambda: {result['x'][0]}\")\n\nMLE for lambda: 3.6846667021660804\n\n\nThe optimization has successfully found the maximum likelihood estimate (MLE) of ùúÜ for the Poisson distribution based on the patent data. The MLE of ùúÜ is approximately 3.685. This indicates that the best estimate for the average number of patents awarded per engineering firm over the observed period, under the assumption of a Poisson distribution, is about 3.685 patents. ‚Äã\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nFirst, I need to update my previous log-likelihood function to reflect that:\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    linear = X @ beta\n    lambda_i = np.exp(np.clip(linear, None, 20))\n    \n    log_likelihood = np.sum(Y * np.log(lambda_i) - lambda_i - np.log(factorial(Y)))\n    return -log_likelihood  \n\nI then used the updated function to find the MLE vector and the Hessian of the Poisson model with covariates. I also printed out the coefficient and the standard effor for each variable.\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom scipy.linalg import inv\n\n\ndf['age_squared'] = df['age'] ** 2\n\nencoder = OneHotEncoder(drop='first')\nregion_encoded = encoder.fit_transform(df[['region']]).toarray()\n\n\nY = df['patents'].values\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the continuous variables\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(df[['age', 'age_squared']])\n\n# Construct the design matrix with scaled features\nX = np.hstack([np.ones((df.shape[0], 1)), scaled_features, df[['iscustomer']].values, region_encoded])\n\ninitial_beta = np.zeros(X.shape[1])\n\n# Run the optimizer with detailed logging\nresult = minimize(\n    fun=poisson_regression_loglikelihood,\n    x0=initial_beta,\n    args=(Y, X),\n    method='L-BFGS-B',\n    bounds=[(None, None)] * X.shape[1],\n    options={'disp': True}\n)\n\nhess_inv = result.hess_inv.todense()  # if using L-BFGS, convert to dense matrix\n\ndef neg_log_likelihood(beta, Y, X):\n    lambda_ = np.exp(np.dot(X, beta))\n    return np.sum(-Y * np.log(lambda_) + lambda_ + gammaln(Y + 1))\n\ndef grad_neg_log_likelihood(beta, Y, X):\n    lambda_ = np.exp(np.dot(X, beta))\n    grad = np.dot(X.T, lambda_ - Y)\n    return grad\n\ndef hessian_neg_log_likelihood(beta, Y, X):\n    lambda_ = np.exp(np.dot(X, beta))\n    diag_lambda = np.diag(lambda_)\n    hessian = np.dot(X.T, np.dot(diag_lambda, X))\n    return hessian\n\nhessian_matrix = hessian_neg_log_likelihood(result.x, Y, X)\n\ncovariance_matrix_from_hessian = inv(hessian_matrix)\n\nstandard_errors_from_hessian = np.sqrt(np.diag(covariance_matrix_from_hessian))\nvariables = ['Age', 'Age Squared', 'Customer Status', 'Region Northeast', 'Region Northwest', 'Region South', 'Region Southwest']\n# Print the coefficients and their standard errors\nfor v, coef, std_err in zip(variables, result.x, standard_errors_from_hessian):\n    print(f\"{v}| Coefficient: {coef:.4f}, Standard Error: {std_err:.4f}\")\n\nAge| Coefficient: 1.2155, Standard Error: 0.0364\nAge Squared| Coefficient: 1.0464, Standard Error: 0.1005\nCustomer Status| Coefficient: -1.1408, Standard Error: 0.1025\nRegion Northeast| Coefficient: 0.1182, Standard Error: 0.0389\nRegion Northwest| Coefficient: 0.0986, Standard Error: 0.0420\nRegion South| Coefficient: -0.0201, Standard Error: 0.0538\nRegion Southwest| Coefficient: 0.0572, Standard Error: 0.0527\n\n\nNow we try to replicate the coefficients and standard errors with statsmodels.GLM()\n\nimport statsmodels.api as sm\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\n\n# Fit the model\nresult = poisson_model.fit()\n\n# Display the summary\n# Extract standard errors\ncoefficients = result.params\nstandard_errors = result.bse\np_values = result.pvalues\nconf_int = pd.DataFrame(result.conf_int(), columns=['95% CI Lower', '95% CI Upper'])\n\nstats_table = pd.DataFrame({\n    'Coefficient': coefficients,\n    'Standard Error': standard_errors,\n    'P-value': p_values,\n    '95% CI Lower': conf_int['95% CI Lower'],\n    '95% CI Upper': conf_int['95% CI Upper']\n})\n\nprint(stats_table)\n\n   Coefficient  Standard Error        P-value  95% CI Lower  95% CI Upper\n0     1.215438        0.036426  4.023701e-244      1.144045      1.286831\n1     1.046460        0.100487   2.144106e-25      0.849508      1.243412\n2    -1.140845        0.102495   8.884562e-29     -1.341731     -0.939960\n3     0.118114        0.038920   2.407229e-03      0.041832      0.194397\n4     0.098596        0.042007   1.891865e-02      0.016264      0.180928\n5    -0.020094        0.053783   7.086909e-01     -0.125508      0.085319\n6     0.057172        0.052676   2.777636e-01     -0.046071      0.160414\n7     0.051347        0.047212   2.767834e-01     -0.041188      0.143882\n\n\nAs seen in the table, coefficients and standard errors perfectly match the ones above.\n\n\n\nThe coefficient for the variable (index 3) representing whether or not it is a customer of Blueprinty is positive (0.1181) and statistically significant, suggests that firms using Blueprinty‚Äôs software likely have a higher expected patent count. The coefficient further implies that using Blueprinty‚Äôs software increases the patent count likelihood by approximately exp(0.1181) ‚âà 1.125, or 12.5%."
  },
  {
    "objectID": "projects/MA/hw2_questions.html#airbnb-case-study",
    "href": "projects/MA/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\n\n\nLoading Data\n\n\n(40628, 13)\n\n\n\n\n\n\n\n\n\n\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\nThe dataframe has 40628 rows and 13 columns, I‚Äôve also shown the first 5 rows of the dataframe\n\n\nData Statistics\n\n\n\n\n\n\n\n\n\n\nid\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n4.062800e+04\n40628.000000\n40468.000000\n40552.000000\n40628.000000\n40628.000000\n30433.000000\n30374.000000\n30372.000000\n\n\nmean\n9.698889e+06\n1102.368219\n1.124592\n1.147046\n144.760732\n15.904426\n9.198370\n9.413544\n9.331522\n\n\nstd\n5.460166e+06\n1383.269358\n0.385884\n0.691746\n210.657597\n29.246009\n1.119935\n0.844949\n0.902966\n\n\nmin\n2.515000e+03\n1.000000\n0.000000\n0.000000\n10.000000\n0.000000\n2.000000\n2.000000\n2.000000\n\n\n25%\n4.889868e+06\n542.000000\n1.000000\n1.000000\n70.000000\n1.000000\n9.000000\n9.000000\n9.000000\n\n\n50%\n9.862878e+06\n996.000000\n1.000000\n1.000000\n100.000000\n4.000000\n10.000000\n10.000000\n10.000000\n\n\n75%\n1.466789e+07\n1535.000000\n1.000000\n1.000000\n170.000000\n17.000000\n10.000000\n10.000000\n10.000000\n\n\nmax\n1.800967e+07\n42828.000000\n8.000000\n10.000000\n10000.000000\n421.000000\n10.000000\n10.000000\n10.000000\n\n\n\n\n\n\n\n\n\n\nData Cleaning\n\ndf.isna().sum()\n\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\nThe dataset appears to contain several columns with null values.\nSince the dataset is large enough, I‚Äôve decided to drop all null values to see if the remaining dataset is still large enough to proceed.\n\ndf_1 = df.dropna()\ndf_1.shape\n\n(30140, 13)\n\n\nWith over 30000 rows, I‚Äôve decided that the dataset is large enough to continue.\n\n\nUpdated Data Statistics\n\n\n\n\n\n\n\n\n\n\nid\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n3.014000e+04\n30140.000000\n30140.000000\n30140.000000\n30140.000000\n30140.000000\n30140.000000\n30140.000000\n30140.000000\n\n\nmean\n8.978322e+06\n1112.048275\n1.122213\n1.151460\n140.211546\n21.168115\n9.201758\n9.415428\n9.334041\n\n\nstd\n5.376960e+06\n644.430782\n0.385031\n0.699039\n188.437967\n32.004711\n1.114472\n0.843181\n0.900595\n\n\nmin\n2.515000e+03\n7.000000\n0.000000\n0.000000\n10.000000\n1.000000\n2.000000\n2.000000\n2.000000\n\n\n25%\n4.276596e+06\n584.000000\n1.000000\n1.000000\n70.000000\n3.000000\n9.000000\n9.000000\n9.000000\n\n\n50%\n9.149773e+06\n1040.000000\n1.000000\n1.000000\n103.000000\n8.000000\n10.000000\n10.000000\n10.000000\n\n\n75%\n1.391476e+07\n1591.000000\n1.000000\n1.000000\n169.000000\n26.000000\n10.000000\n10.000000\n10.000000\n\n\nmax\n1.797369e+07\n3317.000000\n6.000000\n10.000000\n10000.000000\n421.000000\n10.000000\n10.000000\n10.000000\n\n\n\n\n\n\n\n\n\n\nEDA\n\n\n\n\n\n\n\n\n\nThe distribution of the numbers of reviews is heavility skewed to the right, indicating that most listings have relatively few reviews (&lt;100).\nThere also does not seem to have a clean linear relationship between price and the number of reviews. Although houses with more reviews does tend to have lower price, likely because people tend to want to book cheaper houses.\nAdditionally, there does not appear to be a linear relationship between the number of reviews and the number of days listed. Although as the numbers of days listed increases, the number of days also likely increases.\nFinally, houses with around 2-3 bathrooms receive the most reviews.\n\n\nPoisson Model\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndf_1['room_type'] = df_1['room_type'].astype('category')\ndf_1['instant_bookable'] = df_1['instant_bookable'].astype('category').cat.codes\n\nformula = 'number_of_reviews ~ room_type + price + review_scores_cleanliness + review_scores_location + review_scores_value + instant_bookable + days + bathrooms + bedrooms'\npoisson_model = smf.glm(formula=formula, data=df_1, family=sm.families.Poisson()).fit()\n\n# Output model summary\nmodel_params = poisson_model.summary2().tables[1][['Coef.', 'Std.Err.', 'z', 'P&gt;|z|']]\nmodel_params\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n\n\n\n\nIntercept\n2.942697\n0.016633\n176.919813\n0.000000e+00\n\n\nroom_type[T.Private room]\n0.019233\n0.002738\n7.024594\n2.146901e-12\n\n\nroom_type[T.Shared room]\n-0.115193\n0.008650\n-13.317770\n1.824774e-40\n\n\nprice\n-0.000043\n0.000008\n-5.199251\n2.000937e-07\n\n\nreview_scores_cleanliness\n0.110978\n0.001517\n73.158720\n0.000000e+00\n\n\nreview_scores_location\n-0.081481\n0.001617\n-50.402764\n0.000000e+00\n\n\nreview_scores_value\n-0.091055\n0.001847\n-49.310566\n0.000000e+00\n\n\ninstant_bookable\n0.459104\n0.002919\n157.293238\n0.000000e+00\n\n\ndays\n0.000522\n0.000002\n280.429721\n0.000000e+00\n\n\nbathrooms\n-0.113444\n0.003773\n-30.066678\n1.321761e-198\n\n\nbedrooms\n0.075659\n0.002033\n37.214455\n3.983758e-303\n\n\n\n\n\n\n\n\n\n\nAnalysis Summary\nThe analysis reveals that compared to entire homes/apartments, which serve as the base category, private rooms actually have a slight positive effect on the number of reviews (coefficient = +0.019). This suggests that private rooms might receive slightly more bookings compared to entire homes/apartments. Conversely, shared rooms have a substantial negative impact on the number of reviews (coefficient = -0.115), indicating significantly fewer bookings for shared accommodations.\nThe coefficients for review scores indicate varied effects on booking rates. High cleanliness scores are strongly associated with an increase in bookings (coefficient = +0.111), highlighting the importance guests place on cleanliness. Interestingly, better scores for location (coefficient = -0.081) and value (coefficient = -0.091) are associated with fewer bookings. This counterintuitive result may suggest that higher expectations for these aspects could negatively impact guest satisfaction or reflect a trade-off guests are making with other variables such as price.\nThe model outputs indicate that the number of bathrooms negatively affects the number of reviews (coefficient = -0.113). This could suggest that listings with more bathrooms may not necessarily increase the likelihood of bookings. This could be due to higher associated costs or perhaps the type of listings that typically feature multiple bathrooms.\nConversely, an increase in the number of bedrooms has a positive effect on the number of reviews (coefficient = +0.076), indicating that listings with more bedrooms tend to be more popular or accommodating for larger groups, thus potentially receiving more bookings.\nThe coefficient for days listed (days) is positive (coefficient = +0.000522), showing that the longer a listing has been on the platform, the more reviews it accumulates. This trend likely reflects a cumulative effect where older listings have had more time to accumulate reviews, thus suggesting a gradual build-up of bookings over time.\nMost importantly, listings that are instantly bookable tend to have more bookings, which makes sense as it allows guests to book without waiting for host approval, making the process quicker and more convenient."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joshua‚Äôs Website",
    "section": "",
    "text": "Previous Data Science Student, Current IBM Software Developer :::{#main-content}"
  },
  {
    "objectID": "index.html#current",
    "href": "index.html#current",
    "title": "Joshua‚Äôs Website",
    "section": "Current",
    "text": "Current\n\nIBM Software Developer"
  },
  {
    "objectID": "index.html#work-history",
    "href": "index.html#work-history",
    "title": "Joshua‚Äôs Website",
    "section": "Work History",
    "text": "Work History\n\nIBM | 2025 - Present\nAlphaTrAI | 2023 - 2024\nTeaching Assistant | 2023 - 2024\n\nCogs 108 Data Science in Practice\nCogs 14a Introduction to Research Methods\n\nInstructional Assistant | 2022 - 2023\n\nCogs 18 Introduction to Python\nCogs 1 Introduction to Cognitive Science"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Joshua‚Äôs Website",
    "section": "Education",
    "text": "Education\n\nMS Business Analytics | UC San Diego 2024\n\n4.0 GPA\n\nBS Cognitive Science w/ Spec. in Machine Learning | UC San Diego 2023\n\nMagna Cum Laude\n3.966 GPA :::"
  },
  {
    "objectID": "hobbies/games/valorant.html",
    "href": "hobbies/games/valorant.html",
    "title": "Valorant",
    "section": "",
    "text": "Here is a collection of my valorant clips\n\nRaze Ace\n\n\n\nJett Outplay\n\n\n\nJett Ace\n\n\n\nKnifing Chamber"
  },
  {
    "objectID": "hobbies/games/basketball.html",
    "href": "hobbies/games/basketball.html",
    "title": "Joshua's Website",
    "section": "",
    "text": "Placeholder"
  },
  {
    "objectID": "hobbies/games/league.html",
    "href": "hobbies/games/league.html",
    "title": "League of Legends",
    "section": "",
    "text": "Here is a collection of my league clips\n\nAkali Aram Penta\n\n\n\nJhin Aram Penta\n\n\n\nViego Aram Penta\n\n\n\nYone Aram Penta\n\n\n\nViego Aram Penta 2"
  },
  {
    "objectID": "hobbies.html",
    "href": "hobbies.html",
    "title": "Hobbies",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "hobbies.html#games",
    "href": "hobbies.html#games",
    "title": "Hobbies",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "hobbies.html#basketball",
    "href": "hobbies.html#basketball",
    "title": "Hobbies",
    "section": "Basketball",
    "text": "Basketball\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/MA/hw1_questions.html",
    "href": "projects/MA/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nIn their experiment, Dean Karlan and John A. List conducted a large-scale natural field experiment to explore the impact of price on charitable giving. They used direct mail solicitations sent to over 50,000 previous donors of a nonprofit organization to test the effectiveness of matching grants on charitable donations. The experiment randomly assigned individuals to either a control group or a matching grant treatment group. Within the matching grant treatment group, individuals were further randomly assigned to different matching grant rates, matching grant maximum amounts, and suggested donation amounts.\nThe study found that announcing the availability of match money significantly increased both the revenue per solicitation (by 19%) and the probability of making a donation (by 22%). However, larger matching ratios ($3:$1 and $2:$1) did not have an additional impact compared to a smaller matching ratio ($1:$1). The elasticity estimate of the price change from the baseline to the treatment groups was -0.30, which is near the lower range of the elasticity of giving with respect to transitory price changes reported in previous studies.\nInterestingly, the effectiveness of the matching gift varied by the political environment of the donors. In states that voted for George W. Bush in the 2004 presidential election (‚Äúred‚Äù states), the match increased the revenue per solicitation by 55%, while in ‚Äúblue‚Äù states, there was little effect observed.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/MA/hw1_questions.html#introduction",
    "href": "projects/MA/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nIn their experiment, Dean Karlan and John A. List conducted a large-scale natural field experiment to explore the impact of price on charitable giving. They used direct mail solicitations sent to over 50,000 previous donors of a nonprofit organization to test the effectiveness of matching grants on charitable donations. The experiment randomly assigned individuals to either a control group or a matching grant treatment group. Within the matching grant treatment group, individuals were further randomly assigned to different matching grant rates, matching grant maximum amounts, and suggested donation amounts.\nThe study found that announcing the availability of match money significantly increased both the revenue per solicitation (by 19%) and the probability of making a donation (by 22%). However, larger matching ratios ($3:$1 and $2:$1) did not have an additional impact compared to a smaller matching ratio ($1:$1). The elasticity estimate of the price change from the baseline to the treatment groups was -0.30, which is near the lower range of the elasticity of giving with respect to transitory price changes reported in previous studies.\nInterestingly, the effectiveness of the matching gift varied by the political environment of the donors. In states that voted for George W. Bush in the 2004 presidential election (‚Äúred‚Äù states), the match increased the revenue per solicitation by 55%, while in ‚Äúblue‚Äù states, there was little effect observed.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/MA/hw1_questions.html#data",
    "href": "projects/MA/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n\n(50083, 51)\n\n\nThere are 50083 rows and 51 columns in this dataset.\n\n\n           treatment       control    ratio        ratio2        ratio3  \\\ncount   50083.000000  50083.000000    50083  50083.000000  50083.000000   \nunique           NaN           NaN        4           NaN           NaN   \ntop              NaN           NaN  Control           NaN           NaN   \nfreq             NaN           NaN    16687           NaN           NaN   \nmean        0.666813      0.333187      NaN      0.222311      0.222211   \nstd         0.471357      0.471357      NaN      0.415803      0.415736   \nmin         0.000000      0.000000      NaN      0.000000      0.000000   \n25%         0.000000      0.000000      NaN      0.000000      0.000000   \n50%         1.000000      0.000000      NaN      0.000000      0.000000   \n75%         1.000000      1.000000      NaN      0.000000      0.000000   \nmax         1.000000      1.000000      NaN      1.000000      1.000000   \n\n           size        size25        size50       size100        sizeno  ...  \\\ncount     50083  50083.000000  50083.000000  50083.000000  50083.000000  ...   \nunique        5           NaN           NaN           NaN           NaN  ...   \ntop     Control           NaN           NaN           NaN           NaN  ...   \nfreq      16687           NaN           NaN           NaN           NaN  ...   \nmean        NaN      0.166723      0.166623      0.166723      0.166743  ...   \nstd         NaN      0.372732      0.372643      0.372732      0.372750  ...   \nmin         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n25%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n50%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n75%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \nmax         NaN      1.000000      1.000000      1.000000      1.000000  ...   \n\n              redcty       bluecty        pwhite        pblack     page18_39  \\\ncount   49978.000000  49978.000000  48217.000000  48047.000000  48217.000000   \nunique           NaN           NaN           NaN           NaN           NaN   \ntop              NaN           NaN           NaN           NaN           NaN   \nfreq             NaN           NaN           NaN           NaN           NaN   \nmean        0.510245      0.488715      0.819599      0.086710      0.321694   \nstd         0.499900      0.499878      0.168561      0.135868      0.103039   \nmin         0.000000      0.000000      0.009418      0.000000      0.000000   \n25%         0.000000      0.000000      0.755845      0.014729      0.258311   \n50%         1.000000      0.000000      0.872797      0.036554      0.305534   \n75%         1.000000      1.000000      0.938827      0.090882      0.369132   \nmax         1.000000      1.000000      1.000000      0.989622      0.997544   \n\n           ave_hh_sz  median_hhincome        powner  psch_atlstba  \\\ncount   48221.000000     48209.000000  48214.000000  48215.000000   \nunique           NaN              NaN           NaN           NaN   \ntop              NaN              NaN           NaN           NaN   \nfreq             NaN              NaN           NaN           NaN   \nmean        2.429012     54815.700533      0.669418      0.391661   \nstd         0.378115     22027.316665      0.193405      0.186599   \nmin         0.000000      5000.000000      0.000000      0.000000   \n25%         2.210000     39181.000000      0.560222      0.235647   \n50%         2.440000     50673.000000      0.712296      0.373744   \n75%         2.660000     66005.000000      0.816798      0.530036   \nmax         5.270000    200001.000000      1.000000      1.000000   \n\n        pop_propurban  \ncount    48217.000000  \nunique            NaN  \ntop               NaN  \nfreq              NaN  \nmean         0.871968  \nstd          0.258654  \nmin          0.000000  \n25%          0.884929  \n50%          1.000000  \n75%          1.000000  \nmax          1.000000  \n\n[11 rows x 51 columns]\n\n\nThe above shows a general distribution for each variable.\n\n\ntreatment                0\ncontrol                  0\nratio                    0\nratio2                   0\nratio3                   0\nsize                     0\nsize25                   0\nsize50                   0\nsize100                  0\nsizeno                   0\nask                      0\naskd1                    0\naskd2                    0\naskd3                    0\nask1                     0\nask2                     0\nask3                     0\namount                   0\ngave                     0\namountchange             0\nhpa                      0\nltmedmra                 0\nfreq                     0\nyears                    1\nyear5                    0\nmrm2                     1\ndormant                  0\nfemale                1111\ncouple                1148\nstate50one               0\nnonlit                 452\ncases                  452\nstatecnt                 0\nstateresponse            0\nstateresponset           0\nstateresponsec           3\nstateresponsetminc       3\nperbush                 35\nclose25                 35\nred0                    35\nblue0                   35\nredcty                 105\nbluecty                105\npwhite                1866\npblack                2036\npage18_39             1866\nave_hh_sz             1862\nmedian_hhincome       1874\npowner                1869\npsch_atlstba          1868\npop_propurban         1866\ndtype: int64\n\n\nThis shows the number of missing values in each column.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nTesting mrm2 - Months since last donation.\n\n\nT-test results: t-statistic = 0.11953155228177251, p-value = 0.9048549631450832\n\n\n\n\nLinear regression results:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Wed, 24 Dec 2025   Prob (F-statistic):              0.905\nTime:                        12:26:10   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n13.011828117981734\n12.99814226643495\n\n\nUsing both the t-test and linear regerssion, the p-value for the difference in mens between treatment and control groups for mrm2 is 0.905. This means that we fail to reject the null hypothesis that the treatment and control groups are statistically significantly different at the 95% confidence level, further suggesting that the randomization was successful.\n\n\nmean for mrm2 control: 12.99814226643495\nmean for mrm2 control: 13.011828117981734\n\n\nThe above values matches the ones shown in Table 1.\n\n\nTesting hpa - highest previous contribution\n\n\nT-test results: t-statistic = 0.9704273843087994, p-value = 0.3318400397145116\n\n\n\n\nLinear regression results:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    hpa   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.8924\nDate:                Wed, 24 Dec 2025   Prob (F-statistic):              0.345\nTime:                        12:26:10   Log-Likelihood:            -2.8468e+05\nNo. Observations:               50083   AIC:                         5.694e+05\nDf Residuals:                   50081   BIC:                         5.694e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     58.9602      0.551    107.005      0.000      57.880      60.040\ntreatment      0.6371      0.675      0.944      0.345      -0.685       1.960\n==============================================================================\nOmnibus:                    66199.149   Durbin-Watson:                   2.003\nProb(Omnibus):                  0.000   Jarque-Bera (JB):         14448195.271\nSkew:                           7.552   Prob(JB):                         0.00\nKurtosis:                      84.826   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n59.59724\n58.960167\n\n\nUsing both the t-test and linear regerssion, the p-value for the difference in mens between treatment and control groups for hpa are both above 0.94. This means that we fail to reject the null hypothesis that the treatment and control groups are statistically significantly different at the 95% confidence level, further suggesting that the randomization was successful.\n\n\nmean for mrm2 control: 58.960167\nmean for mrm2 control: 59.59724\n\n\nThe above values matches the ones shown in Table 1.  Note: hpa is highest previous contribution and mrm2 is number of months since last donation."
  },
  {
    "objectID": "projects/MA/hw1_questions.html#experimental-results",
    "href": "projects/MA/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\n\n\n\n\n\n\n\nAs seen in the plot above, the control group appears to have a lower proportion compared to the treatment group.\n\n\nT-test results: t-statistic = 3.101361000543946, p-value = 0.0019274025949016982\nLinear regression results:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Wed, 24 Dec 2025   Prob (F-statistic):            0.00193\nTime:                        12:26:11   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe t-test and linear regression both show a p value of 0.002. This suggest that the difference in response rates between the treatment and control groups is statistically significant, meaning that the treatment group does indeed increase the likelihood of making a charitable donation compared to the control group. This finding is also consistent with the results reported in Table 2a.\n\n\ncoefficient 0.004180354512949377\nz-statistic:  3.101361000543931\np-value:  0.001927402594901797\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Wed, 24 Dec 2025   Prob (F-statistic):            0.00193\nTime:                        12:26:11   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe probit regression also confirms that the coefficient on the treatment varialbe is statistically significant (coefficient of 0.004 and p-value of 0.002). This is consistent with the findings on Table 3 column 1, suggesting that people in the treatemnt group does have increased likelihood of making a charitable donation. However, it is do be noted that while Table 3 indicates the use of Probit regression, the above was replicated with a linear regression.\nThe results of a probit regression is shown below:\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\ncoefficient 0.08678462244745795\nz-statistic:  3.112930073794974\np-value:  0.0018523990147786566\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 24 Dec 2025   Pseudo R-squ.:               0.0009783\nTime:                        12:26:11   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\nWhile the coefficient is not the same as that on Table 3 column 1, the p-value is still 0.002, indicating that people in the treatment group does have increased likelihood of making a charitable donation.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\n1:1 and 2:1 match ratiosT-test results: t-statistic = 0.05011583793874515, p-value = 0.9600305283739325\n1:1 and 3:1 match ratiosT-test results: t-statistic = -1.0150255853798622, p-value = 0.31010466370866724\n1:1 and 2:1 match ratiosT-test results: t-statistic = -0.96504713432247, p-value = 0.33453168549723933\n\n\nThe t-tests and the p-values show that the difference in response rate betwen 1:1 and 2:1, 2:1 and 3:1, 1:1 and 3:1 are all not statistically significant at the 95% confidence level. This means that the match ratios does not increase the likelihood of someone making a charitable donation. This is consistent with the authors comments on page 8, which suggested that the figures do not show a clear pattern of increasing repsonse rates as match ratios increase.\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.665\nDate:                Wed, 24 Dec 2025   Prob (F-statistic):             0.0118\nTime:                        12:26:11   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50079   BIC:                        -5.322e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\nratio1         0.0029      0.002      1.661      0.097      -0.001       0.006\nratio2         0.0048      0.002      2.744      0.006       0.001       0.008\nratio3         0.0049      0.002      2.802      0.005       0.001       0.008\n==============================================================================\nOmnibus:                    59812.754   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4316693.217\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.438   Cond. No.                         4.26\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe regression results indicate that the coefficients for the 2:1 and 3:1 match ratios are statistically significant at the 95% confidence interval, suggesting that these match sizes significantly increase the likelihood of donating compared to the baseline category, which is the control group. However, the coefficient for the 1:1 match ratio is not statistically significant, indicating that a 1:1 match does not significantly enhance the likelihood of donating when compared to no matching. Therefore, while the 2:1 and 3:1 matches are effective in increasing donation rates, the 1:1 match does not show a significant effect relative to having no match at all.\n\n\ndifference between 3:1 and 2:1 : 0.00010002398025293902\ndifference between 1:1 and 2:1 : -0.0018842510217149944\ndifference between 3:1 and 1:1 : 0.0019842750019679334\n\n\nThe above is calcualted directly from data\n\n\ndifference between 3:1 and 2:1 : 0.00010002398025313504\ndifference between 1:1 and 2:1 : -0.0018842510217151158\ndifference between 3:1 and 1:1 : 0.001984275001968251\n\n\nThe above is calculated using the coefficients generated from the regression above.\nBoth resulted in the same set of numbers.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n\ndifference in donation amounts between control and treatment:  0.1536054\nt-statistic:  1.8605020225753781\np-value:  0.06282038947470686\n\n\nThe p-value suggest that the difference in donation amount between the treatment and control group is not statistically significant at the 95% confidence level. This suggests that the treatment group does not appear to have a higher donation amount.\n\n\ndifference in donation amounts between control and treatment:  -1.6683922\nt-statistic:  -0.5808388615237938\np-value:  0.5614758782284279\n\n\nNow limiting to only the people that donated, it appears that the treatment group donates about 1.66 units less than the control group. This different is not statistically significant though, as the p-value is 0.5615, well above the 0.05 threshold. Therefore, we cannot confidently assert the causal interpretation that people receiving treatment will result in reduced donation amounts."
  },
  {
    "objectID": "projects/MA/hw1_questions.html#simulation-experiment",
    "href": "projects/MA/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic ‚Äúworks,‚Äù in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\n\n\n\n\n\n\n\nIn this plot, the x-axis represents the number of draws, and the y-axis represents the cumulative average of the differences between the treatment and control draws. The red dashed line represents the true difference in means between the treatment and control distributions.\nAs the number of draws increases, you would expect the cumulative average to approach the true difference in means. This is because, with a larger sample size, the estimate of the difference in means becomes more accurate. If the cumulative average converges to the true difference in means, it indicates that the simulation is correctly capturing the underlying difference between the treatment and control groups.\n\n\nCentral Limit Theorem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe histograms presented illustrate the Central Limit Theorem in action, highlighting how the distribution of the differences between the sample means increasingly approximates a normal distribution as the sample size grows. Given that zero is positioned near the center of these distributions, it suggests that there is no significant difference between the donation amounts of the treatment and control groups. This central positioning of zero within the distribution indicates that any observed difference is likely due to random sampling variability rather than a true effect of the treatment. Thus, the data provides no substantial evidence to suggest that the treatment influences donation amounts compared to the control."
  },
  {
    "objectID": "projects/MA/hw3_questions.html",
    "href": "projects/MA/hw3_questions.html",
    "title": "Multinomial Logit Examples",
    "section": "",
    "text": "This assignment uses uses the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans."
  },
  {
    "objectID": "projects/MA/hw3_questions.html#estimating-yogurt-preferences",
    "href": "projects/MA/hw3_questions.html#estimating-yogurt-preferences",
    "title": "Multinomial Logit Examples",
    "section": "1. Estimating Yogurt Preferences",
    "text": "1. Estimating Yogurt Preferences\n\nLikelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, size, price, etc.).\nWe model the consumer‚Äôs decision as the selection of the product that provides the most utility, and we‚Äôll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\n\n\nYogurt Dataset\nWe will use the yogurt_data dataset, which provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were ‚Äúfeatured‚Äù in the store as a form of advertising (f1:f4), and the products‚Äô prices (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1‚Äôs purchase. Consumers 2 through 7 each bought yogurt 2, etc.\n\n\n\n\n\n\nRaw Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                 \n\n Loading ITables v2.6.1 from the internet... (need help?)\n\n\n\n\n\n\n\n\n\n\n\nLet the vector of product features include brand dummy variables for yogurts 1-3 (we‚Äôll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts‚Äô prices:\n\\[\nx_j' = [\\mathbf{1}(\\text{Yogurt 1}), \\mathbf{1}(\\text{Yogurt 2}), \\mathbf{1}(\\text{Yogurt 3}), X_f, X_p]\n\\]\nThe ‚Äúhard part‚Äù of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)).\nWhat we would like to do is reorganize the data from a ‚Äúwide‚Äù shape with \\(n\\) rows and multiple columns for each covariate, to a ‚Äúlong‚Äù shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we‚Äôll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be ‚Äúpivoted‚Äù or ‚Äúmelted‚Äù from wide to long.\n\n\n\n\n\n\nMelting the Data\n\n\n\n\n\n\n# Melt the data to long format\nyogurt_long = pd.melt(yogurt_data, id_vars=['id'], value_vars=['y1', 'y2', 'y3', 'y4'],var_name='product', value_name='chosen')\n# Extract product number\nyogurt_long['product'] = yogurt_long['product'].str.extract('(\\d)').astype(int)\n# Melt the features to long format\nfeatures = pd.melt(yogurt_data, id_vars=['id'], value_vars=['f1', 'f2', 'f3', 'f4'], var_name='f_product', value_name='featured')\n# Extract product number\nfeatures['f_product'] = features['f_product'].str.extract('(\\d)').astype(int)\n# Melt the prices to long format\nprices = pd.melt(yogurt_data, id_vars=['id'], value_vars=['p1', 'p2', 'p3', 'p4'], var_name='p_product', value_name='price')\n# Extract product number\nprices['p_product'] = prices['p_product'].str.extract('(\\d)').astype(int)\n# Merge the data togehter\nyogurt_long = yogurt_long.merge(features, left_on=['id', 'product'], right_on=['id', 'f_product'])\nyogurt_long = yogurt_long.merge(prices, left_on=['id', 'product'], right_on=['id', 'p_product'])\nyogurt_long.drop(columns=['f_product', 'p_product'], inplace=True)\n# Create dummy variables for products 1, 2, and 3\nyogurt_long['yogurt 1'] = (yogurt_long['product'] == 1).astype(int)\nyogurt_long['yogurt 2'] = (yogurt_long['product'] == 2).astype(int)\nyogurt_long['yogurt 3'] = (yogurt_long['product'] == 3).astype(int)\n\n\n\n\n\n\n\n\n\n\nCleaned Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                 \n\n Loading ITables v2.6.1 from the internet... (need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimation\nThe log likelihood function that I will use it shown below:\n\ndef log_likelihood(beta, data):\n    \"\"\"\n    Calculates the log-likelihood of the MNL model.\n\n    Parameters:\n    beta: Coefficients Œ≤1, Œ≤2, Œ≤3, Œ≤f, Œ≤p.\n    data: The reshaped long format dataframe with columns ['id', 'product', 'chosen', 'featured', 'price', 'yogurt 1', 'yogurt 2', 'yogurt 3'].\n\n    Returns:\n    float: The log-likelihood value.\n    \"\"\"\n    beta1, beta2, beta3, beta_f, beta_p = beta\n    data['utility'] = (beta1 * data['yogurt 1'] + \n                       beta2 * data['yogurt 2'] + \n                       beta3 * data['yogurt 3'] + \n                       beta_f * data['featured'] + \n                       beta_p * data['price'])\n    data['exp_utility'] = np.exp(data['utility'])\n    data['sum_exp_utility'] = data.groupby('id')['exp_utility'].transform('sum')\n    data['probability'] = data['exp_utility'] / data['sum_exp_utility']\n    data['log_likelihood'] = data['chosen'] * np.log(data['probability'])\n    return -data['log_likelihood'].sum()\n\nWith the function defined, I then used scipy.optimize from Python to find the beta values for the 5 parameters. First, I initilized all the guesses as zero, then I used the minimize() fucntion from scipy.optimize to find the beta values. The values are shown below.\n\ninitial = [0,0,0,0,0]\nresult = minimize(log_likelihood, initial, args = (yogurt_long), method = 'BFGS')\nbeta_1, beta_2, beta_3, beta_f, beta_p = result.x\n\n\n\nEstimated parameters:\nbeta_1: 1.3877520023064622\nbeta_2: 0.6435049292323878\nbeta_3: -3.0861128992213978\nbeta_f: 0.48741409137900876\nbeta_p: -37.0578717065307\n\n\n\n\nDiscussion\nFrom the beta intercept values, we learned that:\n\nYogust 1 is the most preferred as (\\(\\beta_1\\)) has the largest positive value\nYogurt 2 is also second most preferred as (\\(\\beta_2\\)) is also positive. However, it is less preferred compared to Yogurt 1\nYogurt 3 is the least preferred, as suggested by the negative (\\(\\beta_3\\))\nYogurt 4 is third preferred. As it is the base comparison and there are 2 yogurts with positive coefficients and 1 yogurt with negative coefficient, making yogurt 4 the third preferred\n(\\(\\beta_f\\)) having a positive value of 0.487 indicates that featuring a yogurt increases its utility and thus its chance of being selected\n(\\(\\beta_p\\)) having a large negative value of 37.058 indicates higher prices reduces the chance of a yogurt being selected\n\nThe estimated price coefficient (\\(\\beta_p\\)) can be used as a dollar-per-util conversion factor. Using this conversion factor, I then calculated the dollar benefit between the most preferred yogurt (Yogurt 1) and the least preferred yogurt (yogurt 3). The per-unit monetary measure of the brand value and the calculation is shown below.\n\nconversion_factor = -1 / beta_p\nutility_difference = beta_1 - beta_3\nmonetary_value = utility_difference * conversion_factor\nmonetary_value\n\n0.12072643936374339\n\n\nThe monetary benefit between the most preferred yogurt (Yogurt 1) and the least preferred yogurt (yogurt 3) is approximately $0.12 per unit, meaning that customers value Yogurt 1 about $0.12 more than Yogurt 3.\nIn addion, with the MNL model, I was able to simulate the counterfactuals (eg, what if the price of yogurt 1 was $0.10/oz instead of $0.08/oz).\nThe first step in achieving this is to define a function that can predit market shares. The function is shown below:\n\ndef predict_market_shares(beta, data):\n    \"\"\"\n    Predict market shares using the estimated beta coefficients.\n\n    Parameters:\n    beta: Coefficients Œ≤1, Œ≤2, Œ≤3, Œ≤f, Œ≤p.\n    data: The reshaped long format dataframe with columns ['id', 'product', 'chosen', 'featured', 'price', 'yogurt 1', 'yogurt 2', 'yogurt 3'].\n\n    Returns:\n    DataFrame: The predicted market shares for each product.\n    \"\"\"\n    data['utility'] = (beta[0] * data['yogurt 1'] + \n                       beta[1] * data['yogurt 2'] + \n                       beta[2] * data['yogurt 3'] + \n                       beta[3] * data['featured'] + \n                       beta[4] * data['price'])\n    data['exp_utility'] = np.exp(data['utility'])\n    data['sum_exp_utility'] = data.groupby('id')['exp_utility'].transform('sum')\n    data['probability'] = data['exp_utility'] / data['sum_exp_utility']\n    market_shares = data.groupby('product')['probability'].mean().reset_index()\n    market_shares.columns = ['product', 'market_share']\n    return market_shares\n\nRunning the predict market shares function, the original market shares are:\n\noriginal_market_shares = predict_market_shares(result.x, yogurt_long)\noriginal_market_shares\n\n\n\n\n\n\n\n\n\nproduct\nmarket_share\n\n\n\n\n0\n1\n0.341975\n\n\n1\n2\n0.401235\n\n\n2\n3\n0.029218\n\n\n3\n4\n0.227572\n\n\n\n\n\n\n\n\nand the new market shares after price increase are:\n\nyogurt_long.loc[yogurt_long['product'] == 1, 'price'] += 0.10\nnew_market_shares = predict_market_shares(result.x, yogurt_long)\nnew_market_shares\n\n\n\n\n\n\n\n\n\nproduct\nmarket_share\n\n\n\n\n0\n1\n0.021118\n\n\n1\n2\n0.591145\n\n\n2\n3\n0.044040\n\n\n3\n4\n0.343697\n\n\n\n\n\n\n\n\nIncreasing the price of Yogurt 1 by $0.10 significantly decreased its market share from 34% to 2%, leading to an increase in market share for the other yogurts. This indicates that Yogurt 1 is highly price-sensitive, suggesting that customers perceive it as less differentiated or less valuable compared to other options, making them more likely to switch to alternatives when its price rises."
  },
  {
    "objectID": "projects/MA/hw3_questions.html#estimating-minivan-preferences",
    "href": "projects/MA/hw3_questions.html#estimating-minivan-preferences",
    "title": "Multinomial Logit Examples",
    "section": "2. Estimating Minivan Preferences",
    "text": "2. Estimating Minivan Preferences\n\nData\n\n\n\n\n\n\nConjoint Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                 \n\n Loading ITables v2.6.1 from the internet... (need help?)\n\n\n\n\n\n\n\n\n\n\n\n\nVariables\n\nresp.id: Respondent identifier\nques: task number\nalt: Alternative number\ncarpool: Carpool option (yes/no)\nseat: Number of seats (6, 7, 8)\ncargo: Cargo space (2ft, 3ft)\neng: Engine type (gas, hybrid, electric)\nprice: Price in thousands of dollars\nchoice: 1 if alternative was chosen 0 if not\n\nThe attributes (levels) were number of seats (6,7,8), cargo space (2ft, 3ft), engine type (gas, hybrid, electric), and price (in thousands of dollars).\n\nnum_resp = conjoint_data['resp.id'].nunique()\nnum_choice = conjoint_data['ques'].nunique()\nnum_alt = conjoint_data['alt'].nunique()\n\n\n\nNumber of respondents: 200\nNumber of choice tasks per respondent: 15\nNumber of alternatives per choice task: 3\n\n\n\n\n\nModel\nTo estimate an MNL moel, I omitted the following levels to avoid multicollinearity: 1. 6 in seat 2. 2ft in cargo 3. Gas in eng\nThe varialbes in the model are: 1. seat_7: Dummy varialbe for 7 seats 2. seat_8: Dummy varialbe for 8 seats 3. cargo_3ft: Dummy varialbe for 3ft cargo space 4. eng_hyb: Dummy variable for hybrid engine 5. eng_elec: Dummy variable for electric engine 6. price: price in thousands of dollars\nI will be running the model with the slightly modified log_likelihood function from the Yogurt report.\nThe function is shown below\n\ndef log_likelihood(beta, data):\n    \"\"\"\n    Calculates the log-likelihood of the MNL model.\n\n    Parameters:\n    beta: Coefficients Œ≤_seat_7, Œ≤_seat_8, Œ≤_cargo_3ft, Œ≤_eng_hyb, Œ≤_eng_elec, Œ≤_price\n    data: The conjoint data with the variables listed above\n    \n    Returns:\n    float: The log-likelihood value.\n    \"\"\"\n    beta_seat_7, beta_seat_8, beta_cargo_3ft, beta_eng_hyb, beta_eng_elec, beta_price = beta\n    data['utility'] = (beta_seat_7 * data['seat_7'] +\n                       beta_seat_8 * data['seat_8'] +\n                       beta_cargo_3ft * data['cargo_3ft'] +\n                       beta_eng_hyb * data['eng_hyb'] +\n                       beta_eng_elec * data['eng_elec'] +\n                       beta_price * data['price'])\n    data['exp_utility'] = np.exp(data['utility'])\n    data['sum_exp_utility'] = data.groupby(['resp.id', 'ques'])['exp_utility'].transform('sum')\n    data['probability'] = data['exp_utility'] / data['sum_exp_utility']\n    data['log_likelihood'] = data['choice'] * np.log(data['probability'])\n    return -data['log_likelihood'].sum()\n\nI then used scipy.optimize from Python to find the beta values for the 6 parameters. First, I initilized all the guesses as zero, then I used the minimize() fucntion from scipy.optimize to find the beta values. The values are shown below.\n\ninitial = np.zeros(6)\nresult = minimize(log_likelihood, initial, args=(conjoint_data), method='BFGS')\nestimated_beta_conjoint = result.x\n\n\nbeta_seat_7 = estimated_beta_conjoint[0]\nbeta_seat_8 = estimated_beta_conjoint[1]\nbeta_cargo_3ft = estimated_beta_conjoint[2]\nbeta_hybrid_engine = estimated_beta_conjoint[3]\nbeta_electric_engine = estimated_beta_conjoint[4]\nbeta_price = estimated_beta_conjoint[5]\n\nprint_statement = f\"\"\"- beta seat 7: {beta_seat_7}\n- beta seat 8: {beta_seat_8}\n- beta cargo 3ft: {beta_cargo_3ft}\n- beta hybrid engine: {beta_hybrid_engine}\n- beta electric engine: {beta_electric_engine}\n- beta price: {beta_price}\"\"\"\n\nprint(print_statement)\n\n- beta seat 7: -0.5345392758909884\n- beta seat 8: -0.30610738374179974\n- beta cargo 3ft: 0.4766936312151858\n- beta hybrid engine: -0.8107338846771628\n- beta electric engine: -1.5291247058947275\n- beta price: -0.17330526829538706\n\n\n\n\nResults\n\nCoefficient Interpretation\n\nSeats\n\n\n7 seats: The negative coefficient suggests that 7 seats are less preferred compared to 6 seats\n8 seats: The negative coefficient suggests that 8 seats are less preferred compared to 6 seats, but not as less preferred as 7 seats\n6 Seats is the most preferred, followed by 8 seats, and 7 seats is the least preferred\n\n\nCargo Space\n\n\n3ft cargo: The positive coefficient suggests that 3ft cargo space is preferred over 2 ft cargo space\n3ft cargo space is most preferred, 2ft cargo space is least preferred\n\n\nEngine\n\n\nHybrid Engine: The negative coefficient suggests that hybrid engines are less preferred compared to gas engines.\nElectric Engine: The negative coefficient suggests that electric engines are also less preferred compared to gas enginges, and also less preferred compared to hybrid engine as the coefficient is larger in magnitude.\nGas engine is the most preferred, followed by hybrid engine, and electric engine is the least preferred\n\n\nPrice\n\n\nThe negative coefficient indicates that higher prices decrease the utility of the minivan, meaning that it is less likely to be selected #### Dollar-Per-Util Calculation\n\nI then used the price coefficent as a dollar-per-util conversion factor and calcualted the value of 3ft cargo space compared to 2ft cargo space. The calculation and value are shown below:\n\nconversion_factor = -1 / estimated_beta_conjoint[5]\nutility_difference = estimated_beta_conjoint[2]\nvalue = utility_difference * conversion_factor * 1000\n\n\n\nThe monetary value of having 3ft of cargo space compared to 2ft of cargo space is approximately $2750.600924622175. This means that, on average, consumers value the additional 1ft of cargo space at $2750.600924622175\n\n\n\n\n\nMarket Share Prediction\nAssuming the market consists of the following 6 minivans, I used the model above (the beta coefficients) to predict the market shares of these 6 minivans.\n\n\n\nMinivan\nSeats\nCargo\nEngine\nPrice\n\n\n\n\nA\n7\n2\nHyb\n30\n\n\nB\n6\n2\nGas\n30\n\n\nC\n8\n2\nGas\n30\n\n\nD\n7\n3\nGas\n40\n\n\nE\n6\n2\nElec\n40\n\n\nF\n7\n2\nHyb\n35\n\n\n\n\n\n\n\n\n\nCalculation\n\n\n\n\n\n\nmarket = pd.DataFrame({\n    'minivan': ['A', 'B', 'C', 'D', 'E', 'F'],\n    'seat': [7, 6, 8, 7, 6, 7],\n    'cargo': ['2ft', '2ft', '2ft', '3ft', '2ft', '2ft'],\n    'eng': ['hyb', 'gas', 'gas', 'gas', 'elec', 'hyb'],\n    'price': [30, 30, 30, 40, 40, 35]\n})\nmarket['seat_7'] = (market['seat'] == 7).astype(int)\nmarket['seat_8'] = (market['seat'] == 8).astype(int)\nmarket['cargo_3ft'] = (market['cargo'] == '3ft').astype(int)\nmarket['eng_hyb'] = (market['eng'] == 'hyb').astype(int)\nmarket['eng_elec'] = (market['eng'] == 'elec').astype(int)\nmarket['utility'] = (estimated_beta_conjoint[0] * market['seat_7'] +\n                     estimated_beta_conjoint[1] * market['seat_8'] +\n                     estimated_beta_conjoint[2] * market['cargo_3ft'] +\n                     estimated_beta_conjoint[3] * market['eng_hyb'] +\n                     estimated_beta_conjoint[4] * market['eng_elec'] +\n                     estimated_beta_conjoint[5] * market['price'])\nmarket['exp_utility'] = np.exp(market['utility'])\nmarket['market_share'] = market['exp_utility'] / market['exp_utility'].sum()\n\n\n\n\nThe predicted market shares are shown below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                 \n\n Loading ITables v2.6.1 from the internet... (need help?)\n\n\n\n\n\n\n\n\n\nMinivan B has the highest predicted market share at 43.3%.\nMinivan C also have a decent amount of market share at 31.9%.\nMinivans with higher prices and non gas engines tend to have lower market shares."
  },
  {
    "objectID": "projects/MA/hw5_questions.html",
    "href": "projects/MA/hw5_questions.html",
    "title": "Segmentation Methods",
    "section": "",
    "text": "In this study, I employ clustering to identify natural groupings in the Iris dataset by utilizing the KMeans algorithm. This method reveals distinct clusters that share similar characteristics, helping us understand the inherent structure of the data. Subsequently, I will apply a Latent Class Multinomial Logit model on the Yogurt Dataset to delve deeper into the decision-making processes within these cluster groups, capturing the diverse preferences and behaviors across the dataset."
  },
  {
    "objectID": "projects/MA/hw5_questions.html#introduction",
    "href": "projects/MA/hw5_questions.html#introduction",
    "title": "Segmentation Methods",
    "section": "",
    "text": "In this study, I employ clustering to identify natural groupings in the Iris dataset by utilizing the KMeans algorithm. This method reveals distinct clusters that share similar characteristics, helping us understand the inherent structure of the data. Subsequently, I will apply a Latent Class Multinomial Logit model on the Yogurt Dataset to delve deeper into the decision-making processes within these cluster groups, capturing the diverse preferences and behaviors across the dataset."
  },
  {
    "objectID": "projects/MA/hw5_questions.html#k-means",
    "href": "projects/MA/hw5_questions.html#k-means",
    "title": "Segmentation Methods",
    "section": "K-Means",
    "text": "K-Means\n\nData Description\n\n\n\n\n\n\nIris Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                 \n\n Loading ITables v2.6.1 from the internet... (need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are 150 rows in this data\nThere are 5 columns in this data\n\n\n\n\nSpecies\nsetosa        50\nversicolor    50\nvirginica     50\nName: count, dtype: int64\n\n\nThere are 3 total species in this dataset, with each of them having 50 entries. ### KMeans Algorithm Overview\nK-means is an iterative clustering algorithm that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean. The algorithm works as follows:\n\nInitialization: Select k initial centroids randomly from the data points.\nAssignment step: Assign each data point to the closest centroid. The ‚Äúcloseness‚Äù is typically determined by the Euclidean distance between points.\nUpdate step: Recalculate the centroids as the mean of all data points assigned to that centroid‚Äôs cluster.\nConvergence Check: Repeat the assignment and update steps until the centroids do not change significantly, or a maximum number of iterations is reached.\n\n\nStep 1 - Initialization\nThe initialization step is crucial in the K-means algorithm as it sets the starting point for the clusters. In this step, k initial centroids are selected randomly from the data points. The choice of initial centroids can significantly affect the final clusters and convergence speed of the algorithm. There are various strategies for choosing these centroids. My implementation randomly pick k unique data points from the dataset.\n\ndef initialize_centroids(X, k):\n    indices = np.random.choice(X.shape[0], k, replace=False)\n    return X[indices]\n\n\n\nStep 2 - Assignment\nDuring the assignment step, each data point in the dataset is assigned to the nearest centroid. ‚ÄúNearest‚Äù in my implementation is defined using the Euclidean distance, although other distance metrics can also be used. This step partitions the input data into k clusters based on the minimal distance criterion.\n\ndef assign_clusters(X, centroids):\n    labels = np.zeros(len(X), dtype=int)  \n    \n    # Iterate over each data point\n    for i, point in enumerate(X):\n        min_distance = float('inf')  # Start with a very large number as the minimum distance\n        \n        # Compare distance of 'point' to each centroid\n        for idx, centroid in enumerate(centroids):\n            # Calculate Euclidean distance from point to centroid\n            distance = np.sqrt(np.sum((point - centroid) ** 2))\n            \n            # Update label for the point if a closer centroid is found\n            if distance &lt; min_distance:\n                min_distance = distance\n                labels[i] = idx\n    \n    return labels\n\n\n\nStep 3 - Update\nThe update step recalculates the position of each centroid based on the mean of the data points assigned to each cluster. The function update_centroids(X, labels, k) computes the new centroids by taking the mean of all points labeled with each respective cluster index. This approach is vectorized, leveraging NumPy‚Äôs array operations to efficiently compute the mean for each cluster. This step is crucial for moving the centroids to the center of their respective clusters, which is key to refining the cluster assignments in subsequent iterations.\n\ndef update_centroids(X, labels, k):\n    new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n    return new_centroids\n\n\n\nStep 4 - Convergence Check\nThe convergence check is where you actually implement the K-means algorithm, where the algorithm iteratively performs the assignment and update steps until the centroids no longer change significantly, indicating convergence, or until a specified maximum number of iterations is reached.\n\ndef k_means(X, k, max_iters=10, plot = True):\n    centroids = initialize_centroids(X, k)\n    for _ in range(max_iters):\n        labels = assign_clusters(X, centroids)\n        new_centroids = update_centroids(X, labels, k)\n        if plot:\n            plot_clusters(X, new_centroids, labels, title = f\"Iteration {int(_+1)}\")\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    return centroids, labels\n\n\n\n\nK-Means Implementation\nSince the Iris dataset have 3 distinct species, I will cluster the data into 3 groups. Below is the visualization of the algorithm as it computes the clusters\n\nk = 3\ncentroids, labels = k_means(X, k)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs seen above, the K means cluster converged after iteration 6, meaning that the positions of the centroids did not change significantly from the previous iteration. This convergence indicates that the algorithm has found a stable clustering of the data where further iterations would not lead to any further significant changes in the positions of the centroids.\n\n\nComparing with Sklearn K-Means\nBelow shows the implementation of Sklearn‚Äôs K-Means:\n\nfrom sklearn.cluster import KMeans\n\nsklearn_kmeans = KMeans(n_clusters=3)\nsklearn_kmeans.fit(X)\nsklearn_labels = sklearn_kmeans.labels_\nsklearn_centroids = sklearn_kmeans.cluster_centers_\n\nAnd the final cluster is shown below:\n\n\n\n\n\n\n\n\n\nAnd this is the final cluster from my own K-means algorithm:\n\n\n\n\n\n\n\n\n\nAs seen above, the clustering are very similar.\nNote: This is with np.random.seed set to 42, with a different seed, it is likely to get different clustering results.\n\n\nWithin - Cluster Sum of Squares and Silhouette Scores\nFor this portion, I will be computing both with my own custom function.\nThe WCSS and Silhouette Scores as the number of clusters increase is shown below:\n\n\n\n\n\n\n\n\n\n\n\nAnalysis\n\nWCSS (Within-Cluster Sum of Squares): The WCSS decreases as the number of clusters increases. This plot also shows a potential elbow around k=3, indicating that adding more clusters beyond this point results in diminishing returns in terms of WCSS reduction.\nSilhouette Score: This metric peaks noticeably at k=2, suggesting that the clusters are most distinct and well-separated at this point. The silhouette scores for higher values of k are lower, indicating less optimal clustering in terms of inter-cluster separation and intra-cluster cohesion.\n\n\n\nConclusion\nBoth the Within-Cluster Sum of Squares (WCSS) and the Silhouette Score provide valuable insights into the optimal number of clusters for the Iris dataset. The WCSS shows a clear elbow at k = 3, suggesting that increasing the number of clusters beyond three leads to diminishing returns in terms of variance reduction. On the other hand, the Silhouette Score peaks at k = 2, indicating excellent separation and cohesion at two clusters.\nHowever, given the context of the Iris dataset, which comprises three distinct species, opting for three clusters is particularly meaningful. This choice not only aligns with the natural divisions in the data but also matches the biological expectation, thereby providing a scientifically coherent clustering solution."
  },
  {
    "objectID": "projects/MA/hw5_questions.html#latent-class-mnl",
    "href": "projects/MA/hw5_questions.html#latent-class-mnl",
    "title": "Segmentation Methods",
    "section": "Latent-Class MNL",
    "text": "Latent-Class MNL\nIn this portion, I explore a Latent-Class MNL model on the yogurt dataset that I used previously here.\n\nLikelihood for the Latent Class Multi-nomial Logit (LC-MNL) Model\nIn the Latent Class Multinomial Logit (LC-MNL) model, we extend the standard MNL model by incorporating latent heterogeneity among consumers. Here, instead of assuming all individuals derive utility from product characteristics in the same way, we assume there are \\(K\\) distinct classes of consumers, each with its own unique set of preferences.\n\nModel Setup\nLet‚Äôs denote: - \\(i = 1, \\ldots, n\\): index for consumers. - \\(j = 1, \\ldots, J\\): index for products. - \\(k = 1, \\ldots, K\\): index for latent classes.\nEach consumer \\(i\\) belongs to one latent class \\(k\\), but this class membership is unobserved. We assume: - Each class \\(k\\) has its own parameter vector \\(\\beta_k\\), reflecting distinct preferences. - \\(z_{ik}\\) is an indicator variable that equals 1 if consumer \\(i\\) is in class \\(k\\) and 0 otherwise.\n\n\nUtility Function\nFor consumer \\(i\\) choosing product \\(j\\) while belonging to class \\(k\\), the utility is given by: \\[ U_{ijk} = x_j' \\beta_k + \\epsilon_{ijk} \\] where \\(\\epsilon_{ijk}\\) is an i.i.d. extreme value error term, similar to the MNL model.\n\n\nProbability of Choice\nThe probability that consumer \\(i\\) in class \\(k\\) chooses product \\(j\\) is: \\[ \\mathbb{P}_{ik}(j) = \\frac{e^{x_j' \\beta_k}}{\\sum_{l=1}^J e^{x_l' \\beta_k}} \\]\n\n\nLatent Class Probabilities\nWe also model the probability that any consumer belongs to class \\(k\\), potentially as a function of some observed characteristics \\(w_i\\) of the consumer: \\[ \\pi_k(w_i; \\gamma) = \\frac{e^{w_i' \\gamma_k}}{\\sum_{m=1}^K e^{w_i' \\gamma_m}} \\] where \\(\\gamma\\) are parameters to be estimated that influence class membership.\n\n\nIndividual Likelihood Function\nThe individual likelihood function for consumer \\(i\\), taking into account the uncertainty about class membership, is: \\[ L_i(\\beta_1, \\ldots, \\beta_K; \\gamma) = \\sum_{k=1}^K \\pi_k(w_i; \\gamma) \\prod_{j=1}^J \\mathbb{P}_{ik}(j)^{\\delta_{ij}} \\] Here, the likelihood is a weighted sum of the probabilities of choosing each product across all classes, weighted by the probability of belonging to each class.\n\n\nJoint Likelihood Across All Consumers\nThe joint likelihood across all consumers is the product of all individual likelihoods: \\[ L_n(\\beta_1, \\ldots, \\beta_K; \\gamma) = \\prod_{i=1}^n L_i(\\beta_1, \\ldots, \\beta_K; \\gamma) \\]\n\n\nLog-Likelihood Function\nThe joint log-likelihood function, which is more practical for estimation due to numerical stability, is: \\[ \\ell_n(\\beta_1, \\ldots, \\beta_K; \\gamma) = \\sum_{i=1}^n \\log \\left( \\sum_{k=1}^K \\pi_k(w_i; \\gamma) \\prod_{j=1}^J \\mathbb{P}_{ik}(j)^{\\delta_{ij}} \\right) \\]\n\n\nKey Differences from MNL\n\nConsumer Heterogeneity: LC-MNL accounts for different consumer preferences by classifying them into latent classes with distinct utility parameters.\nComplexity in Estimation: Estimation involves both the coefficients that describe how product characteristics affect utility and the parameters that govern class membership.\nModel Flexibility: LC-MNL provides a richer understanding of market segmentation and can reveal niche markets or consumer segments that might be obscured in a standard MNL model.\n\nThe LC-MNL model thus offers a robust framework for understanding consumer choice behavior when there are unobserved segments in the market, each with unique preferences, which can be particularly valuable in targeted marketing and product positioning strategies. ### Estimation\nThe latent class log likelihood function I will be using is shown below:\n\ndef log_likelihood_lc(params, data, num_classes):\n    # Split params into class probabilities (in log scale for optimization) and class-specific beta coefficients\n    class_log_probs = params[:num_classes]\n    class_probs = np.exp(class_log_probs - np.max(class_log_probs))  # To prevent numerical overflow\n    class_probs /= np.sum(class_probs)  # Normalize to make probabilities sum to 1\n    betas = params[num_classes:].reshape((num_classes, 5))  # Reshape into class-specific params\n    \n    total_log_likelihood = 0\n    for k in range(num_classes):\n        beta = betas[k]\n        data['utility'] = (beta[0] * data['yogurt 1'] + \n                           beta[1] * data['yogurt 2'] + \n                           beta[2] * data['yogurt 3'] + \n                           beta[3] * data['featured'] + \n                           beta[4] * data['price'])\n        data['exp_utility'] = np.exp(data['utility'])\n        data['sum_exp_utility'] = data.groupby('id')['exp_utility'].transform('sum')\n        data['probability'] = data['exp_utility'] / data['sum_exp_utility']\n        \n        # Compute the log-likelihood for this class\n        class_log_likelihood = data['chosen'] * np.log(data['probability'] + 1e-10)  # Adding small number to avoid log(0)\n        # Weight by class membership probability\n        total_log_likelihood += class_probs[k] * class_log_likelihood.sum()\n\n    return -total_log_likelihood\n\nWith the function defined, I then used scipy.optimize from Python to find the beta values for the 5 parameters and for both class (only 2 latent classes). First, I initilized all the guesses as zero, then I used the minimize() fucntion from scipy.optimize to find the beta values. The values are shown below.\n\n\nClass Probabilities: [0.5 0.5]\nParameters for Class 1: Beta_1 = 1.3877386121518487 , Beta_2 = 0.6435095603213673 , Beta_3 = -3.086078162965555 , Beta_f = 0.4874455398436589 , Beta_p = -37.05704289446939\nParameters for Class 2: Beta_1 = 1.3877386121518487 , Beta_2 = 0.6435095603213673 , Beta_3 = -3.086078162965555 , Beta_f = 0.4874455398436589 , Beta_p = -37.05704289446939\n\n\nIt appears that both class have the same beta values, potentially indicating indistinguishable class characterstics.\n\n\n\nComparing to Original MNL\nThe original parameters from the previous post was:\nEstimated parameters: beta_1: 1.3877520023064622\nbeta_2: 0.6435049292323878\nbeta_3: -3.0861128992213978\nbeta_f: 0.48741409137900876\nbeta_p: -37.0578717065307\nComparing to the latent class model, it appears that the parameters are very similar, with very slight differences only.\n\n\nNumber of Class\nTo determine the optimal number of latent classes to use, I fit the Latent Class Multinomial Logit (LC-MNL) model for a range of class numbers (2, 3, ‚Ä¶, K) and evaluate each model using the Bayesian Information Criterion (BIC).\nThe BIC is a criterion for model selection among a finite set of models; the model with the lowest BIC is preferred. It is calculated as:\n\\[ \\text{BIC} = -2 \\cdot \\ell + p \\cdot \\log(n) \\]\nwhere: - ( ) is the log-likelihood of the model. - ( p ) is the number of estimated parameters. - ( n ) is the number of observations.\nThe BIC penalizes models with more parameters to prevent overfitting, balancing model complexity with goodness of fit. By computing the BIC for models with different numbers of latent classes, we can identify the model that best explains the data without unnecessary complexity.\nIn this analysis, I fit the LC-MNL model for 2 to 7 classes and computed the BIC for each model. The results is shown below:\n\ndef calculate_bic(log_likelihood, num_params, num_obs):\n    return -2 * log_likelihood + num_params * np.log(num_obs)\n\ndef fit_lc_mnl_model(data, max_classes=7):\n    num_obs = data['id'].nunique()\n    num_features = 5  # beta1, beta2, beta3, beta_f, beta_p\n    params_values = {}\n    bic_values = []\n    for num_classes in range(2, max_classes + 1):\n        initial_params = np.zeros(num_classes + num_classes * num_features)\n        result = minimize(log_likelihood_lc, initial_params, args=(data, num_classes), method='L-BFGS-B')\n        \n        if result.success:\n            log_likelihood = -result.fun\n            num_params = num_classes + num_classes * num_features\n            bic = calculate_bic(log_likelihood, num_params, num_obs)\n            bic_values.append((num_classes, bic))\n            params_values[num_classes] = result.x\n\n        else:\n            print(f\"Classes: {num_classes} failed to converge.\")\n    \n    return bic_values, params_values\n\nbic_values, params_values = fit_lc_mnl_model(yogurt_data)\n\noptimal_num_classes = min(bic_values, key=lambda x: x[1])\nbic_values, optimal_num_classes\n\n([(2, 5410.661151671423),\n  (3, 5457.435031702688),\n  (4, 5504.208910689819),\n  (5, 5550.982794165628),\n  (6, 5597.756685952132),\n  (7, 5644.530555504753)],\n (2, 5410.661151671423))\n\n\nThe results indicate that the model with 2 latent classes has the lowest BIC value, suggesting it is the optimal number of classes for our dataset.\nBelow are the optimal parameters:\n\n\nOptimal number of classes: 2\nClass 1 Probabilities: 0.5\nParameters for Class 1:\n  Beta_1 = 1.3877386121518487\n  Beta_2 = 0.6435095603213673\n  Beta_3 = -3.086078162965555\n  Beta_f = 0.4874455398436589\n  Beta_p = -37.05704289446939\n\nClass 2 Probabilities: 0.5\nParameters for Class 2:\n  Beta_1 = 1.3877386121518487\n  Beta_2 = 0.6435095603213673\n  Beta_3 = -3.086078162965555\n  Beta_f = 0.4874455398436589\n  Beta_p = -37.05704289446939\n\n\n\n\n\nDiscussion\nFrom the beta intercept values, it appears that for both class:\n\nYogust 1 is the most preferred as (\\(\\beta_1\\)) has the largest positive value\nYogurt 2 is also second most preferred as (\\(\\beta_2\\)) is also positive. However, it is less preferred compared to Yogurt 1\nYogurt 3 is the least preferred, as suggested by the negative (\\(\\beta_3\\))\nYogurt 4 is third preferred. As it is the base comparison and there are 2 yogurts with positive coefficients and 1 yogurt with negative coefficient, making yogurt 4 the third preferred\n(\\(\\beta_f\\)) having a positive value of 0.487 indicates that featuring a yogurt increases its utility and thus its chance of being selected\n(\\(\\beta_p\\)) having a large negative value of 37.057 indicates higher prices reduces the chance of a yogurt being selected"
  },
  {
    "objectID": "projects/MA/Untitled2.html",
    "href": "projects/MA/Untitled2.html",
    "title": "Joshua's Website",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_stata('karlan_list_2007.dta')\n\n\nimport matplotlib.pyplot as plt\n\n\nproportions = df.groupby('treatment')['gave'].mean().reset_index()\n\n# Create a bar plot\nplt.bar(proportions['treatment'], proportions['gave'], tick_label=['Control', 'Treatment'])\nplt.ylabel('Proportion of People Who Donated')\nplt.title('Proportion of Donations by Group')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy import stats\nimport statsmodels.api as sm\ntreatment_donated = df[df['treatment'] == 1]['gave']\ncontrol_donated = df[df['treatment'] == 0]['gave']\n\n# Perform a t-test\nt_stat, p_value = stats.ttest_ind(treatment_donated, control_donated)\nprint(f\"T-test results: t-statistic = {t_stat}, p-value = {p_value}\")\n\n# Perform a linear regression\ndf['intercept'] = 1\nmodel = sm.OLS(df['gave'], df[['intercept', 'treatment']])\nresults = model.fit()\nprint(\"Linear regression results:\")\nprint(results.summary())\n\nT-test results: t-statistic = 3.101361000543946, p-value = 0.0019274025949016982\nLinear regression results:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Thu, 11 Apr 2024   Prob (F-statistic):            0.00193\nTime:                        11:55:08   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\ndf['intercept'] = 1\n\n# Define the model and fit it\nprobit_model = sm.Probit(df['gave'], df[['intercept', 'treatment']])\nprobit_results = probit_model.fit()\n\n# Print the summary of the regression results\nprint(probit_results.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Thu, 11 Apr 2024   Pseudo R-squ.:               0.0009783\nTime:                        12:04:20   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\n\ndf\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\nintercept\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n1\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n1\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n1\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n1\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n1\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n1\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n1\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n1\n\n\n\n\n50083 rows √ó 52 columns\n\n\n\n\n\nmatch21 = df[df['ratio'] == 2]['gave']\nmatch31 = df[df['ratio']== 3]['gave']\nmatch11 = df[df['ratio'] == 1]['gave']\n\nt_stat, p_value = stats.ttest_ind(match31, match21)\nprint(f\"T-test results: t-statistic = {t_stat}, p-value = {p_value}\")\nt_stat, p_value = stats.ttest_ind(match11, match31)\nprint(f\"T-test results: t-statistic = {t_stat}, p-value = {p_value}\")\nt_stat, p_value = stats.ttest_ind(match11, match21)\nprint(f\"T-test results: t-statistic = {t_stat}, p-value = {p_value}\")\n\nT-test results: t-statistic = 0.05011583793874515, p-value = 0.9600305283739325\nT-test results: t-statistic = -1.0150255853798622, p-value = 0.3101046637086672\nT-test results: t-statistic = -0.96504713432247, p-value = 0.33453168549723933\n\n\n\nprobit_model = sm.OLS(df['gave'], df[['intercept', 'ratio2', 'ratio3']])\nprobit_results = probit_model.fit()\nprint(probit_results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     4.117\nDate:                Thu, 11 Apr 2024   Prob (F-statistic):             0.0163\nTime:                        12:16:26   Log-Likelihood:                 26629.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50080   BIC:                        -5.323e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0190      0.001     22.306      0.000       0.017       0.021\nratio2         0.0036      0.002      2.269      0.023       0.000       0.007\nratio3         0.0037      0.002      2.332      0.020       0.001       0.007\n==============================================================================\nOmnibus:                    59815.856   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317637.927\nSkew:                           6.741   Prob(JB):                         0.00\nKurtosis:                      46.443   Cond. No.                         3.16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "projects/project1/coord.html",
    "href": "projects/project1/coord.html",
    "title": "Coordinate Descent",
    "section": "",
    "text": "The goal of this project is to come up with a new coordinate descent method. Coordinate descent simplifies optimization problems by iteratively focusing on one coordinate at a time, making it efficient for large-scale and high-dimensional datasets. Its simplicity and adaptability to different problem structures lead to faster convergence and reduced computational costs.\nTo test the efficiency of my coordinate descent method, I used the wine dataset on UCI repo and only focused on the first and second class, making it a binary classification probllem."
  },
  {
    "objectID": "projects/project1/coord.html#algorithm-1",
    "href": "projects/project1/coord.html#algorithm-1",
    "title": "Coordinate Descent",
    "section": "Algorithm 1",
    "text": "Algorithm 1\nOnly the coordinate with the largest gradient (i.e., the steepest slope) is selected for optimization in each iteration.\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\n\ndef coordinate_descent_lr(X, y, method='largest_gradient', num_iterations=100, learning_rate=0.01):\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    b = 0\n    losses = []\n\n    for _ in range(num_iterations):\n        for i in range(n_features):\n            if method == 'random':\n                i = np.random.randint(n_features)  # Random feature for random-feature coordinate descent\n            else:\n                y_pred = sigmoid(np.dot(X, w) + b)\n                gradients = -np.dot(X.T, (y-y_pred)) / n_samples\n                i = np.argmax(np.abs(gradients))\n                \n            feature = X[:, i]\n            y_pred = sigmoid(np.dot(X, w) + b)\n\n            # Gradient calculation\n            grad_wi = -np.dot(feature, (y - y_pred)) / n_samples\n            grad_b = -np.mean(y - y_pred)\n\n            # Update weights\n            w[i] -= learning_rate * grad_wi\n            b -= learning_rate * grad_b\n\n        # Record the loss after each full iteration over features\n        losses.append(log_loss(y, sigmoid(np.dot(X, w) + b)))\n\n    return w, b, losses"
  },
  {
    "objectID": "projects/project1/coord.html#algorithm-2",
    "href": "projects/project1/coord.html#algorithm-2",
    "title": "Coordinate Descent",
    "section": "Algorithm 2",
    "text": "Algorithm 2\nRandomly choose a single coordinate to optimize in each iteration Note: Code is the same as above, change method to random instead."
  },
  {
    "objectID": "projects/project1/coord.html#simple-update",
    "href": "projects/project1/coord.html#simple-update",
    "title": "Coordinate Descent",
    "section": "Simple Update",
    "text": "Simple Update\nAt each step, weights is updated with the formula \\(w_i \\leftarrow w_i - \\alpha \\frac{\\partial L}{\\partial w_i}\\), where \\(\\frac{\\partial L}{\\partial w_i}\\) is the partial derivative of loss function L with respect to \\(w_i\\) ## Backtracking Line Search Back tracking line search adatively adjusts the learning rate, ensuring each step in the optimization process is sufficiently large to make progress yet small enough to avoid overshooting minimum.\n\ndef backtracking_line_search(X, y, w, b, grad_wi, i, initial_lr=1, beta=0.8, c=1e-4):\n    lr = initial_lr\n    current_loss = log_loss(y, sigmoid(np.dot(X, w) + b))\n    updated_w = w.copy()\n    \n    while True:\n        updated_w[i] = w[i] - lr * grad_wi\n        new_loss = log_loss(y, sigmoid(np.dot(X, updated_w) + b))\n        if new_loss &lt;= current_loss - c * lr * grad_wi**2:\n            break\n        lr *= beta\n    \n    return lr\n\nImplementing it into Coordinate Descent algorithm above gives:\n\ndef coordinate_descent_lr2(X, y, method='largest_gradient', num_iterations=100):\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    b = 0\n    losses = []\n\n    for _ in range(num_iterations):\n        for i in range(n_features):\n            if method == 'largest_gradient':\n                y_pred = sigmoid(np.dot(X, w) + b)\n                gradients = -np.dot(X.T, (y - y_pred)) / n_samples\n                i = np.argmax(np.abs(gradients))\n            else:\n                i = np.random.randint(n_features)\n\n            feature = X[:, i]\n            y_pred = sigmoid(np.dot(X, w) + b)\n            grad_wi = -np.dot(feature, (y - y_pred)) / n_samples\n            grad_b = -np.mean(y - y_pred)\n\n            # Backtracking line search to find learning rate\n            learning_rate_wi = backtracking_line_search(X, y, w, b, grad_wi, i)\n\n            # Update weights\n            w[i] -= learning_rate_wi * grad_wi\n            b -= learning_rate_wi * grad_b\n\n        losses.append(log_loss(y, sigmoid(np.dot(X, w) + b)))\n\n    return w, b, losses"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "projects.html#machine-learning",
    "href": "projects.html#machine-learning",
    "title": "My Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "projects.html#natural-language-processing",
    "href": "projects.html#natural-language-processing",
    "title": "My Projects",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing\n\n\n\n\n\n\n\n\n\n\nRestaurant Category Prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html#marketing-analytics",
    "href": "projects.html#marketing-analytics",
    "title": "My Projects",
    "section": "Marketing Analytics",
    "text": "Marketing Analytics\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nJoshua Chen\n\n\nDec 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nJoshua Chen\n\n\nDec 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Examples\n\n\n\n\n\n\nJoshua Chen\n\n\nDec 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nKey Drivers Analysis\n\n\n\n\n\n\nJoshua Chen\n\n\nDec 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSegmentation Methods\n\n\n\n\n\n\nJoshua Chen\n\n\nDec 24, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  }
]