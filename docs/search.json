[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joshua’s Website",
    "section": "",
    "text": "Welcome to my website! I am an aspiring data scientist actively seeking internships/jobs in the fields of AI, ML, Data Science, and Data Analytics."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Joshua Chen’s Resume",
    "section": "",
    "text": "This is my general resume:\nDownload PDF file.\nThis is my NLP/DL specific resume\nDownload PDF file.\nThis is my Machine Learning specific resume\nDownload PDF file."
  },
  {
    "objectID": "resume.html#resume",
    "href": "resume.html#resume",
    "title": "Joshua Chen’s Resume",
    "section": "Resume",
    "text": "Resume\nfiles/Joshua_Chen_Resume_General.pdf"
  },
  {
    "objectID": "resume.html#this-is-my-current-resume",
    "href": "resume.html#this-is-my-current-resume",
    "title": "Joshua Chen’s Resume",
    "section": "",
    "text": "Resume\nfiles/Joshua_Chen_Resume_General.pdf"
  },
  {
    "objectID": "resume.html#about-this-site",
    "href": "resume.html#about-this-site",
    "title": "Joshua Chen’s Resume",
    "section": "",
    "text": "This is what there is right now."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "index.html#current",
    "href": "index.html#current",
    "title": "Joshua’s Website",
    "section": "Current",
    "text": "Current\n\nTeaching Assistant | UCSD Cognitive Science Department\n\nCogs 108\n\nMachine Learning Capstone Intern | AlphaTrAI"
  },
  {
    "objectID": "index.html#work-history",
    "href": "index.html#work-history",
    "title": "Joshua’s Website",
    "section": "Work History",
    "text": "Work History\n\nAlphaTrAI | 2023 - Present\nTeaching Assistant | 2023 - Present\n\nCogs 108 Data Science in Practice\nCogs 14a Introduction to Research Methods\n\nInstructional Assistant | 2022 - 2023\n\nCogs 18 Introduction to Python\nCogs 1 Introductoi to Cognitive Science"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Joshua’s Website",
    "section": "Education",
    "text": "Education\n\nMS Business Analytics | UC San Diego 2024\nBS Cognitive Science w/ Spec. in Machine Learning | UC San Diego 2023\n\nMagna Cum Laude"
  },
  {
    "objectID": "projects.html#posts",
    "href": "projects.html#posts",
    "title": "My Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "projects.html#summaries",
    "href": "projects.html#summaries",
    "title": "My Projects",
    "section": "Summaries",
    "text": "Summaries\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html#machine-learning",
    "href": "projects.html#machine-learning",
    "title": "My Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "projects.html#marketing-analytics",
    "href": "projects.html#marketing-analytics",
    "title": "My Projects",
    "section": "Marketing Analytics",
    "text": "Marketing Analytics\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nJoshua Chen\n\n\nMay 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nJoshua Chen\n\n\nMay 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Examples\n\n\n\n\n\n\nJoshua Chen\n\n\nMay 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nKey Drivers Analysis\n\n\n\n\n\n\nJoshua Chen\n\n\nMay 30, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Sample Project",
    "section": "",
    "text": "import numpy as np\n\n# Use the numpy package\narray = np.array([1, 2, 3, 4, 5])\nprint(array)\n\n[1 2 3 4 5]"
  },
  {
    "objectID": "projects/project1/NFL Career Length Predictor.html",
    "href": "projects/project1/NFL Career Length Predictor.html",
    "title": "Predicting NFL PLayers’ Career Length",
    "section": "",
    "text": "This project will analyze data containing NFL player’s draft data and other factors in hopes of creating a machine learning model that will successfully predict a player’s career length. With this data, I also performed a t test to see if 1-2 round draft picks also have a higher career length than 3-5 round picks and 6-7 round picks + undrafted players. These results indicated that 1-2 round draft picks do indeed have a higher career length. However, I was unable to fine tune a model that accurately predicts career length. Future studies should also factor in college data, which this project was unable to do due to the complexities and challenges associated with accessing and integrating college data."
  },
  {
    "objectID": "projects/project1/NFL Career Length Predictor.html#note-the-years-of-experience-is-under-the-to-column",
    "href": "projects/project1/NFL Career Length Predictor.html#note-the-years-of-experience-is-under-the-to-column",
    "title": "Predicting NFL PLayers’ Career Length",
    "section": "Note: The years of experience is under the ‘To’ Column",
    "text": "Note: The years of experience is under the ‘To’ Column\nFirst we will do a descriptive analysis of the dataset:\n\ndf.describe()\n\nIt appears that the wonderlic score is only available for a select few draftees, upon further research, we realized that wonderlic score is only measured for QBs, therefore, we will be dropping that column from now on.\n\ndf = df.drop(columns = 'Wonderlic')\n\nThen, we take a look at the target variable, years of experience:\n\nplt.hist(df['To'], label = 'Years of Experience')\nplt.title(\"Histogram of Years of Experience\")\nplt.xlabel('Years')\nplt.ylabel('Count')\nplt.show()\n\nFrom this histogram, it appears the target variable is skewed right.\nSince we will be attempting to build a machine learning model, this indicates that we should apply either logarithmic transformation or scaling/normaling to the years of experience in order to imporve the model’s performance.\nNext, we investigate whether or not the years of experience differ for positions.\n\n# Potentially add the count of each position in here\nscores=df[['To','POS']]\nax = scores.boxplot(by='POS', meanline=True, showmeans=True, showcaps=True, \n                showbox=True, showfliers=False, return_type='axes', figsize = (10, 5))\na2 = df[['To','POS']]\na2.boxplot(by='POS', meanline=True, showmeans=True, showcaps=True, \n           showbox=True, showfliers=False, ax=ax)\nplt.ylabel('Years')\nplt.show()\n\nFrom the boxplots, we see that QB and K tend to have the highest career length, while defensive linemen tend to have shorter career length (DL, LB, LS, EDG)\nSurprisingly, one position, OL, does not seem to have a box plot. We are going to investigate for that position.\n\ndf[df['POS'] == 'OL']\n\nIt appears that only one player is listed as an OL, we will be removing this player from the dataset.\n\ndf = df[df['POS'] !='OL']"
  },
  {
    "objectID": "projects/project1/NFL Career Length Predictor.html#eda-height",
    "href": "projects/project1/NFL Career Length Predictor.html#eda-height",
    "title": "Predicting NFL PLayers’ Career Length",
    "section": "EDA Height",
    "text": "EDA Height\nHere we take a look at the distribution of the heights:\n\nplt.hist(df['Height (in)'], label = 'Height')\nplt.title(\"Histogram of Height\")\nplt.xlabel('Height')\nplt.ylabel('Count')\nplt.show()\n\nWe see the height is normally distributed around 73.5in, without any clear outliers."
  },
  {
    "objectID": "projects/project1/NFL Career Length Predictor.html#eda-weight",
    "href": "projects/project1/NFL Career Length Predictor.html#eda-weight",
    "title": "Predicting NFL PLayers’ Career Length",
    "section": "EDA Weight",
    "text": "EDA Weight\nHere we take a look at the distribution of the weights:\n\nplt.hist(df['Weight (lbs)'], label = 'Weight')\nplt.title(\"Histogram of Weight\")\nplt.xlabel('Weight')\nplt.ylabel('Count')\nplt.show()\n\nHere we see the weights are somewhat of a bimodal distribution, possibly because linemen positions tend to be heavier than other positions.\n\nscores=df[['Weight (lbs)','POS']]\nax = scores.boxplot(by='POS', meanline=True, showmeans=True, showcaps=True, \n                showbox=True, showfliers=False, return_type='axes', figsize = (10, 5))\na2 = df[['Weight (lbs)','POS']]\na2.boxplot(by='POS', meanline=True, showmeans=True, showcaps=True, \n           showbox=True, showfliers=False, ax=ax)\nplt.ylabel('Weight')\nplt.show()"
  },
  {
    "objectID": "projects/project1/NFL Career Length Predictor.html#eda-40-yard",
    "href": "projects/project1/NFL Career Length Predictor.html#eda-40-yard",
    "title": "Predicting NFL PLayers’ Career Length",
    "section": "EDA 40 Yard",
    "text": "EDA 40 Yard\nDistribution of 40 yard time:\n\nplt.hist(df['40 Yard'], label = '40 Yard')\nplt.title(\"Histogram of 40 Yard Time\")\nplt.xlabel('Time (s)')\nplt.ylabel('Count')\nplt.show()\n\nSkewed right, possibly becuase linemen tend to have slower times, no clear outliers.\n\nscores=df[['40 Yard','POS']]\nax = scores.boxplot(by='POS', meanline=True, showmeans=True, showcaps=True, \n                showbox=True, showfliers=False, return_type='axes', figsize = (10, 5))\na2 = df[['40 Yard','POS']]\na2.boxplot(by='POS', meanline=True, showmeans=True, showcaps=True, \n           showbox=True, showfliers=False, ax=ax)\nplt.ylabel('40 Yard')\nplt.show()"
  },
  {
    "objectID": "projects/project1/NFL Career Length Predictor.html#eda-vertical-leap",
    "href": "projects/project1/NFL Career Length Predictor.html#eda-vertical-leap",
    "title": "Predicting NFL PLayers’ Career Length",
    "section": "EDA Vertical Leap",
    "text": "EDA Vertical Leap\n\nplt.hist(df['Vert Leap (in)'], label = 'Vert Leap (in)')\nplt.title(\"Histogram of Vertical Leap\")\nplt.xlabel('inches')\nplt.ylabel('Count')\nplt.show()\n\nThe players’ vertical appears to be a normal distribution."
  },
  {
    "objectID": "projects/project1/NFL Career Length Predictor.html#eda-bench-press",
    "href": "projects/project1/NFL Career Length Predictor.html#eda-bench-press",
    "title": "Predicting NFL PLayers’ Career Length",
    "section": "EDA Bench Press",
    "text": "EDA Bench Press\n\nplt.hist(df['Bench Press'], label = 'Bench')\nplt.title(\"Histogram of Bench Press\")\nplt.xlabel('Repetitions')\nplt.ylabel('Count')\nplt.show()\n\nThe players’ bench press appears to be a normal distribution."
  },
  {
    "objectID": "projects/project1/NFL Career Length Predictor.html#eda-shuttle",
    "href": "projects/project1/NFL Career Length Predictor.html#eda-shuttle",
    "title": "Predicting NFL PLayers’ Career Length",
    "section": "EDA Shuttle",
    "text": "EDA Shuttle\n\nplt.hist(df['Shuttle'], label = 'Shuttle')\nplt.title(\"Histogram of Shuttle\")\nplt.xlabel('time (s)')\nplt.ylabel('Count')\nplt.show()\n\nThe players’ shuttle appears to be somewhat of a normal distribution."
  },
  {
    "objectID": "projects/project1/NFL Career Length Predictor.html#eda-3cone",
    "href": "projects/project1/NFL Career Length Predictor.html#eda-3cone",
    "title": "Predicting NFL PLayers’ Career Length",
    "section": "EDA 3cone",
    "text": "EDA 3cone\n\nplt.hist(df['3Cone'], label = '3Cone')\nplt.title(\"Histogram of 3Cone\")\nplt.xlabel('time (s)')\nplt.ylabel('Count')\nplt.show()\n\nThe players’ 3Cone appears to be somewhat of a normal distribution."
  },
  {
    "objectID": "projects/project1/NFL Career Length Predictor.html#eda-age",
    "href": "projects/project1/NFL Career Length Predictor.html#eda-age",
    "title": "Predicting NFL PLayers’ Career Length",
    "section": "EDA Age",
    "text": "EDA Age\n\ndf['Age'].value_counts().plot(kind = 'bar')\nplt.title('Barplot of Age')\nplt.xlabel('Years')\nplt.ylabel('Count')\nplt.show()"
  },
  {
    "objectID": "projects/project1/NFL Career Length Predictor.html#overall-eda",
    "href": "projects/project1/NFL Career Length Predictor.html#overall-eda",
    "title": "Predicting NFL PLayers’ Career Length",
    "section": "Overall EDA",
    "text": "Overall EDA\nWe decided to do a heatmap on all numeric variables and target output:\n\nimport seaborn as sns\nplt.figure(figsize=(12, 12))\nnumeric_cols = ['Height (in)', 'Weight (lbs)', '40 Yard', 'Bench Press', 'Vert Leap (in)', 'Broad Jump (in)', 'Shuttle', '3Cone', 'To', 'Age', 'Pick']\ncorr_numeric = df[numeric_cols].corr()\nsns.heatmap(corr_numeric, cmap='YlGnBu', annot=True, square=True)\n\nIt appears that none of the numeric columns are highly correlated with the target output, years of experience.\n\nfig = pd.plotting.scatter_matrix(df)\nplt.figure(figsize=(16, 16))\nfor ax in fig.ravel():\n    ax.xaxis.label.set_rotation(90)\n    ax.xaxis.label.set_ha('right')\n    ax.xaxis.label.set_va('center')\n\n# Rotate y-axis labels\nfor ax in fig.ravel():\n    ax.yaxis.label.set_rotation(360)\n    ax.yaxis.label.set_ha('right')\n    ax.yaxis.label.set_va('center')\n\nplt.show()\n\nFrom the scatterplots, it appears that while certain variables appear to have somewhat of a correlation with another variables. However, none of them seems to have a clear correlation with years of experience."
  },
  {
    "objectID": "projects/project1/NFL Career Length Predictor.html#t-test",
    "href": "projects/project1/NFL Career Length Predictor.html#t-test",
    "title": "Predicting NFL PLayers’ Career Length",
    "section": "T test",
    "text": "T test\nWe aim to conduct an inference between the years of experience of 1-2 round draft picks and 3-5 round and 6-7 + undrafted players to determine which round’s player have higher years of experience.\nStudies have shown that players drafted in 1-2 rounds usually play longer than 3-5, which also play longer than 6-7 round picks.\nNote: A lot of teams sign undrafted players to play for a few games, therefore we will not be including the data for undrafted players\nUndrafted players have been assigned a draft pick of 0; from here on, we will assign undrafted players to a pick number of 300. (Each year’s draft all have less than 300 picks).\n\ndef update_pick(x):\n  if int(x) == 0:\n    return 300\n  else:\n    return int(x)\n\n\ndf['Pick'] = df['Pick'].apply(update_pick)\n\nEven though undrafted players are not part of the t-tests, we decided to take a look at the distribution of the years of experience.\n\nund = df[df['Pick'] == 300]\nplt.hist(und['To'], bins = len(und['To'].unique()))\nplt.title('Distribution of undrafted')\n\nEach round have 32 picks. So the top 2 rounds should have 64 picks.\n\ntop2 = df[df['Pick'] &lt;= 64] # dataframe for 1-2 rounders\nmid3 = df[(df['Pick'] &gt; 64) & (df['Pick'] &lt;= 160)] # dataframe for 3-5 rounders\nbot3 = df[(df['Pick'] &gt;160) & (df['Pick'] &lt; 300)] # dataframe for all other\n\n\ntop2.shape\n\n\nmid3.shape\n\n\nbot3.shape\n\nHere we see the distribution of the years of experience for both groups.\n\nplt.hist(top2['To'], bins = len(top2['To'].unique()))\nplt.title('Distribution of top 2 round')\n\n\nplt.hist((mid3['To']), bins = len(mid3['To'].unique()))\nplt.title('Distribution of 3-5 round')\n\n\nplt.hist((bot3['To']), bins = len(bot3['To'].unique()))\nplt.title('Distribution of 6-7 round')\n\nWe see that the distribution of the top 2 rounds is approximately normal, while the distribution of 3-5 round is slightly skewed right, while the distribution of 6-7 round is skewed right.\n\navg_y1 = top2['To'].mean()\navg_y2 = mid3['To'].mean()\navg_y3 = bot3['To'].mean()\n\n\nprint('Average years of experience of 1-2 rounder is ' + str(avg_y1))\nprint('Average years of experience of 3-5 rounder is ' + str(avg_y2))\nprint('Average years of experience of 6-7 rounder is ' + str(avg_y3))\n\n\nfrom scipy.stats import ttest_ind, chisquare, normaltest\n\n\nt-test of 1-2 round and 3-5 round\n\nt_val, p_val = ttest_ind(top2['To'], mid3['To'])\n\n\nif p_val &lt; 0.01:\n    print('There is a significant difference!')\nelse:\n    print('There is NOT a significant difference!')\n\n\n\nt-test of 1-2 round and 6-7 round\n\nt_val, p_val = ttest_ind(top2['To'], bot3['To'])\n\n\nif p_val &lt; 0.01:\n    print('There is a significant difference!')\nelse:\n    print('There is NOT a significant difference!')\n\n\n\nt-test of 3-5 round and 6-7 round\n\nt_val, p_val = ttest_ind(mid3['To'], bot3['To'])\n\n\nif p_val &lt; 0.01:\n    print('There is a significant difference!')\nelse:\n    print('There is NOT a significant difference!')\n\nWe conclude that the study is right: higher drafted players do indeed seem to have longer career lengths."
  },
  {
    "objectID": "projects/project1/NFL Career Length Predictor.html#machine-learning-models",
    "href": "projects/project1/NFL Career Length Predictor.html#machine-learning-models",
    "title": "Predicting NFL PLayers’ Career Length",
    "section": "Machine Learning Models",
    "text": "Machine Learning Models\n\nPreprocess Data\n\nfrom sklearn.impute import KNNImputer\n\nWe will be using the KNN imputer to impute missing values in the dataset\n\nrandom = df.copy()\n\nWe will be imputing them by positions, to test it out, we will impute the data for only Quarterbacks\n\nrandomQB = random[random['POS'] == 'QB']\nrandomQB.drop(columns = 'Player', inplace = True)\ncollege = randomQB['College_x']\nteam = randomQB['Team']\nrandomQB.drop(columns = ['College_x', 'POS', 'Team'], inplace = True)\nrandomQB.reset_index()\n\n\nrandomQB\n\n\nimputer = KNNImputer(n_neighbors = 5)\nimputer.fit(randomQB)\nrandomQB = pd.DataFrame(imputer.transform(randomQB), columns = randomQB.columns)\nrandomQB['College'] = list(college)\nrandomQB['Team'] = list(team)\nrandomQB\n\n\nrandom['POS'].value_counts()\n\nDL have less than 5 players, for simplicity, we will ignore them.\n\nfilter_list = ['DL']\nrandom = random[~random['POS'].isin(filter_list)]\nrandom\n\nImputer for all positions:\n\nalldf = {}\nallpos = list(random['POS'].unique())\nfor i in allpos:\n  if i !='K' and i !='P':\n    print(i)\n    name = 'random{}'.format(i)\n    subdf = random[random['POS'] ==i]\n    subdf.drop(columns = 'Player', inplace = True)\n    subdf.dropna(subset = 'Pick', inplace = True)\n    college = subdf['College_x']\n    team = subdf['Team']\n    subdf.drop(columns = ['College_x', 'POS', 'Team'], inplace = True)\n    imputer.fit(subdf)\n    subdf = pd.DataFrame(imputer.transform(subdf), columns=subdf.columns)\n    subdf['College'] = list(college)\n    subdf['Team'] = list(team)\n    alldf[name] = subdf\n\nSince Kickers have the shuttle column completely missing, we will have a it separately.\nKickers are most similar to punters, we will be imputing kickers with punters.\n\nrandomKP = random[(random['POS'] == 'K') | (random['POS'] == 'P')]\nrandomKP.drop(columns = 'Player', inplace = True)\ncollege = randomKP['College_x']\nteam = randomKP['Team']\nrandomKP.drop(columns = ['College_x', 'POS', 'Team'], inplace = True)\nrandomKP.reset_index()\nrandomKP\n\n\nimputer = KNNImputer(n_neighbors = 5)\nimputer.fit(randomKP)\nrandomKP = pd.DataFrame(imputer.transform(randomKP), columns = randomKP.columns)\nrandomKP['College'] = list(college)\nrandomKP['Team'] = list(team)\nrandomKP\n\n\nalldf['RandomKP'] = randomKP\n\n\npreprocessed = pd.DataFrame()\nfor df in alldf.values():\n  preprocessed = pd.concat([preprocessed, df])\npreprocessed\n\n\nnan_columns = preprocessed.columns[preprocessed.isna().any()].tolist()\nnan_columns\n\nApparently a few players have np.nan as “College”, we will also drop these rows.\n\npreprocessed = preprocessed.dropna()\npreprocessed\n\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import recall_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn import svm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, precision_recall_curve, recall_score, precision_score, f1_score,accuracy_score, make_scorer\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nimport warnings\nfrom sklearn.exceptions import FitFailedWarning\nfrom sklearn.exceptions import ConvergenceWarning\n\nOne Hot Encode:\n\npreprocessed = pd.get_dummies(preprocessed)\n\n\nX = preprocessed.drop(columns = ['To']) #DF without years of experience and Total Games Played\ny = preprocessed['To']\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\nX.head()\n\n\ny.head()"
  },
  {
    "objectID": "projects/project1/NFL Career Length Predictor.html#baseline-model---linear-regression",
    "href": "projects/project1/NFL Career Length Predictor.html#baseline-model---linear-regression",
    "title": "Predicting NFL PLayers’ Career Length",
    "section": "Baseline Model - Linear Regression",
    "text": "Baseline Model - Linear Regression\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import SimpleImputer\n\n\nfrom sklearn.metrics import accuracy_score, r2_score, mean_squared_error, mean_absolute_error\n\n\nbase = LinearRegression()\n\n\nbase = LinearRegression()\nbase.fit(X_train, y_train)\ny_pred_train = base.predict(X_train)\ny_pred_test = base.predict(X_test)\nprint('Training R2 score:', r2_score(y_train, y_pred_train))\nprint('Testing R2 score:', r2_score(y_test, y_pred_test))\n\n\nplt.scatter(y_pred_train, y_train) #Plot of y_train and y_predicted\nplt.title('prediction on train set')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\n\nplt.scatter(y_pred_test, y_test)\nplt.title('prediction on test set')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\nIt appears that the problem is too complex for the linear regression to generalize to unseen data."
  },
  {
    "objectID": "projects/project1/NFL Career Length Predictor.html#random-forest-regressor",
    "href": "projects/project1/NFL Career Length Predictor.html#random-forest-regressor",
    "title": "Predicting NFL PLayers’ Career Length",
    "section": "Random Forest Regressor",
    "text": "Random Forest Regressor\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()\nrf.fit(X_train, y_train)\ny_pred_train = rf.predict(X_train)\ny_pred_test = rf.predict(X_test)\nprint('Training R2 score:', r2_score(y_train, y_pred_train))\nprint('Testing R2 score:', r2_score(y_test, y_pred_test))\n\n\nplt.scatter(y_pred_train, y_train) #Plot of y_train and y_predicted\nplt.title('prediction on train set')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\n\nplt.scatter(y_pred_test, y_test)\nplt.title('prediction on test set')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\nEven though the random forest regressor did pretty well in training, it still does not generalize well to unseen data.\nNow we run a gridsearch to find best combination of hyperparameters that will result in best test score.\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RepeatedKFold\n# rf = RandomForestRegressor()\n# param_grid = [{'n_estimators': [100, 200, 300],\n#                'min_samples_split': [2, 5, 10],\n#                'min_samples_leaf': [1, 2, 4],\n#               }]\n# grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='r2', verbose=10)\n# grid_search.fit(X,y)\n\n# print('Best hyperparameters:', grid_search.best_params_)\n# print('Best score:', grid_search.best_score_)\n\n\nBest_hyperparameters= {'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 300}\nBest_score= 0.176639443021116834\n\n\nrf = RandomForestRegressor(min_samples_leaf =4, min_samples_split=2, n_estimators=300, random_state=42)\nrf.fit(X_train, y_train)\ny_pred_train = rf.predict(X_train)\ny_pred_test = rf.predict(X_test)\nprint('Training R2 score:', r2_score(y_train, y_pred_train))\nprint('Testing R2 score:', r2_score(y_test, y_pred_test))\n\n\nplt.scatter(y_pred_train, y_train)\nplt.title('prediction on training set')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\n\nplt.scatter(y_pred_test, y_test)\nplt.title('prediction on testing set')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\nEven though the new model has a higher test score, when taking a closer look, we see that it’s not able to predict successfully."
  },
  {
    "objectID": "projects/project1/NFL Career Length Predictor.html#xgboost-regressor",
    "href": "projects/project1/NFL Career Length Predictor.html#xgboost-regressor",
    "title": "Predicting NFL PLayers’ Career Length",
    "section": "XGBoost Regressor",
    "text": "XGBoost Regressor\nXGBoost are ensemble techniques similar to Random Forests, but instead of each model being trained independently, each new model is trained to correct the errors made by the previous models. This model should provide better results than Random Forest.\n\nimport xgboost as xgb\n\n\nxr = xgb.XGBRegressor()\nxr.fit(X_train, y_train)\n\n\ny_pred_train = xr.predict(X_train)\ny_pred_test = xr.predict(X_test)\nprint('Training R2 score:', r2_score(y_train, y_pred_train))\nprint('Testing R2 score:', r2_score(y_test, y_pred_test))\n\n\nplt.scatter(y_pred_train, y_train)\nplt.title('prediction on training set')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\n\nplt.scatter(y_pred_test, y_test)\nplt.title('prediction on testing set')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\n\nXGBoost does not appear to be a better model than Random Forest Regressors. It appears that the data is simply too complex for the regressors to make predictions.\n\n\nAnother way of evaluating career success is whether or not they play more than 4 years, as 3.3 years is the average length of NFL players.\n\ndef morethan4(x):\n    if int(x) &gt;= 4:\n        return True\n    else:\n        return False\n\n\npreprocessed['To'] = preprocessed['To'].apply(morethan4)\n\n\npreprocessed.head()\n\n\npreprocessed['To'].value_counts(True)\n\n\nX = preprocessed.drop(columns = 'To') #DF without years of experience\ny = preprocessed['To']\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
  },
  {
    "objectID": "projects/project1/NFL Career Length Predictor.html#knn-classifier",
    "href": "projects/project1/NFL Career Length Predictor.html#knn-classifier",
    "title": "Predicting NFL PLayers’ Career Length",
    "section": "KNN Classifier",
    "text": "KNN Classifier\n\n#imports for knn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nValidation curve for how many neighbors:\n\nk_range = [10,50,60,70,100,150]\ntrain_scores, val_scores = validation_curve(KNeighborsClassifier(), X_train, y_train, \n                                             param_name='n_neighbors', \n                                             param_range=k_range)\n\n# Plot validation curve\nplt.figure(figsize=(10, 6))\nplt.title(\"KNN Validation Curve\")\nplt.xlabel(\"K\")\nplt.ylabel(\"Accuracy\")\nplt.xticks(k_range)\nplt.plot(k_range, np.median(train_scores, 1), color='blue', label='training score')\nplt.plot(k_range, np.median(val_scores, 1), color='red', label='validation score')\nplt.legend(loc=\"best\")\nplt.show()\n\nIt appears k neighbors of about 50-70 is the sweet spot; we will use neighbors of 50.\n\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score, roc_curve\n# train the KNN classifier\nknn = KNeighborsClassifier(n_neighbors=50, metric='euclidean')\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nprint(\"The accuracy score for this model is:\", metrics.accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\ncm2 = metrics.confusion_matrix(y_test, y_pred)\n\nfig, ax = plt.subplots(figsize=(7.5, 7.5))\nax.matshow(cm2, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(cm2.shape[0]):\n    for j in range(cm2.shape[1]):\n        ax.text(x=j, y=i,s=cm2[i, j], va='center', ha='center', size='xx-large')\n\nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix', fontsize=18)\nplt.show()\nrecall = recall_score(y_test, y_pred, pos_label=True)\nprecision = precision_score(y_test, y_pred, pos_label=True)\nf1 = f1_score(y_test, y_pred, pos_label=True)\nprint(\"Recall:\", recall)\nprint(\"Precision:\", precision)\nprint(\"F1 score:\", f1)\n\n\nfrom sklearn.feature_selection import SelectKBest, f_classif\nselector = SelectKBest(f_classif, k=5) # k specifies the number of top features to select\nselector.fit(X_test, y_test)\nX_selected = selector.transform(X_test)\n\nknn.fit(X_selected, y_test)\n\n# Print the importance of each feature\nfeature_importance = selector.scores_\ntop_feature_indices = selector.get_support(indices=True)\n\n# Get the names of the top features\ntop_feature_names = [X.columns[i] for i in top_feature_indices]\n\n\n# Print the name of the most important feature\ntop_feature_names\n\nFrom the application of SelectKBest to KNN model, age of draft, pick number, and whether or not a person is undrafted seem to be the main feature of importance. This indicates that the higher the pick number, the higher the chances of the player playing over 4 years, which is what we would expect. Surprisingly, bench press seeemed to matter too.\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import VarianceThreshold\npipe = Pipeline([\n('scaler', StandardScaler()),\n('selector', VarianceThreshold()),\n('classifier', KNeighborsClassifier())\n])\n\npipe.fit(X_train, y_train)\nprint('Training set score: ' + str(pipe.score(X_train,y_train)))\nprint('Test set score: ' + str(pipe.score(X_test,y_test)))\n\nGridsearching best n neighbors; from the validation curve, its between 50-70.\n\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {\n 'classifier__n_neighbors': [50, 55, 60, 65, 70]\n}\n# create grid search\ngrid = GridSearchCV(pipe, parameters,scoring=['accuracy', 'roc_auc_ovr', 'f1_micro'],\n                    cv=5, n_jobs = -1, refit=False,verbose=0).fit(X_train, y_train)\n\n\nbest_model = grid.fit(X_train, y_train)\n# get the model with highest accuracy from grid search\np_accu = best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_accuracy']) ]\np_accu\n\n\npipe.set_params(**p_accu)\n\n\n# train on the entire training set with the model with highest accuracy from grid search\nclf = pipe.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nscore2 = clf.score(X_test, y_test)\nprint(\"The accuracy score for the optimized model is\", score2)\ncm5 = metrics.confusion_matrix(y_test, y_pred)\n\nfig, ax = plt.subplots(figsize=(7.5, 7.5))\nax.matshow(cm5, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(cm5.shape[0]):\n    for j in range(cm5.shape[1]):\n        ax.text(x=j, y=i,s=cm5[i, j], va='center', ha='center', size='xx-large')\n\nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix', fontsize=18)\nplt.show()\nfpr = roc_curve(y_test, clf.predict_proba(X_test)[:,1], pos_label=True)[0] # false positiv \ntpr = roc_curve(y_test, clf.predict_proba(X_test)[:,1], pos_label=True)[1] # true positive \nroc_auc2 = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])\n\nKNN seems to be somewhat good at classifying; however, we see that the model is making quite a lot of false positive predicitons.\nNow we use a random forest classifier to see if can be more accurate than the KNN and make less false positives.\n\nRandom Forest Classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf = rf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nscore2 = clf.score(X_test, y_test)\nprint(\"The accuracy score for the optimized model is\", score2)\ncm5 = metrics.confusion_matrix(y_test, y_pred)\n\nfig, ax = plt.subplots(figsize=(7.5, 7.5))\nax.matshow(cm5, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(cm5.shape[0]):\n    for j in range(cm5.shape[1]):\n        ax.text(x=j, y=i,s=cm5[i, j], va='center', ha='center', size='xx-large')\n\nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix', fontsize=18)\nplt.show()\nfpr = roc_curve(y_test, clf.predict_proba(X_test)[:,1], pos_label=True)[0] # false positiv \ntpr = roc_curve(y_test, clf.predict_proba(X_test)[:,1], pos_label=True)[1] # true positive \nroc_auc2 = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])\n\nIt appears that the random forest has increased accuracy and decreased the false positive rate, but also increased false negatives.\n\n\nGridsearch for Random Forest Classifier\n\n# param_grid = {\n#     'n_estimators': [100, 200, 300],\n#     'max_depth': [None, 5, 10],\n#     'min_samples_split': [2, 4, 6],\n#     'min_samples_leaf': [1, 2, 3]\n# }\n\n# # Create a random forest classifier\n# rf = RandomForestClassifier(random_state=42)\n\n# # Perform grid search\n# grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', verbose=10)\n# grid_search.fit(X_train, y_train)\n\n\n# Get the best parameters and best score\n# best_params = grid_search.best_params_\n# best_score = grid_search.best_score_\n\n# print(\"Best Parameters:\", best_params)\n# print(\"Best Score:\", best_score)\n\nBest Parameters: {‘max_depth’: None, ‘min_samples_leaf’: 2, ‘min_samples_split’: 6, ‘n_estimators’: 200}\nBest Score: 0.6890196078431373\n\nrf = RandomForestClassifier(max_depth = None, min_samples_leaf = 2, min_samples_split = 6, n_estimators = 200, random_state=42)\nclf = rf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nscore2 = clf.score(X_test, y_test)\nprint(\"The accuracy score for the optimized model is\", score2)\ncm5 = metrics.confusion_matrix(y_test, y_pred)\n\nfig, ax = plt.subplots(figsize=(7.5, 7.5))\nax.matshow(cm5, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(cm5.shape[0]):\n    for j in range(cm5.shape[1]):\n        ax.text(x=j, y=i,s=cm5[i, j], va='center', ha='center', size='xx-large')\n\nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix', fontsize=18)\nplt.show()"
  },
  {
    "objectID": "projects/project1/NFL Career Length Predictor.html#random-forest-top-100-feature-selection-for-adaboost-classifier",
    "href": "projects/project1/NFL Career Length Predictor.html#random-forest-top-100-feature-selection-for-adaboost-classifier",
    "title": "Predicting NFL PLayers’ Career Length",
    "section": "Random Forest Top 100 Feature Selection for Adaboost Classifier",
    "text": "Random Forest Top 100 Feature Selection for Adaboost Classifier\n\nfrom sklearn.feature_selection import RFE\n\nestimator = RandomForestClassifier(n_estimators=100)\nselector = RFE(estimator, n_features_to_select=100)\nX_new = selector.fit_transform(X, y)\n\n\nX_new\n\n\nselected_feature_indices = selector.support_\n\n# Get the names or column indices of the selected features\nselected_feature_names = [name for name, selected in zip(X.columns, selected_feature_indices) if selected]\nselected_feature_indices = [idx for idx, selected in enumerate(selected_feature_indices) if selected]\n\nprint(\"Selected Feature Names:\", selected_feature_names)\nprint(\"Selected Feature Indices:\", selected_feature_indices)\n\n\nX_new = pd.DataFrame(X_new, columns = selected_feature_names)\n\n\n#Resplitting the data\nX_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.3, random_state=42)\n\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\nensemble = AdaBoostClassifier(base_estimator = rf)\nclf = ensemble.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nscore2 = clf.score(X_test, y_test)\nprint(\"The accuracy score for the optimized model is\", score2)\ncm5 = metrics.confusion_matrix(y_test, y_pred)\n\nfig, ax = plt.subplots(figsize=(7.5, 7.5))\nax.matshow(cm5, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(cm5.shape[0]):\n    for j in range(cm5.shape[1]):\n        ax.text(x=j, y=i,s=cm5[i, j], va='center', ha='center', size='xx-large')\n\nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix', fontsize=18)\nplt.show()\n\n\nGridsearch\n\n# param_grid = {\n#     'n_estimators': [50, 100, 150],\n#     'learning_rate': [0.1, 0.5, 1.0]\n# }\n\n# # Perform grid search using cross-validation\n# grid_search = GridSearchCV(ensemble, param_grid, cv=5, verbose = 10)\n# grid_search.fit(X_train, y_train)\n\n# # Get the best parameters and best score from grid search\n# best_params = grid_search.best_params_\n# best_score = grid_search.best_score_\n# print(\"Best Parameters:\", best_params)\n# print(\"Best Score:\", best_score)\n\n\nensemble = AdaBoostClassifier(base_estimator = rf, learning_rate = 0.5, n_estimators = 100)\nclf = ensemble.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nscore2 = clf.score(X_test, y_test)\nprint(\"The accuracy score for the optimized model is\", score2)\ncm5 = metrics.confusion_matrix(y_test, y_pred)\n\nfig, ax = plt.subplots(figsize=(7.5, 7.5))\nax.matshow(cm5, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(cm5.shape[0]):\n    for j in range(cm5.shape[1]):\n        ax.text(x=j, y=i,s=cm5[i, j], va='center', ha='center', size='xx-large')\n\nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix', fontsize=18)\nplt.show()\n\n\n\nFeature Importance\n\nimportances = ensemble.feature_importances_\nindices = np.argsort(importances)[::-1]\nfeature_names = X_new.columns\n\n\nfor idx in indices[0:10]:\n    print(f\"Feature: {feature_names[idx]}, Importance: {importances[idx]}\")"
  },
  {
    "objectID": "projects/project1/coord.html",
    "href": "projects/project1/coord.html",
    "title": "Coordinate Descent",
    "section": "",
    "text": "The goal of this project is to come up with a new coordinate descent method. Coordinate descent simplifies optimization problems by iteratively focusing on one coordinate at a time, making it efficient for large-scale and high-dimensional datasets. Its simplicity and adaptability to different problem structures lead to faster convergence and reduced computational costs.\nTo test the efficiency of my coordinate descent method, I used the wine dataset on UCI repo and only focused on the first and second class, making it a binary classification probllem."
  },
  {
    "objectID": "projects/project1/coord.html#model-1",
    "href": "projects/project1/coord.html#model-1",
    "title": "Coordinate Descent",
    "section": "Model 1",
    "text": "Model 1\nOnly the coordinate with the largest gradient (i.e., the steepest slope) is selected for optimization in each iteration.\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\n\ndef coordinate_descent_lr(X, y, method='largest_gradient', num_iterations=100, learning_rate=0.01):\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    b = 0\n    losses = []\n\n    for _ in range(num_iterations):\n        for i in range(n_features):\n            if method == 'random':\n                i = np.random.randint(n_features)  # Random feature for random-feature coordinate descent\n            else:\n                y_pred = sigmoid(np.dot(X, w) + b)\n                gradients = -np.dot(X.T, (y-y_pred)) / n_samples\n                i = np.argmax(np.abs(gradients))\n                \n            feature = X[:, i]\n            y_pred = sigmoid(np.dot(X, w) + b)\n\n            # Gradient calculation\n            grad_wi = -np.dot(feature, (y - y_pred)) / n_samples\n            grad_b = -np.mean(y - y_pred)\n\n            # Update weights\n            w[i] -= learning_rate * grad_wi\n            b -= learning_rate * grad_b\n\n        # Record the loss after each full iteration over features\n        losses.append(log_loss(y, sigmoid(np.dot(X, w) + b)))\n\n    return w, b, losses"
  },
  {
    "objectID": "projects/project1/coord.html#model-2",
    "href": "projects/project1/coord.html#model-2",
    "title": "Coordinate Descent",
    "section": "Model 2",
    "text": "Model 2\nRandomly choose a single coordinate to optimize in each iteration Note: Code is the same as above, change method to random instead."
  },
  {
    "objectID": "projects/project1/coord.html#algorithm-1",
    "href": "projects/project1/coord.html#algorithm-1",
    "title": "Coordinate Descent",
    "section": "Algorithm 1",
    "text": "Algorithm 1\nOnly the coordinate with the largest gradient (i.e., the steepest slope) is selected for optimization in each iteration.\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\n\ndef coordinate_descent_lr(X, y, method='largest_gradient', num_iterations=100, learning_rate=0.01):\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    b = 0\n    losses = []\n\n    for _ in range(num_iterations):\n        for i in range(n_features):\n            if method == 'random':\n                i = np.random.randint(n_features)  # Random feature for random-feature coordinate descent\n            else:\n                y_pred = sigmoid(np.dot(X, w) + b)\n                gradients = -np.dot(X.T, (y-y_pred)) / n_samples\n                i = np.argmax(np.abs(gradients))\n                \n            feature = X[:, i]\n            y_pred = sigmoid(np.dot(X, w) + b)\n\n            # Gradient calculation\n            grad_wi = -np.dot(feature, (y - y_pred)) / n_samples\n            grad_b = -np.mean(y - y_pred)\n\n            # Update weights\n            w[i] -= learning_rate * grad_wi\n            b -= learning_rate * grad_b\n\n        # Record the loss after each full iteration over features\n        losses.append(log_loss(y, sigmoid(np.dot(X, w) + b)))\n\n    return w, b, losses"
  },
  {
    "objectID": "projects/project1/coord.html#algorithm-2",
    "href": "projects/project1/coord.html#algorithm-2",
    "title": "Coordinate Descent",
    "section": "Algorithm 2",
    "text": "Algorithm 2\nRandomly choose a single coordinate to optimize in each iteration Note: Code is the same as above, change method to random instead."
  },
  {
    "objectID": "projects/project1/coord.html#simple-update",
    "href": "projects/project1/coord.html#simple-update",
    "title": "Coordinate Descent",
    "section": "Simple Update",
    "text": "Simple Update\nAt each step, weights is updated with the formula \\(w_i \\leftarrow w_i - \\alpha \\frac{\\partial L}{\\partial w_i}\\), where \\(\\frac{\\partial L}{\\partial w_i}\\) is the partial derivative of loss function L with respect to \\(w_i\\) ## Backtracking Line Search Back tracking line search adatively adjusts the learning rate, ensuring each step in the optimization process is sufficiently large to make progress yet small enough to avoid overshooting minimum.\n\ndef backtracking_line_search(X, y, w, b, grad_wi, i, initial_lr=1, beta=0.8, c=1e-4):\n    lr = initial_lr\n    current_loss = log_loss(y, sigmoid(np.dot(X, w) + b))\n    updated_w = w.copy()\n    \n    while True:\n        updated_w[i] = w[i] - lr * grad_wi\n        new_loss = log_loss(y, sigmoid(np.dot(X, updated_w) + b))\n        if new_loss &lt;= current_loss - c * lr * grad_wi**2:\n            break\n        lr *= beta\n    \n    return lr\n\nImplementing it into Coordinate Descent algorithm above gives:\n\ndef coordinate_descent_lr2(X, y, method='largest_gradient', num_iterations=100):\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    b = 0\n    losses = []\n\n    for _ in range(num_iterations):\n        for i in range(n_features):\n            if method == 'largest_gradient':\n                y_pred = sigmoid(np.dot(X, w) + b)\n                gradients = -np.dot(X.T, (y - y_pred)) / n_samples\n                i = np.argmax(np.abs(gradients))\n            else:\n                i = np.random.randint(n_features)\n\n            feature = X[:, i]\n            y_pred = sigmoid(np.dot(X, w) + b)\n            grad_wi = -np.dot(feature, (y - y_pred)) / n_samples\n            grad_b = -np.mean(y - y_pred)\n\n            # Backtracking line search to find learning rate\n            learning_rate_wi = backtracking_line_search(X, y, w, b, grad_wi, i)\n\n            # Update weights\n            w[i] -= learning_rate_wi * grad_wi\n            b -= learning_rate_wi * grad_b\n\n        losses.append(log_loss(y, sigmoid(np.dot(X, w) + b)))\n\n    return w, b, losses"
  },
  {
    "objectID": "projects/project1/proto.html",
    "href": "projects/project1/proto.html",
    "title": "Prototype Selection",
    "section": "",
    "text": "The goal of this project is to find a way to down sample to size n from an entire training set so that the training time for the nearest neighbor could be reduced while remaining the accuracy of the original training set.\nThe algorithms are tested on the MNIST dataset."
  },
  {
    "objectID": "projects/project1/proto.html#algorithm-1",
    "href": "projects/project1/proto.html#algorithm-1",
    "title": "Prototype Selection",
    "section": "Algorithm 1",
    "text": "Algorithm 1\nK-Means Clustering 1: Split the dataset into 10 clusters (as there are 10 digits) and take an equal amount of datapoints from each cluster as the subsample"
  },
  {
    "objectID": "projects/project1/proto.html#algorithm-2",
    "href": "projects/project1/proto.html#algorithm-2",
    "title": "Prototype Selection",
    "section": "Algorithm 2",
    "text": "Algorithm 2\nK-Means Clustering 2: Split the dataset into n clusters and take one datapoint from each cluster as the subsample"
  },
  {
    "objectID": "projects/project1/proto.html#algorithm-3",
    "href": "projects/project1/proto.html#algorithm-3",
    "title": "Prototype Selection",
    "section": "Algorithm 3",
    "text": "Algorithm 3\nModified Active Learning: Randomly select datapoints and run a classification model and select the most uncertain datapoints."
  },
  {
    "objectID": "projects/project1/proto.html#algorithm-4",
    "href": "projects/project1/proto.html#algorithm-4",
    "title": "Prototype Selection",
    "section": "Algorithm 4",
    "text": "Algorithm 4\nModified K-Means: Subsample proportionately to training, identify digits that are likely to be misclassified and apply K-means to clustered digits"
  },
  {
    "objectID": "projects.html#natural-language-processing",
    "href": "projects.html#natural-language-processing",
    "title": "My Projects",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing\n\n\n\n\n\n\n\n\n\n\nRestaurant Category Prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/nlp/restaurant.html",
    "href": "projects/nlp/restaurant.html",
    "title": "Restaurant Category Prediction",
    "section": "",
    "text": "The goal of this project is to predict the restaurant type using details about the restaurant and their reviews. There is also a Kaggle Competition Page associated with this project.\n\n\nThis is my position on the Kaggle Leader Board:"
  },
  {
    "objectID": "projects/nlp/restaurant.html#leaderboard",
    "href": "projects/nlp/restaurant.html#leaderboard",
    "title": "Restaurant Category Prediction",
    "section": "",
    "text": "This is my position on the Kaggle Leader Board:"
  },
  {
    "objectID": "projects/MA/hw1_questions.html",
    "href": "projects/MA/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn their experiment, Dean Karlan and John A. List conducted a large-scale natural field experiment to explore the impact of price on charitable giving. They used direct mail solicitations sent to over 50,000 previous donors of a nonprofit organization to test the effectiveness of matching grants on charitable donations. The experiment randomly assigned individuals to either a control group or a matching grant treatment group. Within the matching grant treatment group, individuals were further randomly assigned to different matching grant rates, matching grant maximum amounts, and suggested donation amounts.\nThe study found that announcing the availability of match money significantly increased both the revenue per solicitation (by 19%) and the probability of making a donation (by 22%). However, larger matching ratios ($3:$1 and $2:$1) did not have an additional impact compared to a smaller matching ratio ($1:$1). The elasticity estimate of the price change from the baseline to the treatment groups was -0.30, which is near the lower range of the elasticity of giving with respect to transitory price changes reported in previous studies.\nInterestingly, the effectiveness of the matching gift varied by the political environment of the donors. In states that voted for George W. Bush in the 2004 presidential election (“red” states), the match increased the revenue per solicitation by 55%, while in “blue” states, there was little effect observed.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/MA/hw1_questions.html#introduction",
    "href": "projects/MA/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn their experiment, Dean Karlan and John A. List conducted a large-scale natural field experiment to explore the impact of price on charitable giving. They used direct mail solicitations sent to over 50,000 previous donors of a nonprofit organization to test the effectiveness of matching grants on charitable donations. The experiment randomly assigned individuals to either a control group or a matching grant treatment group. Within the matching grant treatment group, individuals were further randomly assigned to different matching grant rates, matching grant maximum amounts, and suggested donation amounts.\nThe study found that announcing the availability of match money significantly increased both the revenue per solicitation (by 19%) and the probability of making a donation (by 22%). However, larger matching ratios ($3:$1 and $2:$1) did not have an additional impact compared to a smaller matching ratio ($1:$1). The elasticity estimate of the price change from the baseline to the treatment groups was -0.30, which is near the lower range of the elasticity of giving with respect to transitory price changes reported in previous studies.\nInterestingly, the effectiveness of the matching gift varied by the political environment of the donors. In states that voted for George W. Bush in the 2004 presidential election (“red” states), the match increased the revenue per solicitation by 55%, while in “blue” states, there was little effect observed.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/MA/hw1_questions.html#data",
    "href": "projects/MA/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n\n(50083, 51)\n\n\nThere are 50083 rows and 51 columns in this dataset.\n\n\n           treatment       control    ratio        ratio2        ratio3  \\\ncount   50083.000000  50083.000000    50083  50083.000000  50083.000000   \nunique           NaN           NaN        4           NaN           NaN   \ntop              NaN           NaN  Control           NaN           NaN   \nfreq             NaN           NaN    16687           NaN           NaN   \nmean        0.666813      0.333187      NaN      0.222311      0.222211   \nstd         0.471357      0.471357      NaN      0.415803      0.415736   \nmin         0.000000      0.000000      NaN      0.000000      0.000000   \n25%         0.000000      0.000000      NaN      0.000000      0.000000   \n50%         1.000000      0.000000      NaN      0.000000      0.000000   \n75%         1.000000      1.000000      NaN      0.000000      0.000000   \nmax         1.000000      1.000000      NaN      1.000000      1.000000   \n\n           size        size25        size50       size100        sizeno  ...  \\\ncount     50083  50083.000000  50083.000000  50083.000000  50083.000000  ...   \nunique        5           NaN           NaN           NaN           NaN  ...   \ntop     Control           NaN           NaN           NaN           NaN  ...   \nfreq      16687           NaN           NaN           NaN           NaN  ...   \nmean        NaN      0.166723      0.166623      0.166723      0.166743  ...   \nstd         NaN      0.372732      0.372643      0.372732      0.372750  ...   \nmin         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n25%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n50%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n75%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \nmax         NaN      1.000000      1.000000      1.000000      1.000000  ...   \n\n              redcty       bluecty        pwhite        pblack     page18_39  \\\ncount   49978.000000  49978.000000  48217.000000  48047.000000  48217.000000   \nunique           NaN           NaN           NaN           NaN           NaN   \ntop              NaN           NaN           NaN           NaN           NaN   \nfreq             NaN           NaN           NaN           NaN           NaN   \nmean        0.510245      0.488715      0.819599      0.086710      0.321694   \nstd         0.499900      0.499878      0.168561      0.135868      0.103039   \nmin         0.000000      0.000000      0.009418      0.000000      0.000000   \n25%         0.000000      0.000000      0.755845      0.014729      0.258311   \n50%         1.000000      0.000000      0.872797      0.036554      0.305534   \n75%         1.000000      1.000000      0.938827      0.090882      0.369132   \nmax         1.000000      1.000000      1.000000      0.989622      0.997544   \n\n           ave_hh_sz  median_hhincome        powner  psch_atlstba  \\\ncount   48221.000000     48209.000000  48214.000000  48215.000000   \nunique           NaN              NaN           NaN           NaN   \ntop              NaN              NaN           NaN           NaN   \nfreq             NaN              NaN           NaN           NaN   \nmean        2.429012     54815.700533      0.669418      0.391661   \nstd         0.378115     22027.316665      0.193405      0.186599   \nmin         0.000000      5000.000000      0.000000      0.000000   \n25%         2.210000     39181.000000      0.560222      0.235647   \n50%         2.440000     50673.000000      0.712296      0.373744   \n75%         2.660000     66005.000000      0.816798      0.530036   \nmax         5.270000    200001.000000      1.000000      1.000000   \n\n        pop_propurban  \ncount    48217.000000  \nunique            NaN  \ntop               NaN  \nfreq              NaN  \nmean         0.871968  \nstd          0.258654  \nmin          0.000000  \n25%          0.884929  \n50%          1.000000  \n75%          1.000000  \nmax          1.000000  \n\n[11 rows x 51 columns]\n\n\nThe above shows a general distribution for each variable.\n\n\ntreatment                0\ncontrol                  0\nratio                    0\nratio2                   0\nratio3                   0\nsize                     0\nsize25                   0\nsize50                   0\nsize100                  0\nsizeno                   0\nask                      0\naskd1                    0\naskd2                    0\naskd3                    0\nask1                     0\nask2                     0\nask3                     0\namount                   0\ngave                     0\namountchange             0\nhpa                      0\nltmedmra                 0\nfreq                     0\nyears                    1\nyear5                    0\nmrm2                     1\ndormant                  0\nfemale                1111\ncouple                1148\nstate50one               0\nnonlit                 452\ncases                  452\nstatecnt                 0\nstateresponse            0\nstateresponset           0\nstateresponsec           3\nstateresponsetminc       3\nperbush                 35\nclose25                 35\nred0                    35\nblue0                   35\nredcty                 105\nbluecty                105\npwhite                1866\npblack                2036\npage18_39             1866\nave_hh_sz             1862\nmedian_hhincome       1874\npowner                1869\npsch_atlstba          1868\npop_propurban         1866\ndtype: int64\n\n\nThis shows the number of missing values in each column.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nTesting mrm2 - Months since last donation.\n\n\nT-test results: t-statistic = 0.11953155228177251, p-value = 0.9048549631450832\n\n\n\n\nLinear regression results:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):              0.905\nTime:                        13:04:47   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n13.011828117981734\n12.99814226643495\n\n\nUsing both the t-test and linear regerssion, the p-value for the difference in mens between treatment and control groups for mrm2 is 0.905. This means that we fail to reject the null hypothesis that the treatment and control groups are statistically significantly different at the 95% confidence level, further suggesting that the randomization was successful.\n\n\nmean for mrm2 control: 12.99814226643495\nmean for mrm2 control: 13.011828117981734\n\n\nThe above values matches the ones shown in Table 1.\n\n\nTesting hpa - highest previous contribution\n\n\nT-test results: t-statistic = 0.9704273843087994, p-value = 0.3318400397145116\n\n\n\n\nLinear regression results:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    hpa   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.8924\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):              0.345\nTime:                        13:04:47   Log-Likelihood:            -2.8468e+05\nNo. Observations:               50083   AIC:                         5.694e+05\nDf Residuals:                   50081   BIC:                         5.694e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     58.9602      0.551    107.005      0.000      57.880      60.040\ntreatment      0.6371      0.675      0.944      0.345      -0.685       1.960\n==============================================================================\nOmnibus:                    66199.149   Durbin-Watson:                   2.003\nProb(Omnibus):                  0.000   Jarque-Bera (JB):         14448195.271\nSkew:                           7.552   Prob(JB):                         0.00\nKurtosis:                      84.826   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n59.59724\n58.960167\n\n\nUsing both the t-test and linear regerssion, the p-value for the difference in mens between treatment and control groups for hpa are both above 0.94. This means that we fail to reject the null hypothesis that the treatment and control groups are statistically significantly different at the 95% confidence level, further suggesting that the randomization was successful.\n\n\nmean for mrm2 control: 58.960167\nmean for mrm2 control: 59.59724\n\n\nThe above values matches the ones shown in Table 1.  Note: hpa is highest previous contribution and mrm2 is number of months since last donation."
  },
  {
    "objectID": "projects/MA/hw1_questions.html#experimental-results",
    "href": "projects/MA/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\n\n\n\n\n\n\n\nAs seen in the plot above, the control group appears to have a lower proportion compared to the treatment group.\n\n\nT-test results: t-statistic = 3.101361000543946, p-value = 0.0019274025949016982\nLinear regression results:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):            0.00193\nTime:                        13:04:47   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe t-test and linear regression both show a p value of 0.002. This suggest that the difference in response rates between the treatment and control groups is statistically significant, meaning that the treatment group does indeed increase the likelihood of making a charitable donation compared to the control group. This finding is also consistent with the results reported in Table 2a.\n\n\ncoefficient 0.004180354512949377\nz-statistic:  3.101361000543931\np-value:  0.001927402594901797\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):            0.00193\nTime:                        13:04:47   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe probit regression also confirms that the coefficient on the treatment varialbe is statistically significant (coefficient of 0.004 and p-value of 0.002). This is consistent with the findings on Table 3 column 1, suggesting that people in the treatemnt group does have increased likelihood of making a charitable donation. However, it is do be noted that while Table 3 indicates the use of Probit regression, the above was replicated with a linear regression.\nThe results of a probit regression is shown below:\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\ncoefficient 0.08678462244745795\nz-statistic:  3.112930073794974\np-value:  0.0018523990147786566\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Tue, 16 Apr 2024   Pseudo R-squ.:               0.0009783\nTime:                        13:04:48   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\nWhile the coefficient is not the same as that on Table 3 column 1, the p-value is still 0.002, indicating that people in the treatment group does have increased likelihood of making a charitable donation.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\n1:1 and 2:1 match ratiosT-test results: t-statistic = 0.05011583793874515, p-value = 0.9600305283739325\n1:1 and 3:1 match ratiosT-test results: t-statistic = -1.0150255853798622, p-value = 0.3101046637086672\n1:1 and 2:1 match ratiosT-test results: t-statistic = -0.96504713432247, p-value = 0.33453168549723933\n\n\nThe t-tests and the p-values show that the difference in response rate betwen 1:1 and 2:1, 2:1 and 3:1, 1:1 and 3:1 are all not statistically significant at the 95% confidence level. This means that the match ratios does not increase the likelihood of someone making a charitable donation. This is consistent with the authors comments on page 8, which suggested that the figures do not show a clear pattern of increasing repsonse rates as match ratios increase.\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.665\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):             0.0118\nTime:                        13:04:48   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50079   BIC:                        -5.322e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\nratio1         0.0029      0.002      1.661      0.097      -0.001       0.006\nratio2         0.0048      0.002      2.744      0.006       0.001       0.008\nratio3         0.0049      0.002      2.802      0.005       0.001       0.008\n==============================================================================\nOmnibus:                    59812.754   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4316693.217\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.438   Cond. No.                         4.26\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe regression results indicate that the coefficients for the 2:1 and 3:1 match ratios are statistically significant at the 95% confidence interval, suggesting that these match sizes significantly increase the likelihood of donating compared to the baseline category, which is the control group. However, the coefficient for the 1:1 match ratio is not statistically significant, indicating that a 1:1 match does not significantly enhance the likelihood of donating when compared to no matching. Therefore, while the 2:1 and 3:1 matches are effective in increasing donation rates, the 1:1 match does not show a significant effect relative to having no match at all.\n\n\ndifference between 3:1 and 2:1 : 0.00010002398025293902\ndifference between 1:1 and 2:1 : -0.0018842510217149944\ndifference between 3:1 and 1:1 : 0.0019842750019679334\n\n\nThe above is calcualted directly from data\n\n\ndifference between 3:1 and 2:1 : 0.00010002398025313504\ndifference between 1:1 and 2:1 : -0.0018842510217151158\ndifference between 3:1 and 1:1 : 0.001984275001968251\n\n\nThe above is calculated using the coefficients generated from the regression above.\nBoth resulted in the same set of numbers.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n\ndifference in donation amounts between control and treatment:  0.1536054\nt-statistic:  1.8605020225753781\np-value:  0.06282038947470686\n\n\nThe p-value suggest that the difference in donation amount between the treatment and control group is not statistically significant at the 95% confidence level. This suggests that the treatment group does not appear to have a higher donation amount.\n\n\ndifference in donation amounts between control and treatment:  -1.6683922\nt-statistic:  -0.5808388615237938\np-value:  0.5614758782284279\n\n\nNow limiting to only the people that donated, it appears that the treatment group donates about 1.66 units less than the control group. This different is not statistically significant though, as the p-value is 0.5615, well above the 0.05 threshold. Therefore, we cannot confidently assert the causal interpretation that people receiving treatment will result in reduced donation amounts."
  },
  {
    "objectID": "projects/MA/hw1_questions.html#simulation-experiment",
    "href": "projects/MA/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\n\n\n\n\n\n\n\nIn this plot, the x-axis represents the number of draws, and the y-axis represents the cumulative average of the differences between the treatment and control draws. The red dashed line represents the true difference in means between the treatment and control distributions.\nAs the number of draws increases, you would expect the cumulative average to approach the true difference in means. This is because, with a larger sample size, the estimate of the difference in means becomes more accurate. If the cumulative average converges to the true difference in means, it indicates that the simulation is correctly capturing the underlying difference between the treatment and control groups.\n\n\nCentral Limit Theorem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe histograms presented illustrate the Central Limit Theorem in action, highlighting how the distribution of the differences between the sample means increasingly approximates a normal distribution as the sample size grows. Given that zero is positioned near the center of these distributions, it suggests that there is no significant difference between the donation amounts of the treatment and control groups. This central positioning of zero within the distribution indicates that any observed difference is likely due to random sampling variability rather than a true effect of the treatment. Thus, the data provides no substantial evidence to suggest that the treatment influences donation amounts compared to the control."
  },
  {
    "objectID": "projects/MA/Untitled2.html",
    "href": "projects/MA/Untitled2.html",
    "title": "Joshua's Website",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_stata('karlan_list_2007.dta')\n\n\nimport matplotlib.pyplot as plt\n\n\nproportions = df.groupby('treatment')['gave'].mean().reset_index()\n\n# Create a bar plot\nplt.bar(proportions['treatment'], proportions['gave'], tick_label=['Control', 'Treatment'])\nplt.ylabel('Proportion of People Who Donated')\nplt.title('Proportion of Donations by Group')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy import stats\nimport statsmodels.api as sm\ntreatment_donated = df[df['treatment'] == 1]['gave']\ncontrol_donated = df[df['treatment'] == 0]['gave']\n\n# Perform a t-test\nt_stat, p_value = stats.ttest_ind(treatment_donated, control_donated)\nprint(f\"T-test results: t-statistic = {t_stat}, p-value = {p_value}\")\n\n# Perform a linear regression\ndf['intercept'] = 1\nmodel = sm.OLS(df['gave'], df[['intercept', 'treatment']])\nresults = model.fit()\nprint(\"Linear regression results:\")\nprint(results.summary())\n\nT-test results: t-statistic = 3.101361000543946, p-value = 0.0019274025949016982\nLinear regression results:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Thu, 11 Apr 2024   Prob (F-statistic):            0.00193\nTime:                        11:55:08   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\ndf['intercept'] = 1\n\n# Define the model and fit it\nprobit_model = sm.Probit(df['gave'], df[['intercept', 'treatment']])\nprobit_results = probit_model.fit()\n\n# Print the summary of the regression results\nprint(probit_results.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Thu, 11 Apr 2024   Pseudo R-squ.:               0.0009783\nTime:                        12:04:20   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\n\ndf\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\nintercept\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n1\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n1\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n1\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n1\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n1\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n1\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n1\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n1\n\n\n\n\n50083 rows × 52 columns\n\n\n\n\n\nmatch21 = df[df['ratio'] == 2]['gave']\nmatch31 = df[df['ratio']== 3]['gave']\nmatch11 = df[df['ratio'] == 1]['gave']\n\nt_stat, p_value = stats.ttest_ind(match31, match21)\nprint(f\"T-test results: t-statistic = {t_stat}, p-value = {p_value}\")\nt_stat, p_value = stats.ttest_ind(match11, match31)\nprint(f\"T-test results: t-statistic = {t_stat}, p-value = {p_value}\")\nt_stat, p_value = stats.ttest_ind(match11, match21)\nprint(f\"T-test results: t-statistic = {t_stat}, p-value = {p_value}\")\n\nT-test results: t-statistic = 0.05011583793874515, p-value = 0.9600305283739325\nT-test results: t-statistic = -1.0150255853798622, p-value = 0.3101046637086672\nT-test results: t-statistic = -0.96504713432247, p-value = 0.33453168549723933\n\n\n\nprobit_model = sm.OLS(df['gave'], df[['intercept', 'ratio2', 'ratio3']])\nprobit_results = probit_model.fit()\nprint(probit_results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     4.117\nDate:                Thu, 11 Apr 2024   Prob (F-statistic):             0.0163\nTime:                        12:16:26   Log-Likelihood:                 26629.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50080   BIC:                        -5.323e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0190      0.001     22.306      0.000       0.017       0.021\nratio2         0.0036      0.002      2.269      0.023       0.000       0.007\nratio3         0.0037      0.002      2.332      0.020       0.001       0.007\n==============================================================================\nOmnibus:                    59815.856   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317637.927\nSkew:                           6.741   Prob(JB):                         0.00\nKurtosis:                      46.443   Cond. No.                         3.16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "hobbies/games/league.html",
    "href": "hobbies/games/league.html",
    "title": "League of Legends",
    "section": "",
    "text": "Here is a collection of my league clips\n\nAkali Aram Penta\n\n\n\nJhin Aram Penta\n\n\n\nViego Aram Penta\n\n\n\nYone Aram Penta\n\n\n\nViego Aram Penta 2"
  },
  {
    "objectID": "hobbies/games/basketball.html",
    "href": "hobbies/games/basketball.html",
    "title": "Joshua's Website",
    "section": "",
    "text": "Placeholder"
  },
  {
    "objectID": "hobbies.html",
    "href": "hobbies.html",
    "title": "Hobbies",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "hobbies.html#games",
    "href": "hobbies.html#games",
    "title": "Hobbies",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "hobbies.html#basketball",
    "href": "hobbies.html#basketball",
    "title": "Hobbies",
    "section": "Basketball",
    "text": "Basketball\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hobbies/games/valorant.html",
    "href": "hobbies/games/valorant.html",
    "title": "Valorant",
    "section": "",
    "text": "Here is a collection of my valorant clips\n\nRaze Ace\n\n\n\nJett Outplay\n\n\n\nJett Ace\n\n\n\nKnifing Chamber"
  },
  {
    "objectID": "projects/MA/hw2_questions.html",
    "href": "projects/MA/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nThis is the first 5 rows of the data\n\n\nThe shape of the dataframe is: (1500, 4)\n\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n1\n0\nMidwest\n32.5\n0\n\n\n786\n3\nSouthwest\n37.5\n0\n\n\n348\n4\nNorthwest\n27.0\n1\n\n\n927\n3\nNortheast\n24.5\n0\n\n\n830\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\nThere are 1500 rows and 4 columns\n\n\n\n\n\n\n\n\n\n\n\n\npatents\nage\niscustomer\n\n\n\n\ncount\n1500.000000\n1500.000000\n1500.000000\n\n\nmean\n3.684667\n26.357667\n0.131333\n\n\nstd\n2.352500\n7.242528\n0.337877\n\n\nmin\n0.000000\n9.000000\n0.000000\n\n\n25%\n2.000000\n21.000000\n0.000000\n\n\n50%\n3.000000\n26.000000\n0.000000\n\n\n75%\n5.000000\n31.625000\n0.000000\n\n\nmax\n16.000000\n49.000000\n1.000000\n\n\n\n\n\n\n\n\n\n\n\n\npatents: Number of patents awarded over the last 5 years.\nregion: Regional location of the firm.\nage: Age of the firm since incorporation.\niscustomer: Indicates whether the firm uses Blueprinty’s software (1 = customer, 0 = not a customer)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean_customers = df[df['iscustomer'] == 1]['patents'].mean()\nmean_non_customers = df[df['iscustomer'] == 0]['patents'].mean()\n(mean_customers, mean_non_customers)\n\n(4.091370558375634, 3.6231772831926325)\n\n\nThe mean number of patents for customers appears to be slightly higher at 4.09, while the mean number of patents for non-customers appear to be lower at 3.62.\n\n\n\nHowever, Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nTo investigate any systematic differences, below are plots that compare regions and ages by customer status\n\n\n\n\n\n\n\n\n\n\nmean_age_customers = data_customers['age'].mean()\nmean_age_non_customers = data_non_customers['age'].mean()\n(mean_age_customers, mean_age_non_customers)\n\n(24.1497461928934, 26.691481197237145)\n\n\nIt appears that the average age of customers are lower than the average age of non customers.\n\n\n\n\n\n\n\n\n\nThe distribution of customers and non-customers across regions is not uniform. Both histograms reveal differences in the concentration of customers versus non-customers in various regions, suggesting regional preferences or market penetration differences.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe likelihood function for a set of independent and identically distributed observations ( Y_1, Y_2, , Y_n ) from a Poisson distribution, where each ( Y_i ) represents the number of patents awarded to an engineering firm in a given period and follows a Poisson distribution with parameter ( ), is given by:\n\\[\nL(\\lambda | Y_1, Y_2, \\ldots, Y_n) = \\prod_{i=1}^n f(Y_i | \\lambda)\n\\]\nGiven the probability mass function of the Poisson distribution ( f(Y|) = e{-}Y/Y! ), the likelihood function can be written as:\n\\[\nL(\\lambda | Y_1, Y_2, \\ldots, Y_n) = \\prod_{i=1}^n \\left( \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!} \\right)\n\\]\nThis can be simplified to:\n\\[\nL(\\lambda | Y_1, Y_2, \\ldots, Y_n) = e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i} \\prod_{i=1}^n \\frac{1}{Y_i!}\n\\]\nHere, ( n ) is the total number of observations (engineering firms), ( {i=1}^n Y_i ) is the total number of patents awarded across all firms, and ( {i=1}^n ) is the product of the factorials of the counts of patents for each firm.\nThe log-likelihood function for the Poisson model, coded in a function of lambda and Y would look like the following:\n\nimport numpy as np\nfrom scipy.special import factorial\n\ndef poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        return -np.inf  # log-likelihood is negative infinity if lambda is non-positive\n    return np.sum(-lambda_ + Y * np.log(lambda_) - np.log(factorial(Y)))\n\nBelow I use the function above to plot lambda on the horizontal axis and the likelihood on the vertical axis for a range of lambdas, which I used the number of patents as the input.\n\n\n\n\n\n\n\n\n\nNow, I used scipy.optimize to find the MLE after optimizing the likelihood function\n\nfrom scipy.optimize import minimize\n\n# Define the negative log-likelihood function since we minimize in the optimization\ndef negative_loglikelihood(lambda_, Y):\n    if lambda_[0] &lt;= 0:\n        return np.inf  # Return infinity if lambda is non-positive\n    lambda_val = lambda_[0]\n    return -np.sum(-lambda_val + Y * np.log(lambda_val) - np.log(factorial(Y)))\n\n# Initial guess for lambda\ninitial_lambda = [1.0]\n\n# Using scipy's minimize function to find the MLE of lambda\nresult = minimize(negative_loglikelihood, initial_lambda, args=(patents_data,), method='L-BFGS-B', bounds=[(0, None)])\n\nprint(f\"MLE for lambda: {result['x'][0]}\")\n\nMLE for lambda: 3.6846667021660804\n\n\nThe optimization has successfully found the maximum likelihood estimate (MLE) of 𝜆 for the Poisson distribution based on the patent data. The MLE of 𝜆 is approximately 3.685. This indicates that the best estimate for the average number of patents awarded per engineering firm over the observed period, under the assumption of a Poisson distribution, is about 3.685 patents. ​\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nFirst, I need to update my previous log-likelihood function to reflect that:\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    linear = X @ beta\n    lambda_i = np.exp(np.clip(linear, None, 20))\n    \n    log_likelihood = np.sum(Y * np.log(lambda_i) - lambda_i - np.log(factorial(Y)))\n    return -log_likelihood  \n\nI then used the updated function to find the MLE vector and the Hessian of the Poisson model with covariates. I also printed out the coefficient and the standard effor for each variable.\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom scipy.linalg import inv\n\n\ndf['age_squared'] = df['age'] ** 2\n\nencoder = OneHotEncoder(drop='first')\nregion_encoded = encoder.fit_transform(df[['region']]).toarray()\n\n\nY = df['patents'].values\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the continuous variables\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(df[['age', 'age_squared']])\n\n# Construct the design matrix with scaled features\nX = np.hstack([np.ones((df.shape[0], 1)), scaled_features, df[['iscustomer']].values, region_encoded])\n\ninitial_beta = np.zeros(X.shape[1])\n\n# Run the optimizer with detailed logging\nresult = minimize(\n    fun=poisson_regression_loglikelihood,\n    x0=initial_beta,\n    args=(Y, X),\n    method='L-BFGS-B',\n    bounds=[(None, None)] * X.shape[1],\n    options={'disp': True}\n)\n\nhess_inv = result.hess_inv.todense()  # if using L-BFGS, convert to dense matrix\n\ndef neg_log_likelihood(beta, Y, X):\n    lambda_ = np.exp(np.dot(X, beta))\n    return np.sum(-Y * np.log(lambda_) + lambda_ + gammaln(Y + 1))\n\ndef grad_neg_log_likelihood(beta, Y, X):\n    lambda_ = np.exp(np.dot(X, beta))\n    grad = np.dot(X.T, lambda_ - Y)\n    return grad\n\ndef hessian_neg_log_likelihood(beta, Y, X):\n    lambda_ = np.exp(np.dot(X, beta))\n    diag_lambda = np.diag(lambda_)\n    hessian = np.dot(X.T, np.dot(diag_lambda, X))\n    return hessian\n\nhessian_matrix = hessian_neg_log_likelihood(result.x, Y, X)\n\ncovariance_matrix_from_hessian = inv(hessian_matrix)\n\nstandard_errors_from_hessian = np.sqrt(np.diag(covariance_matrix_from_hessian))\nvariables = ['Age', 'Age Squared', 'Customer Status', 'Region Northeast', 'Region Northwest', 'Region South', 'Region Southwest']\n# Print the coefficients and their standard errors\nfor v, coef, std_err in zip(variables, result.x, standard_errors_from_hessian):\n    print(f\"{v}| Coefficient: {coef:.4f}, Standard Error: {std_err:.4f}\")\n\nAge| Coefficient: 1.2155, Standard Error: 0.0364\nAge Squared| Coefficient: 1.0464, Standard Error: 0.1005\nCustomer Status| Coefficient: -1.1408, Standard Error: 0.1025\nRegion Northeast| Coefficient: 0.1182, Standard Error: 0.0389\nRegion Northwest| Coefficient: 0.0986, Standard Error: 0.0420\nRegion South| Coefficient: -0.0201, Standard Error: 0.0538\nRegion Southwest| Coefficient: 0.0572, Standard Error: 0.0527\n\n\nNow we try to replicate the coefficients and standard errors with statsmodels.GLM()\n\nimport statsmodels.api as sm\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\n\n# Fit the model\nresult = poisson_model.fit()\n\n# Display the summary\n# Extract standard errors\ncoefficients = result.params\nstandard_errors = result.bse\np_values = result.pvalues\nconf_int = pd.DataFrame(result.conf_int(), columns=['95% CI Lower', '95% CI Upper'])\n\nstats_table = pd.DataFrame({\n    'Coefficient': coefficients,\n    'Standard Error': standard_errors,\n    'P-value': p_values,\n    '95% CI Lower': conf_int['95% CI Lower'],\n    '95% CI Upper': conf_int['95% CI Upper']\n})\n\nprint(stats_table)\n\n   Coefficient  Standard Error        P-value  95% CI Lower  95% CI Upper\n0     1.215438        0.036426  4.023701e-244      1.144045      1.286831\n1     1.046460        0.100487   2.144106e-25      0.849508      1.243412\n2    -1.140845        0.102495   8.884562e-29     -1.341731     -0.939960\n3     0.118114        0.038920   2.407229e-03      0.041832      0.194397\n4     0.098596        0.042007   1.891865e-02      0.016264      0.180928\n5    -0.020094        0.053783   7.086909e-01     -0.125508      0.085319\n6     0.057172        0.052676   2.777636e-01     -0.046071      0.160414\n7     0.051347        0.047212   2.767834e-01     -0.041188      0.143882\n\n\nAs seen in the table, coefficients and standard errors perfectly match the ones above.\n\n\n\nThe coefficient for the variable (index 3) representing whether or not it is a customer of Blueprinty is positive (0.1181) and statistically significant, suggests that firms using Blueprinty’s software likely have a higher expected patent count. The coefficient further implies that using Blueprinty’s software increases the patent count likelihood by approximately exp(0.1181) ≈ 1.125, or 12.5%."
  },
  {
    "objectID": "projects/MA/hw2_questions.html#blueprinty-case-study",
    "href": "projects/MA/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nThis is the first 5 rows of the data\n\n\nThe shape of the dataframe is: (1500, 4)\n\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n1\n0\nMidwest\n32.5\n0\n\n\n786\n3\nSouthwest\n37.5\n0\n\n\n348\n4\nNorthwest\n27.0\n1\n\n\n927\n3\nNortheast\n24.5\n0\n\n\n830\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\nThere are 1500 rows and 4 columns\n\n\n\n\n\n\n\n\n\n\n\n\npatents\nage\niscustomer\n\n\n\n\ncount\n1500.000000\n1500.000000\n1500.000000\n\n\nmean\n3.684667\n26.357667\n0.131333\n\n\nstd\n2.352500\n7.242528\n0.337877\n\n\nmin\n0.000000\n9.000000\n0.000000\n\n\n25%\n2.000000\n21.000000\n0.000000\n\n\n50%\n3.000000\n26.000000\n0.000000\n\n\n75%\n5.000000\n31.625000\n0.000000\n\n\nmax\n16.000000\n49.000000\n1.000000\n\n\n\n\n\n\n\n\n\n\n\n\npatents: Number of patents awarded over the last 5 years.\nregion: Regional location of the firm.\nage: Age of the firm since incorporation.\niscustomer: Indicates whether the firm uses Blueprinty’s software (1 = customer, 0 = not a customer)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean_customers = df[df['iscustomer'] == 1]['patents'].mean()\nmean_non_customers = df[df['iscustomer'] == 0]['patents'].mean()\n(mean_customers, mean_non_customers)\n\n(4.091370558375634, 3.6231772831926325)\n\n\nThe mean number of patents for customers appears to be slightly higher at 4.09, while the mean number of patents for non-customers appear to be lower at 3.62.\n\n\n\nHowever, Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nTo investigate any systematic differences, below are plots that compare regions and ages by customer status\n\n\n\n\n\n\n\n\n\n\nmean_age_customers = data_customers['age'].mean()\nmean_age_non_customers = data_non_customers['age'].mean()\n(mean_age_customers, mean_age_non_customers)\n\n(24.1497461928934, 26.691481197237145)\n\n\nIt appears that the average age of customers are lower than the average age of non customers.\n\n\n\n\n\n\n\n\n\nThe distribution of customers and non-customers across regions is not uniform. Both histograms reveal differences in the concentration of customers versus non-customers in various regions, suggesting regional preferences or market penetration differences.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe likelihood function for a set of independent and identically distributed observations ( Y_1, Y_2, , Y_n ) from a Poisson distribution, where each ( Y_i ) represents the number of patents awarded to an engineering firm in a given period and follows a Poisson distribution with parameter ( ), is given by:\n\\[\nL(\\lambda | Y_1, Y_2, \\ldots, Y_n) = \\prod_{i=1}^n f(Y_i | \\lambda)\n\\]\nGiven the probability mass function of the Poisson distribution ( f(Y|) = e{-}Y/Y! ), the likelihood function can be written as:\n\\[\nL(\\lambda | Y_1, Y_2, \\ldots, Y_n) = \\prod_{i=1}^n \\left( \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!} \\right)\n\\]\nThis can be simplified to:\n\\[\nL(\\lambda | Y_1, Y_2, \\ldots, Y_n) = e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i} \\prod_{i=1}^n \\frac{1}{Y_i!}\n\\]\nHere, ( n ) is the total number of observations (engineering firms), ( {i=1}^n Y_i ) is the total number of patents awarded across all firms, and ( {i=1}^n ) is the product of the factorials of the counts of patents for each firm.\nThe log-likelihood function for the Poisson model, coded in a function of lambda and Y would look like the following:\n\nimport numpy as np\nfrom scipy.special import factorial\n\ndef poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        return -np.inf  # log-likelihood is negative infinity if lambda is non-positive\n    return np.sum(-lambda_ + Y * np.log(lambda_) - np.log(factorial(Y)))\n\nBelow I use the function above to plot lambda on the horizontal axis and the likelihood on the vertical axis for a range of lambdas, which I used the number of patents as the input.\n\n\n\n\n\n\n\n\n\nNow, I used scipy.optimize to find the MLE after optimizing the likelihood function\n\nfrom scipy.optimize import minimize\n\n# Define the negative log-likelihood function since we minimize in the optimization\ndef negative_loglikelihood(lambda_, Y):\n    if lambda_[0] &lt;= 0:\n        return np.inf  # Return infinity if lambda is non-positive\n    lambda_val = lambda_[0]\n    return -np.sum(-lambda_val + Y * np.log(lambda_val) - np.log(factorial(Y)))\n\n# Initial guess for lambda\ninitial_lambda = [1.0]\n\n# Using scipy's minimize function to find the MLE of lambda\nresult = minimize(negative_loglikelihood, initial_lambda, args=(patents_data,), method='L-BFGS-B', bounds=[(0, None)])\n\nprint(f\"MLE for lambda: {result['x'][0]}\")\n\nMLE for lambda: 3.6846667021660804\n\n\nThe optimization has successfully found the maximum likelihood estimate (MLE) of 𝜆 for the Poisson distribution based on the patent data. The MLE of 𝜆 is approximately 3.685. This indicates that the best estimate for the average number of patents awarded per engineering firm over the observed period, under the assumption of a Poisson distribution, is about 3.685 patents. ​\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nFirst, I need to update my previous log-likelihood function to reflect that:\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    linear = X @ beta\n    lambda_i = np.exp(np.clip(linear, None, 20))\n    \n    log_likelihood = np.sum(Y * np.log(lambda_i) - lambda_i - np.log(factorial(Y)))\n    return -log_likelihood  \n\nI then used the updated function to find the MLE vector and the Hessian of the Poisson model with covariates. I also printed out the coefficient and the standard effor for each variable.\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom scipy.linalg import inv\n\n\ndf['age_squared'] = df['age'] ** 2\n\nencoder = OneHotEncoder(drop='first')\nregion_encoded = encoder.fit_transform(df[['region']]).toarray()\n\n\nY = df['patents'].values\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the continuous variables\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(df[['age', 'age_squared']])\n\n# Construct the design matrix with scaled features\nX = np.hstack([np.ones((df.shape[0], 1)), scaled_features, df[['iscustomer']].values, region_encoded])\n\ninitial_beta = np.zeros(X.shape[1])\n\n# Run the optimizer with detailed logging\nresult = minimize(\n    fun=poisson_regression_loglikelihood,\n    x0=initial_beta,\n    args=(Y, X),\n    method='L-BFGS-B',\n    bounds=[(None, None)] * X.shape[1],\n    options={'disp': True}\n)\n\nhess_inv = result.hess_inv.todense()  # if using L-BFGS, convert to dense matrix\n\ndef neg_log_likelihood(beta, Y, X):\n    lambda_ = np.exp(np.dot(X, beta))\n    return np.sum(-Y * np.log(lambda_) + lambda_ + gammaln(Y + 1))\n\ndef grad_neg_log_likelihood(beta, Y, X):\n    lambda_ = np.exp(np.dot(X, beta))\n    grad = np.dot(X.T, lambda_ - Y)\n    return grad\n\ndef hessian_neg_log_likelihood(beta, Y, X):\n    lambda_ = np.exp(np.dot(X, beta))\n    diag_lambda = np.diag(lambda_)\n    hessian = np.dot(X.T, np.dot(diag_lambda, X))\n    return hessian\n\nhessian_matrix = hessian_neg_log_likelihood(result.x, Y, X)\n\ncovariance_matrix_from_hessian = inv(hessian_matrix)\n\nstandard_errors_from_hessian = np.sqrt(np.diag(covariance_matrix_from_hessian))\nvariables = ['Age', 'Age Squared', 'Customer Status', 'Region Northeast', 'Region Northwest', 'Region South', 'Region Southwest']\n# Print the coefficients and their standard errors\nfor v, coef, std_err in zip(variables, result.x, standard_errors_from_hessian):\n    print(f\"{v}| Coefficient: {coef:.4f}, Standard Error: {std_err:.4f}\")\n\nAge| Coefficient: 1.2155, Standard Error: 0.0364\nAge Squared| Coefficient: 1.0464, Standard Error: 0.1005\nCustomer Status| Coefficient: -1.1408, Standard Error: 0.1025\nRegion Northeast| Coefficient: 0.1182, Standard Error: 0.0389\nRegion Northwest| Coefficient: 0.0986, Standard Error: 0.0420\nRegion South| Coefficient: -0.0201, Standard Error: 0.0538\nRegion Southwest| Coefficient: 0.0572, Standard Error: 0.0527\n\n\nNow we try to replicate the coefficients and standard errors with statsmodels.GLM()\n\nimport statsmodels.api as sm\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\n\n# Fit the model\nresult = poisson_model.fit()\n\n# Display the summary\n# Extract standard errors\ncoefficients = result.params\nstandard_errors = result.bse\np_values = result.pvalues\nconf_int = pd.DataFrame(result.conf_int(), columns=['95% CI Lower', '95% CI Upper'])\n\nstats_table = pd.DataFrame({\n    'Coefficient': coefficients,\n    'Standard Error': standard_errors,\n    'P-value': p_values,\n    '95% CI Lower': conf_int['95% CI Lower'],\n    '95% CI Upper': conf_int['95% CI Upper']\n})\n\nprint(stats_table)\n\n   Coefficient  Standard Error        P-value  95% CI Lower  95% CI Upper\n0     1.215438        0.036426  4.023701e-244      1.144045      1.286831\n1     1.046460        0.100487   2.144106e-25      0.849508      1.243412\n2    -1.140845        0.102495   8.884562e-29     -1.341731     -0.939960\n3     0.118114        0.038920   2.407229e-03      0.041832      0.194397\n4     0.098596        0.042007   1.891865e-02      0.016264      0.180928\n5    -0.020094        0.053783   7.086909e-01     -0.125508      0.085319\n6     0.057172        0.052676   2.777636e-01     -0.046071      0.160414\n7     0.051347        0.047212   2.767834e-01     -0.041188      0.143882\n\n\nAs seen in the table, coefficients and standard errors perfectly match the ones above.\n\n\n\nThe coefficient for the variable (index 3) representing whether or not it is a customer of Blueprinty is positive (0.1181) and statistically significant, suggests that firms using Blueprinty’s software likely have a higher expected patent count. The coefficient further implies that using Blueprinty’s software increases the patent count likelihood by approximately exp(0.1181) ≈ 1.125, or 12.5%."
  },
  {
    "objectID": "projects/MA/hw2_questions.html#airbnb-case-study",
    "href": "projects/MA/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\n\n\nLoading Data\n\n\n(40628, 13)\n\n\n\n\n\n\n\n\n\n\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\nThe dataframe has 40628 rows and 13 columns, I’ve also shown the first 5 rows of the dataframe\n\n\nData Statistics\n\n\n\n\n\n\n\n\n\n\nid\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n4.062800e+04\n40628.000000\n40468.000000\n40552.000000\n40628.000000\n40628.000000\n30433.000000\n30374.000000\n30372.000000\n\n\nmean\n9.698889e+06\n1102.368219\n1.124592\n1.147046\n144.760732\n15.904426\n9.198370\n9.413544\n9.331522\n\n\nstd\n5.460166e+06\n1383.269358\n0.385884\n0.691746\n210.657597\n29.246009\n1.119935\n0.844949\n0.902966\n\n\nmin\n2.515000e+03\n1.000000\n0.000000\n0.000000\n10.000000\n0.000000\n2.000000\n2.000000\n2.000000\n\n\n25%\n4.889868e+06\n542.000000\n1.000000\n1.000000\n70.000000\n1.000000\n9.000000\n9.000000\n9.000000\n\n\n50%\n9.862878e+06\n996.000000\n1.000000\n1.000000\n100.000000\n4.000000\n10.000000\n10.000000\n10.000000\n\n\n75%\n1.466789e+07\n1535.000000\n1.000000\n1.000000\n170.000000\n17.000000\n10.000000\n10.000000\n10.000000\n\n\nmax\n1.800967e+07\n42828.000000\n8.000000\n10.000000\n10000.000000\n421.000000\n10.000000\n10.000000\n10.000000\n\n\n\n\n\n\n\n\n\n\nData Cleaning\n\ndf.isna().sum()\n\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\nThe dataset appears to contain several columns with null values.\nSince the dataset is large enough, I’ve decided to drop all null values to see if the remaining dataset is still large enough to proceed.\n\ndf_1 = df.dropna()\ndf_1.shape\n\n(30140, 13)\n\n\nWith over 30000 rows, I’ve decided that the dataset is large enough to continue.\n\n\nUpdated Data Statistics\n\n\n\n\n\n\n\n\n\n\nid\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n3.014000e+04\n30140.000000\n30140.000000\n30140.000000\n30140.000000\n30140.000000\n30140.000000\n30140.000000\n30140.000000\n\n\nmean\n8.978322e+06\n1112.048275\n1.122213\n1.151460\n140.211546\n21.168115\n9.201758\n9.415428\n9.334041\n\n\nstd\n5.376960e+06\n644.430782\n0.385031\n0.699039\n188.437967\n32.004711\n1.114472\n0.843181\n0.900595\n\n\nmin\n2.515000e+03\n7.000000\n0.000000\n0.000000\n10.000000\n1.000000\n2.000000\n2.000000\n2.000000\n\n\n25%\n4.276596e+06\n584.000000\n1.000000\n1.000000\n70.000000\n3.000000\n9.000000\n9.000000\n9.000000\n\n\n50%\n9.149773e+06\n1040.000000\n1.000000\n1.000000\n103.000000\n8.000000\n10.000000\n10.000000\n10.000000\n\n\n75%\n1.391476e+07\n1591.000000\n1.000000\n1.000000\n169.000000\n26.000000\n10.000000\n10.000000\n10.000000\n\n\nmax\n1.797369e+07\n3317.000000\n6.000000\n10.000000\n10000.000000\n421.000000\n10.000000\n10.000000\n10.000000\n\n\n\n\n\n\n\n\n\n\nEDA\n\n\n\n\n\n\n\n\n\nThe distribution of the numbers of reviews is heavility skewed to the right, indicating that most listings have relatively few reviews (&lt;100).\nThere also does not seem to have a clean linear relationship between price and the number of reviews. Although houses with more reviews does tend to have lower price, likely because people tend to want to book cheaper houses.\nAdditionally, there does not appear to be a linear relationship between the number of reviews and the number of days listed. Although as the numbers of days listed increases, the number of days also likely increases.\nFinally, houses with around 2-3 bathrooms receive the most reviews.\n\n\nPoisson Model\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndf_1['room_type'] = df_1['room_type'].astype('category')\ndf_1['instant_bookable'] = df_1['instant_bookable'].astype('category').cat.codes\n\nformula = 'number_of_reviews ~ room_type + price + review_scores_cleanliness + review_scores_location + review_scores_value + instant_bookable + days + bathrooms + bedrooms'\npoisson_model = smf.glm(formula=formula, data=df_1, family=sm.families.Poisson()).fit()\n\n# Output model summary\nmodel_params = poisson_model.summary2().tables[1][['Coef.', 'Std.Err.', 'z', 'P&gt;|z|']]\nmodel_params\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n\n\n\n\nIntercept\n2.942697\n0.016633\n176.919813\n0.000000e+00\n\n\nroom_type[T.Private room]\n0.019233\n0.002738\n7.024594\n2.146901e-12\n\n\nroom_type[T.Shared room]\n-0.115193\n0.008650\n-13.317770\n1.824774e-40\n\n\nprice\n-0.000043\n0.000008\n-5.199251\n2.000937e-07\n\n\nreview_scores_cleanliness\n0.110978\n0.001517\n73.158720\n0.000000e+00\n\n\nreview_scores_location\n-0.081481\n0.001617\n-50.402764\n0.000000e+00\n\n\nreview_scores_value\n-0.091055\n0.001847\n-49.310566\n0.000000e+00\n\n\ninstant_bookable\n0.459104\n0.002919\n157.293238\n0.000000e+00\n\n\ndays\n0.000522\n0.000002\n280.429721\n0.000000e+00\n\n\nbathrooms\n-0.113444\n0.003773\n-30.066678\n1.321761e-198\n\n\nbedrooms\n0.075659\n0.002033\n37.214455\n3.983758e-303\n\n\n\n\n\n\n\n\n\n\nAnalysis Summary\nThe analysis reveals that compared to entire homes/apartments, which serve as the base category, private rooms actually have a slight positive effect on the number of reviews (coefficient = +0.019). This suggests that private rooms might receive slightly more bookings compared to entire homes/apartments. Conversely, shared rooms have a substantial negative impact on the number of reviews (coefficient = -0.115), indicating significantly fewer bookings for shared accommodations.\nThe coefficients for review scores indicate varied effects on booking rates. High cleanliness scores are strongly associated with an increase in bookings (coefficient = +0.111), highlighting the importance guests place on cleanliness. Interestingly, better scores for location (coefficient = -0.081) and value (coefficient = -0.091) are associated with fewer bookings. This counterintuitive result may suggest that higher expectations for these aspects could negatively impact guest satisfaction or reflect a trade-off guests are making with other variables such as price.\nThe model outputs indicate that the number of bathrooms negatively affects the number of reviews (coefficient = -0.113). This could suggest that listings with more bathrooms may not necessarily increase the likelihood of bookings. This could be due to higher associated costs or perhaps the type of listings that typically feature multiple bathrooms.\nConversely, an increase in the number of bedrooms has a positive effect on the number of reviews (coefficient = +0.076), indicating that listings with more bedrooms tend to be more popular or accommodating for larger groups, thus potentially receiving more bookings.\nThe coefficient for days listed (days) is positive (coefficient = +0.000522), showing that the longer a listing has been on the platform, the more reviews it accumulates. This trend likely reflects a cumulative effect where older listings have had more time to accumulate reviews, thus suggesting a gradual build-up of bookings over time.\nMost importantly, listings that are instantly bookable tend to have more bookings, which makes sense as it allows guests to book without waiting for host approval, making the process quicker and more convenient."
  },
  {
    "objectID": "projects/MA/hw3_questions.html",
    "href": "projects/MA/hw3_questions.html",
    "title": "Multinomial Logit Examples",
    "section": "",
    "text": "This assignment uses uses the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans."
  },
  {
    "objectID": "projects/MA/hw3_questions.html#estimating-yogurt-preferences",
    "href": "projects/MA/hw3_questions.html#estimating-yogurt-preferences",
    "title": "Multinomial Logit Examples",
    "section": "1. Estimating Yogurt Preferences",
    "text": "1. Estimating Yogurt Preferences\n\nLikelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, size, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\n\n\nYogurt Dataset\nWe will use the yogurt_data dataset, which provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc.\n\n\n\n\n\n\nRaw Dataset\n\n\n\n\n\n\n\n\n\n    \n      \n      id\n      y1\n      y2\n      y3\n      y4\n      f1\n      f2\n      f3\n      f4\n      p1\n      p2\n      p3\n      p4\n    \n  Loading... (need help?)\n\n\n\n\n\n\n\n\n\nLet the vector of product features include brand dummy variables for yogurts 1-3 (we’ll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts’ prices:\n\\[\nx_j' = [\\mathbf{1}(\\text{Yogurt 1}), \\mathbf{1}(\\text{Yogurt 2}), \\mathbf{1}(\\text{Yogurt 3}), X_f, X_p]\n\\]\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)).\nWhat we would like to do is reorganize the data from a “wide” shape with \\(n\\) rows and multiple columns for each covariate, to a “long” shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we’ll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be “pivoted” or “melted” from wide to long.\n\n\n\n\n\n\nMelting the Data\n\n\n\n\n\n\n# Melt the data to long format\nyogurt_long = pd.melt(yogurt_data, id_vars=['id'], value_vars=['y1', 'y2', 'y3', 'y4'],var_name='product', value_name='chosen')\n# Extract product number\nyogurt_long['product'] = yogurt_long['product'].str.extract('(\\d)').astype(int)\n# Melt the features to long format\nfeatures = pd.melt(yogurt_data, id_vars=['id'], value_vars=['f1', 'f2', 'f3', 'f4'], var_name='f_product', value_name='featured')\n# Extract product number\nfeatures['f_product'] = features['f_product'].str.extract('(\\d)').astype(int)\n# Melt the prices to long format\nprices = pd.melt(yogurt_data, id_vars=['id'], value_vars=['p1', 'p2', 'p3', 'p4'], var_name='p_product', value_name='price')\n# Extract product number\nprices['p_product'] = prices['p_product'].str.extract('(\\d)').astype(int)\n# Merge the data togehter\nyogurt_long = yogurt_long.merge(features, left_on=['id', 'product'], right_on=['id', 'f_product'])\nyogurt_long = yogurt_long.merge(prices, left_on=['id', 'product'], right_on=['id', 'p_product'])\nyogurt_long.drop(columns=['f_product', 'p_product'], inplace=True)\n# Create dummy variables for products 1, 2, and 3\nyogurt_long['yogurt 1'] = (yogurt_long['product'] == 1).astype(int)\nyogurt_long['yogurt 2'] = (yogurt_long['product'] == 2).astype(int)\nyogurt_long['yogurt 3'] = (yogurt_long['product'] == 3).astype(int)\n\n\n\n\n\n\n\n\n\n\nCleaned Dataset\n\n\n\n\n\n\n\n\n\n    \n      \n      id\n      product\n      chosen\n      featured\n      price\n      yogurt 1\n      yogurt 2\n      yogurt 3\n    \n  Loading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\nEstimation\nThe log likelihood function that I will use it shown below:\n\ndef log_likelihood(beta, data):\n    \"\"\"\n    Calculates the log-likelihood of the MNL model.\n\n    Parameters:\n    beta: Coefficients β1, β2, β3, βf, βp.\n    data: The reshaped long format dataframe with columns ['id', 'product', 'chosen', 'featured', 'price', 'yogurt 1', 'yogurt 2', 'yogurt 3'].\n\n    Returns:\n    float: The log-likelihood value.\n    \"\"\"\n    beta1, beta2, beta3, beta_f, beta_p = beta\n    data['utility'] = (beta1 * data['yogurt 1'] + \n                       beta2 * data['yogurt 2'] + \n                       beta3 * data['yogurt 3'] + \n                       beta_f * data['featured'] + \n                       beta_p * data['price'])\n    data['exp_utility'] = np.exp(data['utility'])\n    data['sum_exp_utility'] = data.groupby('id')['exp_utility'].transform('sum')\n    data['probability'] = data['exp_utility'] / data['sum_exp_utility']\n    data['log_likelihood'] = data['chosen'] * np.log(data['probability'])\n    return -data['log_likelihood'].sum()\n\nWith the function defined, I then used scipy.optimize from Python to find the beta values for the 5 parameters. First, I initilized all the guesses as zero, then I used the minimize() fucntion from scipy.optimize to find the beta values. The values are shown below.\n\ninitial = [0,0,0,0,0]\nresult = minimize(log_likelihood, initial, args = (yogurt_long), method = 'BFGS')\nbeta_1, beta_2, beta_3, beta_f, beta_p = result.x\n\n\n\nEstimated parameters:\nbeta_1: 1.3877520023064622\nbeta_2: 0.6435049292323878\nbeta_3: -3.0861128992213978\nbeta_f: 0.48741409137900876\nbeta_p: -37.0578717065307\n\n\n\n\nDiscussion\nFrom the beta intercept values, we learned that:\n\nYogust 1 is the most preferred as (\\(\\beta_1\\)) has the largest positive value\nYogurt 2 is also second most preferred as (\\(\\beta_2\\)) is also positive. However, it is less preferred compared to Yogurt 1\nYogurt 3 is the least preferred, as suggested by the negative (\\(\\beta_3\\))\nYogurt 4 is third preferred. As it is the base comparison and there are 2 yogurts with positive coefficients and 1 yogurt with negative coefficient, making yogurt 4 the third preferred\n(\\(\\beta_f\\)) having a positive value of 0.487 indicates that featuring a yogurt increases its utility and thus its chance of being selected\n(\\(\\beta_p\\)) having a large negative value of 37.058 indicates higher prices reduces the chance of a yogurt being selected\n\nThe estimated price coefficient (\\(\\beta_p\\)) can be used as a dollar-per-util conversion factor. Using this conversion factor, I then calculated the dollar benefit between the most preferred yogurt (Yogurt 1) and the least preferred yogurt (yogurt 3). The per-unit monetary measure of the brand value and the calculation is shown below.\n\nconversion_factor = -1 / beta_p\nutility_difference = beta_1 - beta_3\nmonetary_value = utility_difference * conversion_factor\nmonetary_value\n\n0.12072643936374339\n\n\nThe monetary benefit between the most preferred yogurt (Yogurt 1) and the least preferred yogurt (yogurt 3) is approximately $0.12 per unit, meaning that customers value Yogurt 1 about $0.12 more than Yogurt 3.\nIn addion, with the MNL model, I was able to simulate the counterfactuals (eg, what if the price of yogurt 1 was $0.10/oz instead of $0.08/oz).\nThe first step in achieving this is to define a function that can predit market shares. The function is shown below:\n\ndef predict_market_shares(beta, data):\n    \"\"\"\n    Predict market shares using the estimated beta coefficients.\n\n    Parameters:\n    beta: Coefficients β1, β2, β3, βf, βp.\n    data: The reshaped long format dataframe with columns ['id', 'product', 'chosen', 'featured', 'price', 'yogurt 1', 'yogurt 2', 'yogurt 3'].\n\n    Returns:\n    DataFrame: The predicted market shares for each product.\n    \"\"\"\n    data['utility'] = (beta[0] * data['yogurt 1'] + \n                       beta[1] * data['yogurt 2'] + \n                       beta[2] * data['yogurt 3'] + \n                       beta[3] * data['featured'] + \n                       beta[4] * data['price'])\n    data['exp_utility'] = np.exp(data['utility'])\n    data['sum_exp_utility'] = data.groupby('id')['exp_utility'].transform('sum')\n    data['probability'] = data['exp_utility'] / data['sum_exp_utility']\n    market_shares = data.groupby('product')['probability'].mean().reset_index()\n    market_shares.columns = ['product', 'market_share']\n    return market_shares\n\nRunning the predict market shares function, the original market shares are:\n\noriginal_market_shares = predict_market_shares(result.x, yogurt_long)\noriginal_market_shares\n\n\n\n\n\n\n\n\n\nproduct\nmarket_share\n\n\n\n\n0\n1\n0.341975\n\n\n1\n2\n0.401235\n\n\n2\n3\n0.029218\n\n\n3\n4\n0.227572\n\n\n\n\n\n\n\n\nand the new market shares after price increase are:\n\nyogurt_long.loc[yogurt_long['product'] == 1, 'price'] += 0.10\nnew_market_shares = predict_market_shares(result.x, yogurt_long)\nnew_market_shares\n\n\n\n\n\n\n\n\n\nproduct\nmarket_share\n\n\n\n\n0\n1\n0.021118\n\n\n1\n2\n0.591145\n\n\n2\n3\n0.044040\n\n\n3\n4\n0.343697\n\n\n\n\n\n\n\n\nIncreasing the price of Yogurt 1 by $0.10 significantly decreased its market share from 34% to 2%, leading to an increase in market share for the other yogurts. This indicates that Yogurt 1 is highly price-sensitive, suggesting that customers perceive it as less differentiated or less valuable compared to other options, making them more likely to switch to alternatives when its price rises."
  },
  {
    "objectID": "projects/MA/hw3_questions.html#estimating-minivan-preferences",
    "href": "projects/MA/hw3_questions.html#estimating-minivan-preferences",
    "title": "Multinomial Logit Examples",
    "section": "2. Estimating Minivan Preferences",
    "text": "2. Estimating Minivan Preferences\n\nData\n\n\n\n\n\n\nConjoint Dataset\n\n\n\n\n\n\n\n\n\n    \n      \n      resp.id\n      ques\n      alt\n      carpool\n      seat\n      cargo\n      eng\n      price\n      choice\n    \n  Loading... (need help?)\n\n\n\n\n\n\n\n\n\n\nVariables\n\nresp.id: Respondent identifier\nques: task number\nalt: Alternative number\ncarpool: Carpool option (yes/no)\nseat: Number of seats (6, 7, 8)\ncargo: Cargo space (2ft, 3ft)\neng: Engine type (gas, hybrid, electric)\nprice: Price in thousands of dollars\nchoice: 1 if alternative was chosen 0 if not\n\nThe attributes (levels) were number of seats (6,7,8), cargo space (2ft, 3ft), engine type (gas, hybrid, electric), and price (in thousands of dollars).\n\nnum_resp = conjoint_data['resp.id'].nunique()\nnum_choice = conjoint_data['ques'].nunique()\nnum_alt = conjoint_data['alt'].nunique()\n\n\n\nNumber of respondents: 200\nNumber of choice tasks per respondent: 15\nNumber of alternatives per choice task: 3\n\n\n\n\n\nModel\nTo estimate an MNL moel, I omitted the following levels to avoid multicollinearity: 1. 6 in seat 2. 2ft in cargo 3. Gas in eng\nThe varialbes in the model are: 1. seat_7: Dummy varialbe for 7 seats 2. seat_8: Dummy varialbe for 8 seats 3. cargo_3ft: Dummy varialbe for 3ft cargo space 4. eng_hyb: Dummy variable for hybrid engine 5. eng_elec: Dummy variable for electric engine 6. price: price in thousands of dollars\nI will be running the model with the slightly modified log_likelihood function from the Yogurt report.\nThe function is shown below\n\ndef log_likelihood(beta, data):\n    \"\"\"\n    Calculates the log-likelihood of the MNL model.\n\n    Parameters:\n    beta: Coefficients β_seat_7, β_seat_8, β_cargo_3ft, β_eng_hyb, β_eng_elec, β_price\n    data: The conjoint data with the variables listed above\n    \n    Returns:\n    float: The log-likelihood value.\n    \"\"\"\n    beta_seat_7, beta_seat_8, beta_cargo_3ft, beta_eng_hyb, beta_eng_elec, beta_price = beta\n    data['utility'] = (beta_seat_7 * data['seat_7'] +\n                       beta_seat_8 * data['seat_8'] +\n                       beta_cargo_3ft * data['cargo_3ft'] +\n                       beta_eng_hyb * data['eng_hyb'] +\n                       beta_eng_elec * data['eng_elec'] +\n                       beta_price * data['price'])\n    data['exp_utility'] = np.exp(data['utility'])\n    data['sum_exp_utility'] = data.groupby(['resp.id', 'ques'])['exp_utility'].transform('sum')\n    data['probability'] = data['exp_utility'] / data['sum_exp_utility']\n    data['log_likelihood'] = data['choice'] * np.log(data['probability'])\n    return -data['log_likelihood'].sum()\n\nI then used scipy.optimize from Python to find the beta values for the 6 parameters. First, I initilized all the guesses as zero, then I used the minimize() fucntion from scipy.optimize to find the beta values. The values are shown below.\n\ninitial = np.zeros(6)\nresult = minimize(log_likelihood, initial, args=(conjoint_data), method='BFGS')\nestimated_beta_conjoint = result.x\n\n\nbeta_seat_7 = estimated_beta_conjoint[0]\nbeta_seat_8 = estimated_beta_conjoint[1]\nbeta_cargo_3ft = estimated_beta_conjoint[2]\nbeta_hybrid_engine = estimated_beta_conjoint[3]\nbeta_electric_engine = estimated_beta_conjoint[4]\nbeta_price = estimated_beta_conjoint[5]\n\nprint_statement = f\"\"\"- beta seat 7: {beta_seat_7}\n- beta seat 8: {beta_seat_8}\n- beta cargo 3ft: {beta_cargo_3ft}\n- beta hybrid engine: {beta_hybrid_engine}\n- beta electric engine: {beta_electric_engine}\n- beta price: {beta_price}\"\"\"\n\nprint(print_statement)\n\n- beta seat 7: -0.5345392758909884\n- beta seat 8: -0.30610738374179974\n- beta cargo 3ft: 0.4766936312151858\n- beta hybrid engine: -0.8107338846771628\n- beta electric engine: -1.5291247058947275\n- beta price: -0.17330526829538706\n\n\n\n\nResults\n\nCoefficient Interpretation\n\nSeats\n\n\n7 seats: The negative coefficient suggests that 7 seats are less preferred compared to 6 seats\n8 seats: The negative coefficient suggests that 8 seats are less preferred compared to 6 seats, but not as less preferred as 7 seats\n6 Seats is the most preferred, followed by 8 seats, and 7 seats is the least preferred\n\n\nCargo Space\n\n\n3ft cargo: The positive coefficient suggests that 3ft cargo space is preferred over 2 ft cargo space\n3ft cargo space is most preferred, 2ft cargo space is least preferred\n\n\nEngine\n\n\nHybrid Engine: The negative coefficient suggests that hybrid engines are less preferred compared to gas engines.\nElectric Engine: The negative coefficient suggests that electric engines are also less preferred compared to gas enginges, and also less preferred compared to hybrid engine as the coefficient is larger in magnitude.\nGas engine is the most preferred, followed by hybrid engine, and electric engine is the least preferred\n\n\nPrice\n\n\nThe negative coefficient indicates that higher prices decrease the utility of the minivan, meaning that it is less likely to be selected #### Dollar-Per-Util Calculation\n\nI then used the price coefficent as a dollar-per-util conversion factor and calcualted the value of 3ft cargo space compared to 2ft cargo space. The calculation and value are shown below:\n\nconversion_factor = -1 / estimated_beta_conjoint[5]\nutility_difference = estimated_beta_conjoint[2]\nvalue = utility_difference * conversion_factor * 1000\n\n\n\nThe monetary value of having 3ft of cargo space compared to 2ft of cargo space is approximately $2750.600924622175. This means that, on average, consumers value the additional 1ft of cargo space at $2750.600924622175\n\n\n\n\n\nMarket Share Prediction\nAssuming the market consists of the following 6 minivans, I used the model above (the beta coefficients) to predict the market shares of these 6 minivans.\n\n\n\nMinivan\nSeats\nCargo\nEngine\nPrice\n\n\n\n\nA\n7\n2\nHyb\n30\n\n\nB\n6\n2\nGas\n30\n\n\nC\n8\n2\nGas\n30\n\n\nD\n7\n3\nGas\n40\n\n\nE\n6\n2\nElec\n40\n\n\nF\n7\n2\nHyb\n35\n\n\n\n\n\n\n\n\n\nCalculation\n\n\n\n\n\n\nmarket = pd.DataFrame({\n    'minivan': ['A', 'B', 'C', 'D', 'E', 'F'],\n    'seat': [7, 6, 8, 7, 6, 7],\n    'cargo': ['2ft', '2ft', '2ft', '3ft', '2ft', '2ft'],\n    'eng': ['hyb', 'gas', 'gas', 'gas', 'elec', 'hyb'],\n    'price': [30, 30, 30, 40, 40, 35]\n})\nmarket['seat_7'] = (market['seat'] == 7).astype(int)\nmarket['seat_8'] = (market['seat'] == 8).astype(int)\nmarket['cargo_3ft'] = (market['cargo'] == '3ft').astype(int)\nmarket['eng_hyb'] = (market['eng'] == 'hyb').astype(int)\nmarket['eng_elec'] = (market['eng'] == 'elec').astype(int)\nmarket['utility'] = (estimated_beta_conjoint[0] * market['seat_7'] +\n                     estimated_beta_conjoint[1] * market['seat_8'] +\n                     estimated_beta_conjoint[2] * market['cargo_3ft'] +\n                     estimated_beta_conjoint[3] * market['eng_hyb'] +\n                     estimated_beta_conjoint[4] * market['eng_elec'] +\n                     estimated_beta_conjoint[5] * market['price'])\nmarket['exp_utility'] = np.exp(market['utility'])\nmarket['market_share'] = market['exp_utility'] / market['exp_utility'].sum()\n\n\n\n\nThe predicted market shares are shown below:\n\n\n\n\n    \n      \n      minivan\n      market_share\n    \n  Loading... (need help?)\n\n\n\n\n\n\n\nMinivan B has the highest predicted market share at 43.3%.\nMinivan C also have a decent amount of market share at 31.9%.\nMinivans with higher prices and non gas engines tend to have lower market shares."
  },
  {
    "objectID": "projects/MA/hw4_questions.html",
    "href": "projects/MA/hw4_questions.html",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "This post implements a few measure of variable importance, interpreted as a key drivers analysis, for certain aspects of a payment card on customer satisfaction with that payment card.\nTo start with the computation of variable importance, I first loaded the data."
  },
  {
    "objectID": "projects/MA/hw4_questions.html#data",
    "href": "projects/MA/hw4_questions.html#data",
    "title": "Key Drivers Analysis",
    "section": "Data",
    "text": "Data\n\nKey Variables\nThe explanatory variables of the dataset are:\n\ntrust - Is this a brand I trust\nbuild - Does the card build credit quickly\ndiffers - Is it different from other cards\neasy - Is it easy to use\nappealing - Does it have appealing benefits/rewards\nrewarding - Does it reward me for responsible usage\npopular - Is it used by a lot of people\nservice - Does it provide outstanding customer service\nimpact - Does it make a difference in my life\n\nThe target variable is:\n\nsatisfaction - How satisfied the customer is with the card (1-5)\n\n\n\nData Description\n\n\n\n\n\n\nSurvey Data\n\n\n\n\n\n\n\n\n\n    \n      \n      brand\n      id\n      satisfaction\n      trust\n      build\n      differs\n      easy\n      appealing\n      rewarding\n      popular\n      service\n      impact\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.1.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\n\nAs seen above, the explanatory variables are all binary representing the survey responses.\n\n\nThere are 2553 rows in this data\nThere are 12 columns in this data\n\n\nThe extra column is brand, numbered from 1-10, it will be excluded from further analysis."
  },
  {
    "objectID": "projects/MA/hw4_questions.html#calculating-variable-importance",
    "href": "projects/MA/hw4_questions.html#calculating-variable-importance",
    "title": "Key Drivers Analysis",
    "section": "Calculating Variable Importance",
    "text": "Calculating Variable Importance\n\nPearson Correlation\nThe Pearson correlation coefficient measures the strength and direction of a linear relationship between two continuous variables. The coefficient ranges from -1 to 1, where:\n\n1 indicates a perfect positive linear relationship,\n-1 indicates a perfect negative linear relationship,\n0 indicates no linear relationship.\n\nValues closer to 1 or -1 suggest a stronger linear relationship, while values near 0 indicate a weaker relationship. Positive values mean that as one variable increases, so does the other, and negative values indicate that as one variable increases, the other decreases.\nTo calculate the Pearson Correlation, I used the corr method of the pandas DataFrame and specified the method as pearson.\nThe calculation is shown below:\n\ncorrelation_matrix = data.corr(method='pearson')['satisfaction'].drop(['satisfaction', 'brand', 'id'])\n\n\n\n\n\n\n\n\n\n\n\nPearson Correlation\n\n\n\n\ntrust\n0.255706\n\n\nbuild\n0.191896\n\n\ndiffers\n0.184801\n\n\neasy\n0.212985\n\n\nappealing\n0.207997\n\n\nrewarding\n0.194561\n\n\npopular\n0.171425\n\n\nservice\n0.251098\n\n\nimpact\n0.254539\n\n\n\n\n\n\n\n\nThe results show that features trust, service, and impact have the highest positive correlations with customer satisfaction\n\n\nPolychoric Correlations\nPolychoric correlations are used to estimate the relationship between two ordinal variables by assuming that each is a discretized representation of an underlying continuous variable. This statistical method is particularly useful for data derived from surveys using Likert scales or similar ordinal scales, where the actual data points represent categories that approximate a continuous scale. Polychoric correlations operate by estimating the thresholds that separate these continuous latent variables into observed ordinal categories and then calculating the correlation between these latent variables.\nTo calculate the Polychoric Correlations, I used the semopy module from Python\nThe calculation is shown below:\n\nfrom semopy import Model\nfrom semopy.examples import multivariate_regression\n\ndesc = '''satisfaction ~ trust + build + differs + easy + appealing + rewarding + popular + service + impact'''\nmod = Model(desc)\nmod.fit(data)\nprint(mod.inspect())\n\n           lval  op          rval  Estimate  Std. Err    z-value       p-value\n0  satisfaction   ~         trust  0.272677  0.056080   4.862253  1.160573e-06\n1  satisfaction   ~         build  0.045942  0.053785   0.854175  3.930081e-01\n2  satisfaction   ~       differs  0.069876  0.055060   1.269073  2.044149e-01\n3  satisfaction   ~          easy  0.052377  0.056755   0.922853  3.560838e-01\n4  satisfaction   ~     appealing  0.078803  0.055862   1.410653  1.583469e-01\n5  satisfaction   ~     rewarding  0.010950  0.056160   0.194979  8.454093e-01\n6  satisfaction   ~       popular  0.038642  0.051026   0.757302  4.488689e-01\n7  satisfaction   ~       service  0.209313  0.056714   3.690688  2.236485e-04\n8  satisfaction   ~        impact  0.319913  0.056430   5.669205  1.434619e-08\n9  satisfaction  ~~  satisfaction  1.222821  0.034226  35.728140  0.000000e+00\n\n\nThe results show that features trust, service, and impact have the highest positive Polychoric correlations with customer satisfaction\n\n\nStandardized Regression Coefficients\nStandardized regression coefficients, often referred to as beta coefficients, are used in multiple regression analyses to assess the relative importance and impact of each independent variable on the dependent variable. These coefficients are derived from a regression model in which all variables (both independent and dependent) have been standardized to have a mean of zero and a standard deviation of one. This standardization removes the units, allowing the coefficients to be compared directly.\nTo compute the Standardized Regression Coefficients, I used a linear regression model from sklearn and also scaled the data with StandardScaler from sklearn\nThe computation is shown below\n\nX = data.drop(['satisfaction', 'brand', 'id'], axis=1)\ny = data['satisfaction']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nmodel = LinearRegression()\nmodel.fit(X_scaled, y)\n\nstandardized_coefficients = model.coef_\n\n\n\n\n\n\n\n\n\n\n\nStandardized Coefficients\n\n\n\n\ntrust\n0.135635\n\n\nbuild\n0.023411\n\n\ndiffers\n0.032631\n\n\neasy\n0.025744\n\n\nappealing\n0.039647\n\n\nrewarding\n0.005937\n\n\npopular\n0.019470\n\n\nservice\n0.103573\n\n\nimpact\n0.150482\n\n\n\n\n\n\n\n\nThe results show that features trust, service, and impact have the highest significant positive effect on satisfaction when all other variables are held constant.\n\n\nUsefulness (Shapley Values)\nShapley values, a concept borrowed from cooperative game theory, offer a powerful method for interpreting machine learning models. These values measure the contribution of each feature to the prediction of a particular instance, by considering all possible combinations of features.\nTo compute the Shapley Values, I used the Shap module from Python and passed in the linear regression model above.\nThe computation is shown below\n\nexplainer = shap.Explainer(model, X_scaled)\n\n# Compute SHAP values\nshap_values = explainer(X_scaled)\n\n# Get the mean absolute SHAP values for each feature across all data points\nshap_values = np.abs(shap_values.values).mean(axis=0)\n\n\n\n\n\n\n\n\n\n\n\nShapley Values\n\n\n\n\ntrust\n0.136576\n\n\nbuild\n0.023157\n\n\ndiffers\n0.028857\n\n\neasy\n0.025924\n\n\nappealing\n0.039060\n\n\nrewarding\n0.005861\n\n\npopular\n0.019465\n\n\nservice\n0.102030\n\n\nimpact\n0.130708\n\n\n\n\n\n\n\n\nThe results show that features trust, service, and impact have the highest positive impact on the linear regression model’s prediction for satisfaction.\n\n\nUsefulness (Permutation Importance)\nPermutation importance is a technique used to measure the importance of individual features in a predictive model by evaluating the impact of shuffling each feature on the model’s performance. The process involves systematically randomizing the values of each feature across the dataset and observing the change in the model’s accuracy or other performance metrics. By disrupting the association between the feature and the target, the model’s performance typically decreases if the feature is important. The magnitude of the decrease, averaged over multiple shuffles, quantifies the feature’s importance.\nTo compute the Permutatio Importance, I used the permutation_importance function from sklearn.inspection and passed in the linear regression model above.\nThe computation is shown below:\n\nfrom sklearn.inspection import permutation_importance\n\n# Fit a linear model again to ensure it's correctly fit\nmodel.fit(X_scaled, y)\n\n# Compute permutation importance\nresults = permutation_importance(model, X_scaled, y, n_repeats=30, random_state=42)\n\n\n\n\n\n\n\n\n\n\n\nFeature\nPermutation Importance\n\n\n\n\n0\ntrust\n0.027039\n\n\n1\nbuild\n0.000866\n\n\n2\ndiffers\n0.001641\n\n\n3\neasy\n0.001097\n\n\n4\nappealing\n0.002386\n\n\n5\nrewarding\n0.000097\n\n\n6\npopular\n0.000523\n\n\n7\nservice\n0.016162\n\n\n8\nimpact\n0.033854\n\n\n\n\n\n\n\n\nThe results show that features trust, service, and impact have the highest permutation importance, suggesting that they are the most importance features in determining customer satisfaction in the linear regression model.\n\n\nJoshnson’s Relative Weights\nJohnson’s relative weights are a method used to assess the importance of predictor variables in a regression model, especially useful in contexts where predictors are correlated. This technique calculates the contribution of each predictor to the R-squared value, adjusted for the overlap with other predictors. Each predictor’s relative weight is computed by first transforming the predictor variables into orthogonal (uncorrelated) components. Then, the squared multiple correlation (R-squared) from a regression of the dependent variable on these orthogonal components is computed. Each original predictor’s relative contribution is assessed by reconstructing the R-squared from these orthogonal components, attributing portions of the variance explained back to the original correlated predictors.\nThe computation is shown below:\n\nbetas = model.coef_\nfeature_stds = X.std().values\n\n# Calculate the relative importance of each feature using Johnson's relative weights method\nraw_importance = np.square(betas * feature_stds)\nrelative_weights = raw_importance / raw_importance.sum()\n\n\n\n\n\n\n\n\n\n\n\nJohnson Relative Weight\n\n\n\n\ntrust\n0.343209\n\n\nbuild\n0.010266\n\n\ndiffers\n0.017863\n\n\neasy\n0.012421\n\n\nappealing\n0.029334\n\n\nrewarding\n0.000658\n\n\npopular\n0.007105\n\n\nservice\n0.201248\n\n\nimpact\n0.377896\n\n\n\n\n\n\n\n\nThe results show that features trust, service, and impact have the highest relative weights, suggesting that they are the most importance features in explaining the prediction variance of customer satisfaction in the linear regression model.\n\n\nMean Decrease in Gini Coefficient\nThe Mean Decrease in Gini Coefficient is a feature importance metric used in tree-based models such as Decision Trees and Random Forests. It quantifies each feature’s contribution to the model by calculating the average decrease in node impurity, measured by the Gini impurity, when the model splits on that feature. Gini impurity indicates the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the node. A feature with a higher Mean Decrease in Gini Coefficient is considered more important as it contributes more significantly to reducing uncertainty or “purifying” the outcomes at each split, thus enhancing the model’s predictive accuracy and efficiency in classifying or predicting the target variable.\nTo compute the Mean Decrease in Gini Coefficient, I trained a Random Forest and extracted the feature importance (which is the mean decrease in Gini Coefficient).\nThe computation is shown below:\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_model.fit(X_scaled, y)\n\n# Extract the feature importances (Mean Decrease in Gini Coefficient)\nfeature_importances = rf_model.feature_importances_\n\n\n\n\n\n\n\n\n\n\n\nFeature\nMean Decrease in Gini\n\n\n\n\n0\ntrust\n0.155865\n\n\n1\nbuild\n0.102301\n\n\n2\ndiffers\n0.089897\n\n\n3\neasy\n0.099904\n\n\n4\nappealing\n0.085534\n\n\n5\nrewarding\n0.101057\n\n\n6\npopular\n0.094944\n\n\n7\nservice\n0.129664\n\n\n8\nimpact\n0.140834\n\n\n\n\n\n\n\n\nThe results show that features trust, service, and impact have the highest impurity decrease, suggesting that they are the most importance features in reducing impurity in the RF, indicating that they are the most importance features in predicting customer satisfaction.\n\n\nXGBoost Feature Importance\nXGBoost feature importance quantifies the contribution of each feature to the model’s predictive power, typically calculated using three different metrics: gain, cover, and frequency. Gain measures the average contribution of a feature to the model’s performance, specifically how much each feature contributes to improving the accuracy of splits it is involved in, weighted by the number of observations affected. Cover evaluates the number of observations affected by a feature’s inclusion in splits, emphasizing the feature’s relevance across different data points. Frequency counts how often a feature is used in splits across all trees, providing insight into its general utility. To compute the Mean Decrease in Gini Coefficient, I trained a Random Forest and extracted the feature importance (which is the mean decrease in Gini Coefficient).\nTo compute the XGBoost Feature Importance, I trained a XGBoost Classifier and extracted the feature importance.\nThe computation is shown below:\n\nxgb_model = xgb.XGBRegressor(random_state=42)\nxgb_model.fit(X_scaled, y)\n\n# Get feature importances from the XGBoost model\nxgb_importances = pd.Series(xgb_model.feature_importances_)\n\n\n\n\n\n\n\n\n\n\n\nFeature\nXGBoost Feature Importance\n\n\n\n\n0\ntrust\n0.289867\n\n\n1\nbuild\n0.078933\n\n\n2\ndiffers\n0.056405\n\n\n3\neasy\n0.071498\n\n\n4\nappealing\n0.065786\n\n\n5\nrewarding\n0.065396\n\n\n6\npopular\n0.076136\n\n\n7\nservice\n0.111327\n\n\n8\nimpact\n0.184652\n\n\n\n\n\n\n\n\nThe results show that features trust, service, and impact have the highest feature importance, suggesting that they are the most importance in predicting customer satisfaction for the XGBoost model."
  },
  {
    "objectID": "projects/MA/hw4_questions.html#analysis-and-summary",
    "href": "projects/MA/hw4_questions.html#analysis-and-summary",
    "title": "Key Drivers Analysis",
    "section": "Analysis and Summary",
    "text": "Analysis and Summary\nBelow is the table that combines all the metrics created above:\n\n\n\n\n\n\n\n\n\n\nFeature\nPearson Correlation\nPolychoric Correlation\nStandardized Regression Coefficients\nShapley Values\nJohnson's Relative Weights\nPermutation Importance\nMean Decrease in Gini\nXGBoost Feature Importance\n\n\n\n\n0\ntrust\n0.255706\n0.272677\n0.135635\n0.136576\n0.343209\n0.027039\n0.155865\n0.289867\n\n\n1\nbuild\n0.191896\n0.045942\n0.023411\n0.023157\n0.010266\n0.000866\n0.102301\n0.078933\n\n\n2\ndiffers\n0.184801\n0.069876\n0.032631\n0.028857\n0.017863\n0.001641\n0.089897\n0.056405\n\n\n3\neasy\n0.212985\n0.052377\n0.025744\n0.025924\n0.012421\n0.001097\n0.099904\n0.071498\n\n\n4\nappealing\n0.207997\n0.078803\n0.039647\n0.039060\n0.029334\n0.002386\n0.085534\n0.065786\n\n\n5\nrewarding\n0.194561\n0.010950\n0.005937\n0.005861\n0.000658\n0.000097\n0.101057\n0.065396\n\n\n6\npopular\n0.171425\n0.038642\n0.019470\n0.019465\n0.007105\n0.000523\n0.094944\n0.076136\n\n\n7\nservice\n0.251098\n0.209313\n0.103573\n0.102030\n0.201248\n0.016162\n0.129664\n0.111327\n\n\n8\nimpact\n0.254539\n0.319913\n0.150482\n0.130708\n0.377896\n0.033854\n0.140834\n0.184652\n\n\n\n\n\n\n\n\nFrom the table above, we can see that Trust, Impact, and Service are consistantly ranked top 3 in all of the metrics created. Trust is either first or Impact is first and Service is always third.\nThis suggests that Trust, Impact and Service are the most important features for customer satisfaction when it comes to payment cards, indicating that customers value whether or not the brand is trustable, the impact the card will have in the customers life, and the customer service the company provides when they are considering their choice of cards.\nWith this information, payment card companies should:\n\nFocus on building and maintaining their company’s image as a trusted braand to the public\nOffer benefits that can positive impact customer’s life\nInvest in their customer service representatives and aim to offer more support to customers as well as increase the quality of support."
  }
]