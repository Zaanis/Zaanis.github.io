---
title: "Poisson Regression Examples"
author: "Joshua Chen"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
editor_options: 
  chunk_output_type: console
jupyter: python3
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data

```{python}
#| echo: false
import pandas as pd
df = pd.read_csv('blueprinty.csv', index_col= 0)
```
This is the first 5 rows of the data
```{python}
#| echo: false

print('The shape of the dataframe is: ' +  str(df.shape))
df.head(5)
```
There are 1500 rows and 4 columns

#### Summary Statistics

```{python}
#| echo: false
df.describe()
```

#### Key variables

- patents: Number of patents awarded over the last 5 years.

- region: Regional location of the firm.

- age: Age of the firm since incorporation.

- iscustomer: Indicates whether the firm uses Blueprinty's software (1 = customer, 0 = not a customer)

#### Distribution of Customers

```{python, echo = false}
#| echo: false
data_customers = df[df['iscustomer'] == 1]
data_non_customers = df[df['iscustomer'] == 0]
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.hist(df[df['iscustomer'] == 0]['patents'], bins=range(0, 17, 1), alpha=0.7, color='Red')
plt.title('Histogram of Patents (Non-Customers)')
plt.xlabel('Number of Patents')
plt.ylabel('Frequency')
plt.grid(True)

plt.subplot(1, 2, 2)
plt.hist(df[df['iscustomer'] == 1]['patents'], bins=range(0, 17, 1), alpha=0.7, color='Orange')
plt.title('Histogram of Patents (Customers)')
plt.xlabel('Number of Patents')
plt.ylabel('Frequency')
plt.grid(True)

plt.tight_layout()
plt.show()
```
```{python}
mean_customers = df[df['iscustomer'] == 1]['patents'].mean()
mean_non_customers = df[df['iscustomer'] == 0]['patents'].mean()
(mean_customers, mean_non_customers)
```
The mean number of patents for customers appears to be slightly higher at 4.09, while the mean number of patents for non-customers appear to be lower at 3.62.

#### Comparing Regions

However, Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

To investigate any systematic differences, below are plots that compare regions and ages by customer status

```{python}
#| echo: false
plt.figure(figsize=(12, 6))
plt.hist(data_customers['age'], alpha=1, label='Customers', bins=15, edgecolor='black')
plt.hist(data_non_customers['age'], alpha=0.4, label='Non-customers', bins=15, edgecolor='black')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.title('Histogram of Age by Customer Status')
plt.legend()
plt.show()
```
```{python}
mean_age_customers = data_customers['age'].mean()
mean_age_non_customers = data_non_customers['age'].mean()
(mean_age_customers, mean_age_non_customers)
```
It appears that the average age of customers are lower than the average age of non customers.
```{python}
#| echo: false
region_counts_customers = data_customers['region'].value_counts()
region_counts_non_customers = data_non_customers['region'].value_counts()
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))
region_counts_customers.plot(kind='bar', ax=axes[0], title='Customers by Region', color='blue')
region_counts_non_customers.plot(kind='bar', ax=axes[1], title='Non-Customers by Region', color='green')
axes[0].set_ylabel('Count')
axes[1].set_ylabel('Count')
plt.tight_layout()
plt.show()
```

The distribution of customers and non-customers across regions is not uniform. Both histograms reveal differences in the concentration of customers versus non-customers in various regions, suggesting regional preferences or market penetration differences.


### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

The likelihood function for a set of independent and identically distributed observations \( Y_1, Y_2, \ldots, Y_n \) from a Poisson distribution, where each \( Y_i \) represents the number of patents awarded to an engineering firm in a given period and follows a Poisson distribution with parameter \( \lambda \), is given by:

$$
L(\lambda | Y_1, Y_2, \ldots, Y_n) = \prod_{i=1}^n f(Y_i | \lambda)
$$

Given the probability mass function of the Poisson distribution \( f(Y|\lambda) = e^{-\lambda}\lambda^Y/Y! \), the likelihood function can be written as:

$$
L(\lambda | Y_1, Y_2, \ldots, Y_n) = \prod_{i=1}^n \left( \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!} \right)
$$

This can be simplified to:

$$
L(\lambda | Y_1, Y_2, \ldots, Y_n) = e^{-n\lambda} \lambda^{\sum_{i=1}^n Y_i} \prod_{i=1}^n \frac{1}{Y_i!}
$$

Here, \( n \) is the total number of observations (engineering firms), \( \sum_{i=1}^n Y_i \) is the total number of patents awarded across all firms, and \( \prod_{i=1}^n \frac{1}{Y_i!} \) is the product of the factorials of the counts of patents for each firm.


The log-likelihood function for the Poisson model, coded in a function of lambda and Y would look like the following:

```{python}
import numpy as np
from scipy.special import factorial

def poisson_loglikelihood(lambda_, Y):
    if lambda_ <= 0:
        return -np.inf  # log-likelihood is negative infinity if lambda is non-positive
    return np.sum(-lambda_ + Y * np.log(lambda_) - np.log(factorial(Y)))
```

Below I use the function above to plot lambda on the horizontal axis and the likelihood on the vertical axis for a range of lambdas, which I used the number of patents as the input.

```{python}
#| echo: false

patents_data = df['patents']
lambdas = np.linspace(0.1, 10, 100)
# Calculate log-likelihoods for each lambda using the actual dataset
log_likelihoods_actual = [poisson_loglikelihood(l, patents_data) for l in lambdas]

# Plotting the log-likelihood curve using the actual dataset
plt.figure(figsize=(10, 5))
plt.plot(lambdas, log_likelihoods_actual, label='Log-Likelihood with Actual Data')
plt.xlabel('Lambda (Rate Parameter)')
plt.ylabel('Log-Likelihood')
plt.title('Log-Likelihood of Poisson Distribution with Actual Patent Data')
plt.legend()
plt.show()
```

Now, I used scipy.optimize to find the MLE after optimizing the likelihood function
```{python}
from scipy.optimize import minimize

# Define the negative log-likelihood function since we minimize in the optimization
def negative_loglikelihood(lambda_, Y):
    if lambda_[0] <= 0:
        return np.inf  # Return infinity if lambda is non-positive
    lambda_val = lambda_[0]
    return -np.sum(-lambda_val + Y * np.log(lambda_val) - np.log(factorial(Y)))

# Initial guess for lambda
initial_lambda = [1.0]

# Using scipy's minimize function to find the MLE of lambda
result = minimize(negative_loglikelihood, initial_lambda, args=(patents_data,), method='L-BFGS-B', bounds=[(0, None)])

print(f"MLE for lambda: {result['x'][0]}")
```
The optimization has successfully found the maximum likelihood estimate (MLE) of ðœ† for the Poisson distribution based on the patent data. The MLE of ðœ† is approximately 3.685. This indicates that the best estimate for the average number of patents awarded per engineering firm over the observed period, under the assumption of a Poisson distribution, is about 3.685 patents. â€‹


### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.


First, I need to update my previous log-likelihood function to reflect that:
```{python}
def poisson_regression_loglikelihood(beta, Y, X):
    linear = X @ beta
    lambda_i = np.exp(np.clip(linear, None, 20))
    
    log_likelihood = np.sum(Y * np.log(lambda_i) - lambda_i - np.log(factorial(Y)))
    return -log_likelihood  
```

I then used the updated function to find the MLE vector and the Hessian of the Poisson model with covariates. I also printed out the coefficient and the standard effor for each variable.

```{python}
from sklearn.preprocessing import OneHotEncoder
from scipy.linalg import inv


df['age_squared'] = df['age'] ** 2

encoder = OneHotEncoder(drop='first')
region_encoded = encoder.fit_transform(df[['region']]).toarray()


Y = df['patents'].values




from sklearn.preprocessing import StandardScaler

# Scale the continuous variables
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df[['age', 'age_squared']])

# Construct the design matrix with scaled features
X = np.hstack([np.ones((df.shape[0], 1)), scaled_features, df[['iscustomer']].values, region_encoded])

initial_beta = np.zeros(X.shape[1])

# Run the optimizer with detailed logging
result = minimize(
    fun=poisson_regression_loglikelihood,
    x0=initial_beta,
    args=(Y, X),
    method='L-BFGS-B',
    bounds=[(None, None)] * X.shape[1],
    options={'disp': True}
)

hess_inv = result.hess_inv.todense()  # if using L-BFGS, convert to dense matrix

def neg_log_likelihood(beta, Y, X):
    lambda_ = np.exp(np.dot(X, beta))
    return np.sum(-Y * np.log(lambda_) + lambda_ + gammaln(Y + 1))

def grad_neg_log_likelihood(beta, Y, X):
    lambda_ = np.exp(np.dot(X, beta))
    grad = np.dot(X.T, lambda_ - Y)
    return grad

def hessian_neg_log_likelihood(beta, Y, X):
    lambda_ = np.exp(np.dot(X, beta))
    diag_lambda = np.diag(lambda_)
    hessian = np.dot(X.T, np.dot(diag_lambda, X))
    return hessian

hessian_matrix = hessian_neg_log_likelihood(result.x, Y, X)

covariance_matrix_from_hessian = inv(hessian_matrix)

standard_errors_from_hessian = np.sqrt(np.diag(covariance_matrix_from_hessian))
variables = ['Age', 'Age Squared', 'Customer Status', 'Region Northeast', 'Region Northwest', 'Region South', 'Region Southwest']
# Print the coefficients and their standard errors
for v, coef, std_err in zip(variables, result.x, standard_errors_from_hessian):
    print(f"{v}| Coefficient: {coef:.4f}, Standard Error: {std_err:.4f}")
```

Now we try to replicate the coefficients and standard errors with statsmodels.GLM()

```{python}
import statsmodels.api as sm

poisson_model = sm.GLM(Y, X, family=sm.families.Poisson())

# Fit the model
result = poisson_model.fit()

# Display the summary
# Extract standard errors
coefficients = result.params
standard_errors = result.bse
p_values = result.pvalues
conf_int = pd.DataFrame(result.conf_int(), columns=['95% CI Lower', '95% CI Upper'])

stats_table = pd.DataFrame({
    'Coefficient': coefficients,
    'Standard Error': standard_errors,
    'P-value': p_values,
    '95% CI Lower': conf_int['95% CI Lower'],
    '95% CI Upper': conf_int['95% CI Upper']
})

print(stats_table)
```
As seen in the table, coefficients and standard errors perfectly match the ones above.

### Analysis Summary

The coefficient for the variable (index 3) representing whether or not it is a customer of Blueprinty is positive (0.1181) and statistically significant, suggests that firms using Blueprinty's software likely have a higher expected patent count. The coefficient further implies that using Blueprinty's software increases the patent count likelihood by approximately exp(0.1181) â‰ˆ 1.125, or 12.5%.


## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::


_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._

### Loading Data

```{python, echo = false}
#| echo: false
import pandas as pd
import numpy as np
df = pd.read_csv('airbnb.csv', index_col = 0)
```
```{python}
#| echo: false
print(df.shape)
df.head(5)
```
The dataframe has 40628 rows and 13 columns, I've also shown the first 5 rows of the dataframe

### Data Statistics
```{python}
#| echo: false
df.describe()
```

### Data Cleaning
```{python}
df.isna().sum()
```
The dataset appears to contain several columns with null values.

Since the dataset is large enough, I've decided to drop all null values to see if the remaining dataset is still large enough to proceed.

```{python}
df_1 = df.dropna()
df_1.shape
```
With over 30000 rows, I've decided that the dataset is large enough to continue.

### Updated Data Statistics
```{python}
#| echo: false
df_1 = df.dropna()
df_1.describe()
```
### EDA 


```{python}
#| echo: false
import seaborn as sns
import matplotlib.pyplot as plt

# Set up the matplotlib figure
plt.figure(figsize=(12, 6))

# Distribution of the number of reviews
plt.subplot(2, 2, 1)
sns.histplot(df.dropna()['number_of_reviews'], kde=True)
plt.title('Distribution of Number of Reviews')
plt.xlabel('Number of Reviews')
plt.ylabel('Frequency')

# Relationship between number of reviews and price
plt.subplot(2, 2, 2)
sns.scatterplot(x='price', y='number_of_reviews', data=df.dropna())
plt.title('Relationship between Price and Number of Reviews')
plt.xlabel('Price')
plt.ylabel('Number of Reviews')

plt.subplot(2, 2, 3)
sns.scatterplot(x='days', y='number_of_reviews', data=df.dropna())
plt.title('Relationship between Days and Number of Reviews')
plt.xlabel('Days')
plt.ylabel('Number of Reviews')

plt.subplot(2, 2, 4)
sns.scatterplot(x='bathrooms', y='number_of_reviews', data=df.dropna())
plt.title('Relationship between Bathrooms and Number of Reviews')
plt.xlabel('Bathrooms')
plt.ylabel('Number of Reviews')
plt.tight_layout()
plt.show()
```

The distribution of the numbers of reviews is heavility skewed to the right, indicating that most listings have relatively few reviews (<100).

There also does not seem to have a clean linear relationship between price and the number of reviews. Although houses with more reviews does tend to have lower price, likely because people tend to want to book cheaper houses.

Additionally, there does not appear to be a linear relationship between the number of reviews and the number of days listed. Although as the numbers of days listed increases, the number of days also likely increases.

Finally, houses with around 2-3 bathrooms receive the most reviews.

### Poisson Model
```{python}
import statsmodels.api as sm
import statsmodels.formula.api as smf
import warnings
warnings.filterwarnings('ignore')


df_1['room_type'] = df_1['room_type'].astype('category')
df_1['instant_bookable'] = df_1['instant_bookable'].astype('category').cat.codes

formula = 'number_of_reviews ~ room_type + price + review_scores_cleanliness + review_scores_location + review_scores_value + instant_bookable + days + bathrooms + bedrooms'
poisson_model = smf.glm(formula=formula, data=df_1, family=sm.families.Poisson()).fit()

# Output model summary
model_params = poisson_model.summary2().tables[1][['Coef.', 'Std.Err.', 'z', 'P>|z|']]
model_params
```

### Analysis Summary
The analysis reveals that compared to entire homes/apartments, which serve as the base category, private rooms actually have a slight positive effect on the number of reviews (coefficient = +0.019). This suggests that private rooms might receive slightly more bookings compared to entire homes/apartments. Conversely, shared rooms have a substantial negative impact on the number of reviews (coefficient = -0.115), indicating significantly fewer bookings for shared accommodations.

The coefficients for review scores indicate varied effects on booking rates. High cleanliness scores are strongly associated with an increase in bookings (coefficient = +0.111), highlighting the importance guests place on cleanliness. Interestingly, better scores for location (coefficient = -0.081) and value (coefficient = -0.091) are associated with fewer bookings. This counterintuitive result may suggest that higher expectations for these aspects could negatively impact guest satisfaction or reflect a trade-off guests are making with other variables such as price.

The model outputs indicate that the number of bathrooms negatively affects the number of reviews (coefficient = -0.113). This could suggest that listings with more bathrooms may not necessarily increase the likelihood of bookings. This could be due to higher associated costs or perhaps the type of listings that typically feature multiple bathrooms.

Conversely, an increase in the number of bedrooms has a positive effect on the number of reviews (coefficient = +0.076), indicating that listings with more bedrooms tend to be more popular or accommodating for larger groups, thus potentially receiving more bookings.

The coefficient for days listed (days) is positive (coefficient = +0.000522), showing that the longer a listing has been on the platform, the more reviews it accumulates. This trend likely reflects a cumulative effect where older listings have had more time to accumulate reviews, thus suggesting a gradual build-up of bookings over time.

Most importantly, listings that are instantly bookable tend to have more bookings, which makes sense as it allows guests to book without waiting for host approval, making the process quicker and more convenient.