---
title: "Poisson Regression Examples"
author: "Joshua Chen"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
editor_options: 
  chunk_output_type: console
jupyter: python3
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data

```{python}
#| echo: false
import pandas as pd
df = pd.read_csv('blueprinty.csv', index_col= 0)
```
This is the first 5 rows of the data
```{python}
print('The shape of the dataframe is: ' +  str(df.shape))
df.head(5)
```
There are 1500 rows and 4 columns

Below is a histogram of patents by customer status.
```{python, echo = false}
#| echo: false

import matplotlib.pyplot as plt
data_customers = df[df['iscustomer'] == 1]
data_non_customers = df[df['iscustomer'] == 0]

# Create histograms for the number of patents
plt.figure(figsize=(12, 6))
plt.hist(data_customers['patents'], alpha=0.5, label='Customers', bins=20, edgecolor='black')
plt.hist(data_non_customers['patents'], alpha=0.5, label='Non-customers', bins=20, edgecolor='black')
plt.xlabel('Number of Patents')
plt.ylabel('Frequency')
plt.title('Histogram of Number of Patents by Customer Status')
plt.legend()
plt.show()
```
```{python}
mean_customers = data_customers['patents'].mean()
mean_non_customers = data_non_customers['patents'].mean()
(mean_customers, mean_non_customers)
```
The mean number of patents for customers appears to be slightly higher at 4.09, while the mean number of patents for non-customers appear to be lower at 3.62.

However, Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

To investigate any systematic differences, below are plots that compare regions and ages by customer status

```{python}
#| echo: false
plt.figure(figsize=(12, 6))
plt.hist(data_customers['age'], alpha=1, label='Customers', bins=15, edgecolor='black')
plt.hist(data_non_customers['age'], alpha=0.4, label='Non-customers', bins=15, edgecolor='black')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.title('Histogram of Age by Customer Status')
plt.legend()
plt.show()
```
```{python}
mean_age_customers = data_customers['age'].mean()
mean_age_non_customers = data_non_customers['age'].mean()
(mean_age_customers, mean_age_non_customers)
```
It appears that the average age of customers are lower than the average age of non customers.
```{python}
#| echo: false
region_counts_customers = data_customers['region'].value_counts()
region_counts_non_customers = data_non_customers['region'].value_counts()
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))
region_counts_customers.plot(kind='bar', ax=axes[0], title='Customers by Region', color='blue')
region_counts_non_customers.plot(kind='bar', ax=axes[1], title='Non-Customers by Region', color='green')
axes[0].set_ylabel('Count')
axes[1].set_ylabel('Count')
plt.tight_layout()
plt.show()
```

The distribution of customers and non-customers across regions is not uniform. Both histograms reveal differences in the concentration of customers versus non-customers in various regions, suggesting regional preferences or market penetration differences.


### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

_todo: Write down mathematically the likelihood for_ $Y \sim \text{Poisson}(\lambda)$. Note that $f(Y|\lambda) = e^{-\lambda}\lambda^Y/Y!$.
The likelihood function for a set of independent and identically distributed observations \( Y_1, Y_2, \ldots, Y_n \) from a Poisson distribution, where each \( Y_i \) represents the number of patents awarded to an engineering firm in a given period and follows a Poisson distribution with parameter \( \lambda \), is given by:

$$
L(\lambda | Y_1, Y_2, \ldots, Y_n) = \prod_{i=1}^n f(Y_i | \lambda)
$$

Given the probability mass function of the Poisson distribution \( f(Y|\lambda) = e^{-\lambda}\lambda^Y/Y! \), the likelihood function can be written as:

$$
L(\lambda | Y_1, Y_2, \ldots, Y_n) = \prod_{i=1}^n \left( \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!} \right)
$$

This can be simplified to:

$$
L(\lambda | Y_1, Y_2, \ldots, Y_n) = e^{-n\lambda} \lambda^{\sum_{i=1}^n Y_i} \prod_{i=1}^n \frac{1}{Y_i!}
$$

Here, \( n \) is the total number of observations (engineering firms), \( \sum_{i=1}^n Y_i \) is the total number of patents awarded across all firms, and \( \prod_{i=1}^n \frac{1}{Y_i!} \) is the product of the factorials of the counts of patents for each firm.

_todo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:_

The log-likelihood function for the Poisson model, coded in a function of lambda and Y would look like the following:

```{python}
import numpy as np
from scipy.special import factorial
def poisson_loglikelihood(lambda_, Y):
    if lambda_ <= 0:
        return -np.inf  # log-likelihood is negative infinity if lambda is non-positive
    return np.sum(-lambda_ + Y * np.log(lambda_) - np.log(factorial(Y)))
```

Below I use the function above to plot lambda on the horizontal axis and the likelihood on the vertical axis for a range of lambdas, which I used the number of patents as the input.
_todo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y)._

```{python}
#| echo: false

patents_data = df['patents']
lambdas = np.linspace(0.1, 10, 100)
# Calculate log-likelihoods for each lambda using the actual dataset
log_likelihoods_actual = [poisson_loglikelihood(l, patents_data) for l in lambdas]

# Plotting the log-likelihood curve using the actual dataset
plt.figure(figsize=(10, 5))
plt.plot(lambdas, log_likelihoods_actual, label='Log-Likelihood with Actual Data')
plt.xlabel('Lambda (Rate Parameter)')
plt.ylabel('Log-Likelihood')
plt.title('Log-Likelihood of Poisson Distribution with Actual Patent Data')
plt.legend()
plt.show()
```
_todo: If you're feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which "feels right" because the mean of a Poisson distribution is lambda._

_todo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python._
Now, I used scipy.optimize to find the MLE after optimizing the likelihood function
```{python}
from scipy.optimize import minimize

# Define the negative log-likelihood function since we minimize in the optimization
def negative_loglikelihood(lambda_, Y):
    if lambda_[0] <= 0:
        return np.inf  # Return infinity if lambda is non-positive
    lambda_val = lambda_[0]
    return -np.sum(-lambda_val + Y * np.log(lambda_val) - np.log(factorial(Y)))

# Initial guess for lambda
initial_lambda = [1.0]

# Using scipy's minimize function to find the MLE of lambda
result = minimize(negative_loglikelihood, initial_lambda, args=(patents_data,), method='L-BFGS-B', bounds=[(0, None)])

print(f"MLE for lambda: {result['x'][0]}")
```
The optimization has successfully found the maximum likelihood estimate (MLE) of 𝜆 for the Poisson distribution based on the patent data. The MLE of 𝜆 is approximately 3.685. This indicates that the best estimate for the average number of patents awarded per engineering firm over the observed period, under the assumption of a Poisson distribution, is about 3.685 patents. ​


### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

_todo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g() to be exp() so that_ $\lambda_i = e^{X_i'\beta}$. _For example:_

First, I need to update my previous log-likelihood function to reflect that:
```{python}
def poisson_regression_loglikelihood(beta, Y, X):
    linear = X @ beta
    lambda_i = np.exp(np.clip(linear, None, 20))
    
    log_likelihood = np.sum(Y * np.log(lambda_i) - lambda_i - np.log(factorial(Y)))
    return -log_likelihood  
```


_todo: Use your function along with R's optim() or Python's sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1's to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors._

```{python}
from sklearn.preprocessing import OneHotEncoder

df['age_squared'] = df['age'] ** 2

encoder = OneHotEncoder(drop='first')
region_encoded = encoder.fit_transform(df[['region']]).toarray()


Y = df['patents'].values




from sklearn.preprocessing import StandardScaler

# Scale the continuous variables
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df[['age', 'age_squared']])

# Construct the design matrix with scaled features
X = np.hstack([np.ones((df.shape[0], 1)), scaled_features, df[['iscustomer']].values, region_encoded])

initial_beta = np.zeros(X.shape[1])

# Run the optimizer with detailed logging
result = minimize(
    fun=poisson_regression_loglikelihood,
    x0=initial_beta,
    args=(Y, X),
    method='L-BFGS-B',
    bounds=[(None, None)] * X.shape[1],
    options={'disp': True}
)

hess_inv = result.hess_inv.todense()  # if using L-BFGS, convert to dense matrix

standard_errors = np.sqrt(np.diag(hess_inv))


# Print the coefficients and their standard errors
for coef, std_err in zip(result.x, standard_errors):
    print(f"Coefficient: {coef:.4f}, Standard Error: {std_err:.4f}")
```

Now we try to replicate the coefficients and standard errors with statsmodels.GLM()

```{python}
import statsmodels.api as sm

poisson_model = sm.GLM(Y, X, family=sm.families.Poisson())

# Fit the model
result = poisson_model.fit()

# Display the summary
# Extract standard errors
coefficients = result.params
standard_errors = result.bse
p_values = result.pvalues
conf_int = pd.DataFrame(result.conf_int(), columns=['95% CI Lower', '95% CI Upper'])

stats_table = pd.DataFrame({
    'Coefficient': coefficients,
    'Standard Error': standard_errors,
    'P-value': p_values,
    '95% CI Lower': conf_int['95% CI Lower'],
    '95% CI Upper': conf_int['95% CI Upper']
})

print(stats_table)
```
Note: Although the coefficients matched up, I am unsure as to how to match the standard errors.

The coefficient for the variable (index 3) representing whether or not it is a customer of Blueprinty is positive (0.1181) and statistically significant, suggests that firms using Blueprinty's software likely have a higher expected patent count. The coefficient further implies that using Blueprinty's software increases the patent count likelihood by approximately exp(0.1181) ≈ 1.125, or 12.5%.


## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::


_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._

### Loading Data

```{python, echo = false}
#| echo: false
import pandas as pd
import numpy as np
df = pd.read_csv('airbnb.csv', index_col = 0)
```
```{python}
print(df.shape)
df.head(5)
```
The dataframe has 40628 rows and 13 columns, I've also shown the first 5 rows of the dataframe

### Data Cleaning
```{python}
df.isna().sum()
```
The dataset appears to contain several columns with null values.

Since the dataset is large enough, I've decided to drop all null values to see if the remaining dataset is still large enough to proceed.

```{python}
df_1 = df.dropna()
df_1.shape
```
With over 30000 rows, I've decided that the dataset is large enough to continue.

### EDA 

Note: EDA is done with original data with dropped null value rows.

```{python}
#| echo: false
import seaborn as sns
import matplotlib.pyplot as plt

# Set up the matplotlib figure
plt.figure(figsize=(12, 6))

# Distribution of the number of reviews
plt.subplot(2, 2, 1)
sns.histplot(df.dropna()['number_of_reviews'], kde=True)
plt.title('Distribution of Number of Reviews')
plt.xlabel('Number of Reviews')
plt.ylabel('Frequency')

# Relationship between number of reviews and price
plt.subplot(2, 2, 2)
sns.scatterplot(x='price', y='number_of_reviews', data=df.dropna())
plt.title('Relationship between Price and Number of Reviews')
plt.xlabel('Price')
plt.ylabel('Number of Reviews')

plt.subplot(2, 2, 3)
sns.scatterplot(x='days', y='number_of_reviews', data=df.dropna())
plt.title('Relationship between Days and Number of Reviews')
plt.xlabel('Days')
plt.ylabel('Number of Reviews')

plt.subplot(2, 2, 4)
sns.scatterplot(x='bathrooms', y='number_of_reviews', data=df.dropna())
plt.title('Relationship between Bathrooms and Number of Reviews')
plt.xlabel('Bathrooms')
plt.ylabel('Number of Reviews')
plt.tight_layout()
plt.show()
```

The distributions are both heavility skewed to the right, indicating that most listings have relatively few reviews (<100).

There also does not seem to have a clean linear relationship between price and the number of reviews. Although houses with more reviews does tend to have lower price, likely because people tend to want to book cheaper houses.

### Poisson Model
```{python}
import statsmodels.api as sm
import statsmodels.formula.api as smf

# Prepare data for modeling
# Convert 'room_type' and 'instant_bookable' to categorical
df_1['room_type'] = df_1['room_type'].astype('category')
df_1['instant_bookable'] = df_1['instant_bookable'].astype('category').cat.codes

# Fit Poisson regression model
formula = 'number_of_reviews ~ room_type + price + review_scores_cleanliness + review_scores_location + review_scores_value + instant_bookable'
poisson_model = smf.glm(formula=formula, data=df_1, family=sm.families.Poisson()).fit()

# Output model summary
model_params = poisson_model.summary2().tables[1][['Coef.', 'Std.Err.', 'z', 'P>|z|']]
model_params
```

Compared to entire homes/apartments (base category), private rooms have a slightly negative effect on the number of reviews (-0.031), and shared rooms have a more substantial negative effect (-0.27). Assuming number of reviews as a good proxy of number of bookings, this could indicate that privates rooms and shared rooms generally receive less bookings.

In addition, higher cleanliness scores increases the number of bookings while higher location scores and values are associated with fewer bookings. 

Lastly, listings that are instantly bookable tend to have more bookings, which makes sense as it allows guests to book without waiting for host approval, making the process quicker and more convenient.